{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Leaderboard Leaderboard: General Purpose Algorithms on matbench_v0.1 Find more information about this benchmark on the benchmark info page Task name Samples Algorithm Verified MAE (unit) or ROCAUC Notes matbench_steels 312 fictitious_compound 53.3729 (MPa) matbench_jdft2d 636 fictitious_compound 21.9636 (meV/atom) matbench_phonons 1,265 fictitious_compound 13.6984 (cm^-1) matbench_expt_gap 4,604 fictitious_compound 0.1001 (eV) matbench_dielectric 4,764 fictitious_compound 0.1981 (unitless) matbench_expt_is_metal 4,921 fictitious_compound 0.9937 matbench_glass 5,680 fictitious_compound 0.9690 matbench_log_gvrh 10,987 fictitious_compound 0.0393 (log10(GPa)) matbench_log_kvrh 10,987 fictitious_compound 0.0302 (log10(GPa)) matbench_perovskites 18,928 fictitious_compound 0.0218 (eV/unit cell) matbench_mp_gap 106,113 fictitious_compound 0.0787 (eV) matbench_mp_is_metal 106,113 fictitious_compound 0.9627 matbench_mp_e_form 132,752 fictitious_compound 0.0135 (eV/atom) Scaled errors for regressions on this leaderboard plot are assessed as the ratio of mean absolute error to mean absolute deviation: $$ \\text{Scaled Error} = \\frac{\\text{MAE}}{\\text{MAD}} = \\frac{\\sum_i^N | y_i - y_i^{pred} |}{\\sum_i^N | y_i - \\bar{y} | } $$ Overview Matbench is an ImageNet for materials science ; a curated set of 13 supervised, pre-cleaned, ready-to-use ML tasks for benchmarking and fair comparison. The tasks span a wide domain of inorganic materials science applications including electronic, thermodynamic, mechanical, and thermal properties among crystals, 2D materials, disordered metals, and more. The Matbench python package provides everything needed to use Matbench with your ML algorithm in ~10 lines of code or less. What can Matbench offer? This website Leaderboard of results for state-of-the-art materials ML algorithms on standardized test problems Interactively explore and download the tasks on MPContribs-ML , a platform hosted by The Materials Project . See Benchmark Info for links to each dataset. Each and every result is backed by a peer-reviewed publication and a jupyter notebook (similar to Papers With Code) - i.e., how were these results were obtained? Glossary of all algorithms' results on the Matbench problems The Matbench Python package Probe ML algorithms strengths and weaknesses across a wide range of materials property prediction tasks Run a full benchmark in ~10 lines of code Submit results as a PR to the Matbench repo to compare with other algorithms and appear on the leaderboard Benchmark both general purpose ML models as well as algorithms specialized for particular domains Summary of Matbench's Tasks Matbench's 13 tasks can be broken down into various categories; it includes both the small - less than 10,000 samples - datasets that characterize experimental materials data as well as larger datasets from computer modelling methods like density functional theory (DFT). Each task in Matbench consists of a three things: A set of inputs: crystal structures or chemical compositions. A set of outputs: target properties, such as formation energy. A test procedure: a way to get a score for your algorithm The Matbench Python package provides functions for getting the first two (packaged together for each task as a dataset ) as well as running the test procedure. See the How to use documentation page to get started. Citing Matbench You can find details and results on the benchmark in our paper Benchmarking materials property prediction methods: the Matbench test set and Automatminer reference algorithm . Please consider citing this paper if you use Matbench v0.1 for benchmarking, comparison, or prototyping. You can cite Matbench using this reference: Dunn, A., Wang, Q., Ganose, A., Dopp, D., Jain, A. Benchmarking Materials Property Prediction Methods: The Matbench Test Set and Automatminer Reference Algorithm. npj Computational Materials 6, 138 (2020). https://doi.org/10.1038/s41524-020-00406-3","title":"Leaderboard"},{"location":"#leaderboard","text":"","title":"Leaderboard"},{"location":"#leaderboard-general-purpose-algorithms-on-matbench_v01","text":"Find more information about this benchmark on the benchmark info page Task name Samples Algorithm Verified MAE (unit) or ROCAUC Notes matbench_steels 312 fictitious_compound 53.3729 (MPa) matbench_jdft2d 636 fictitious_compound 21.9636 (meV/atom) matbench_phonons 1,265 fictitious_compound 13.6984 (cm^-1) matbench_expt_gap 4,604 fictitious_compound 0.1001 (eV) matbench_dielectric 4,764 fictitious_compound 0.1981 (unitless) matbench_expt_is_metal 4,921 fictitious_compound 0.9937 matbench_glass 5,680 fictitious_compound 0.9690 matbench_log_gvrh 10,987 fictitious_compound 0.0393 (log10(GPa)) matbench_log_kvrh 10,987 fictitious_compound 0.0302 (log10(GPa)) matbench_perovskites 18,928 fictitious_compound 0.0218 (eV/unit cell) matbench_mp_gap 106,113 fictitious_compound 0.0787 (eV) matbench_mp_is_metal 106,113 fictitious_compound 0.9627 matbench_mp_e_form 132,752 fictitious_compound 0.0135 (eV/atom) Scaled errors for regressions on this leaderboard plot are assessed as the ratio of mean absolute error to mean absolute deviation: $$ \\text{Scaled Error} = \\frac{\\text{MAE}}{\\text{MAD}} = \\frac{\\sum_i^N | y_i - y_i^{pred} |}{\\sum_i^N | y_i - \\bar{y} | } $$","title":"Leaderboard: General Purpose Algorithms on matbench_v0.1"},{"location":"#overview","text":"Matbench is an ImageNet for materials science ; a curated set of 13 supervised, pre-cleaned, ready-to-use ML tasks for benchmarking and fair comparison. The tasks span a wide domain of inorganic materials science applications including electronic, thermodynamic, mechanical, and thermal properties among crystals, 2D materials, disordered metals, and more. The Matbench python package provides everything needed to use Matbench with your ML algorithm in ~10 lines of code or less.","title":"Overview"},{"location":"#what-can-matbench-offer","text":"","title":"What can Matbench offer?"},{"location":"#this-website","text":"Leaderboard of results for state-of-the-art materials ML algorithms on standardized test problems Interactively explore and download the tasks on MPContribs-ML , a platform hosted by The Materials Project . See Benchmark Info for links to each dataset. Each and every result is backed by a peer-reviewed publication and a jupyter notebook (similar to Papers With Code) - i.e., how were these results were obtained? Glossary of all algorithms' results on the Matbench problems","title":"This website"},{"location":"#the-matbench-python-package","text":"Probe ML algorithms strengths and weaknesses across a wide range of materials property prediction tasks Run a full benchmark in ~10 lines of code Submit results as a PR to the Matbench repo to compare with other algorithms and appear on the leaderboard Benchmark both general purpose ML models as well as algorithms specialized for particular domains","title":"The Matbench Python package"},{"location":"#summary-of-matbenchs-tasks","text":"Matbench's 13 tasks can be broken down into various categories; it includes both the small - less than 10,000 samples - datasets that characterize experimental materials data as well as larger datasets from computer modelling methods like density functional theory (DFT). Each task in Matbench consists of a three things: A set of inputs: crystal structures or chemical compositions. A set of outputs: target properties, such as formation energy. A test procedure: a way to get a score for your algorithm The Matbench Python package provides functions for getting the first two (packaged together for each task as a dataset ) as well as running the test procedure. See the How to use documentation page to get started.","title":"Summary of Matbench's Tasks"},{"location":"#citing-matbench","text":"You can find details and results on the benchmark in our paper Benchmarking materials property prediction methods: the Matbench test set and Automatminer reference algorithm . Please consider citing this paper if you use Matbench v0.1 for benchmarking, comparison, or prototyping. You can cite Matbench using this reference: Dunn, A., Wang, Q., Ganose, A., Dopp, D., Jain, A. Benchmarking Materials Property Prediction Methods: The Matbench Test Set and Automatminer Reference Algorithm. npj Computational Materials 6, 138 (2020). https://doi.org/10.1038/s41524-020-00406-3","title":"Citing Matbench"},{"location":"Benchmark%20Info/matbench_v0.1/","text":"Benchmark info for matbench_v0.1 The matbench_v0.1 benchmark contains 13 tasks: Task name Task type Target column (unit) Input type Samples MAD (regression) or Fraction True (classification) Links Submissions matbench_steels regression yield strength (MPa) composition 312 229.3743 download , interactive 7 matbench_jdft2d regression exfoliation_en (meV/atom) structure 636 67.2020 download , interactive 9 matbench_phonons regression last phdos peak (cm^-1) structure 1,265 323.7870 download , interactive 9 matbench_expt_gap regression gap expt (eV) composition 4,604 1.1432 download , interactive 9 matbench_dielectric regression n (unitless) structure 4,764 0.8085 download , interactive 9 matbench_expt_is_metal classification is_metal composition 4,921 0.4981 download , interactive 6 matbench_glass classification gfa composition 5,680 0.7104 download , interactive 6 matbench_log_gvrh regression log10(G_VRH) (log10(GPa)) structure 10,987 0.2931 download , interactive 9 matbench_log_kvrh regression log10(K_VRH) (log10(GPa)) structure 10,987 0.2897 download , interactive 9 matbench_perovskites regression e_form (eV/unit cell) structure 18,928 0.5660 download , interactive 9 matbench_mp_gap regression gap pbe (eV) structure 106,113 1.3271 download , interactive 9 matbench_mp_is_metal classification is_metal structure 106,113 0.4349 download , interactive 8 matbench_mp_e_form regression e_form (eV/atom) structure 132,752 1.0059 download , interactive 9","title":"Benchmark info for `matbench_v0.1`"},{"location":"Benchmark%20Info/matbench_v0.1/#benchmark-info-for-matbench_v01","text":"The matbench_v0.1 benchmark contains 13 tasks: Task name Task type Target column (unit) Input type Samples MAD (regression) or Fraction True (classification) Links Submissions matbench_steels regression yield strength (MPa) composition 312 229.3743 download , interactive 7 matbench_jdft2d regression exfoliation_en (meV/atom) structure 636 67.2020 download , interactive 9 matbench_phonons regression last phdos peak (cm^-1) structure 1,265 323.7870 download , interactive 9 matbench_expt_gap regression gap expt (eV) composition 4,604 1.1432 download , interactive 9 matbench_dielectric regression n (unitless) structure 4,764 0.8085 download , interactive 9 matbench_expt_is_metal classification is_metal composition 4,921 0.4981 download , interactive 6 matbench_glass classification gfa composition 5,680 0.7104 download , interactive 6 matbench_log_gvrh regression log10(G_VRH) (log10(GPa)) structure 10,987 0.2931 download , interactive 9 matbench_log_kvrh regression log10(K_VRH) (log10(GPa)) structure 10,987 0.2897 download , interactive 9 matbench_perovskites regression e_form (eV/unit cell) structure 18,928 0.5660 download , interactive 9 matbench_mp_gap regression gap pbe (eV) structure 106,113 1.3271 download , interactive 9 matbench_mp_is_metal classification is_metal structure 106,113 0.4349 download , interactive 8 matbench_mp_e_form regression e_form (eV/atom) structure 132,752 1.0059 download , interactive 9","title":"Benchmark info for matbench_v0.1"},{"location":"Benchmark%20Info/notes/","text":"Notes on Benchmarking General-purpose vs Task-specific algorithms \"General purpose\" algorithms are treated differently from task-specific algorithms in Matbench for the purposes of ranking. We make this distinction because some algorithms can be trained and used - in theory - for predicting any property of a material as long as they are trained on sufficient data. Others are specialized for particular domains and need a separate comparison for fair analysis. General purpose algorithms General purpose algorithms are valid for all the tasks in a benchmark using the same human-chosen configuration . Beyond defining a single configuration before beginning a benchmark, a human should not be hand-tuning or informing the algorithm about architecture, parameters, or hyperparameters. However, general purpose algorithms can automatically determine hyperparameters and parameters as part of their fitting processes in each fold. We include algorithms as \"general purpose\" to include on the general purpose leaderboard if any one of the following criteria is met for Matbench v0.1: All 13 tasks are recorded, OR... All 10 regression tasks are recorded, OR... All 9 structure tasks are recorded. If only the 9 structure tasks are recorded, the algorithm is marked with \"requires structure\". General purpose algorithms' results will appear on both the General Purpose Leaderboard as well as the Task-specific leaderboards. Task specific algorithms Task-specific algorithms can fit on any subset of tasks; for example, a single task. Task-specific algorithms may be valid or specialized only for a subset of the tasks in the benchmark. For example, if you have a model which was specifically created for predicting bulk metallic glasses, you may submit a benchmark containing only results for the matbench_glass dataset. Task-specific results will only appear on the Task-specific leaderboards, not on the General Purpose Leaderboard . Why MAE? Mean absolute error was chosen as the ranking metric for regression because: The meaning of MAE is the most easily inferred Dataset targets which should be analyzed according to relative error (such as bulk moduli) have their target transformed to order-of-magnitude form (e.g., log10). MAE are valid for all target values, unlike mean absolute percentage errors, which are invalid for 0-valued targets. That being said, other error metrics are also informative beyond what MAE can offer. Therefore, Matbench offers multiple error metrics to help assess generalization error. Mean absolute percentage error ( mape* ) scores Mean absolute percentage error is only valid on sets of data without any true values of zero. Also, small true values can result in very large MAPE for samples with even very small predicted error. A threshold of 1e-5 is applied to mask samples with true absolute values smaller than this from the MAPE calculation. The reported MAPE is the decimal (not percentage) among these masked samples; i.e., a MAPE of 11% corresponds to mape*=0.11 , and a MAPE of 11,000% corresponds to mape*=110 . Please use the given MAPE scores with a grain of salt, as they are not complete for the reason given above .","title":"Notes on Benchmarking"},{"location":"Benchmark%20Info/notes/#notes-on-benchmarking","text":"","title":"Notes on Benchmarking"},{"location":"Benchmark%20Info/notes/#general-purpose-vs-task-specific-algorithms","text":"\"General purpose\" algorithms are treated differently from task-specific algorithms in Matbench for the purposes of ranking. We make this distinction because some algorithms can be trained and used - in theory - for predicting any property of a material as long as they are trained on sufficient data. Others are specialized for particular domains and need a separate comparison for fair analysis.","title":"General-purpose vs Task-specific algorithms"},{"location":"Benchmark%20Info/notes/#general-purpose-algorithms","text":"General purpose algorithms are valid for all the tasks in a benchmark using the same human-chosen configuration . Beyond defining a single configuration before beginning a benchmark, a human should not be hand-tuning or informing the algorithm about architecture, parameters, or hyperparameters. However, general purpose algorithms can automatically determine hyperparameters and parameters as part of their fitting processes in each fold. We include algorithms as \"general purpose\" to include on the general purpose leaderboard if any one of the following criteria is met for Matbench v0.1: All 13 tasks are recorded, OR... All 10 regression tasks are recorded, OR... All 9 structure tasks are recorded. If only the 9 structure tasks are recorded, the algorithm is marked with \"requires structure\". General purpose algorithms' results will appear on both the General Purpose Leaderboard as well as the Task-specific leaderboards.","title":"General purpose algorithms"},{"location":"Benchmark%20Info/notes/#task-specific-algorithms","text":"Task-specific algorithms can fit on any subset of tasks; for example, a single task. Task-specific algorithms may be valid or specialized only for a subset of the tasks in the benchmark. For example, if you have a model which was specifically created for predicting bulk metallic glasses, you may submit a benchmark containing only results for the matbench_glass dataset. Task-specific results will only appear on the Task-specific leaderboards, not on the General Purpose Leaderboard .","title":"Task specific algorithms"},{"location":"Benchmark%20Info/notes/#why-mae","text":"Mean absolute error was chosen as the ranking metric for regression because: The meaning of MAE is the most easily inferred Dataset targets which should be analyzed according to relative error (such as bulk moduli) have their target transformed to order-of-magnitude form (e.g., log10). MAE are valid for all target values, unlike mean absolute percentage errors, which are invalid for 0-valued targets. That being said, other error metrics are also informative beyond what MAE can offer. Therefore, Matbench offers multiple error metrics to help assess generalization error.","title":"Why MAE?"},{"location":"Benchmark%20Info/notes/#mean-absolute-percentage-error-mape-scores","text":"Mean absolute percentage error is only valid on sets of data without any true values of zero. Also, small true values can result in very large MAPE for samples with even very small predicted error. A threshold of 1e-5 is applied to mask samples with true absolute values smaller than this from the MAPE calculation. The reported MAPE is the decimal (not percentage) among these masked samples; i.e., a MAPE of 11% corresponds to mape*=0.11 , and a MAPE of 11,000% corresponds to mape*=110 . Please use the given MAPE scores with a grain of salt, as they are not complete for the reason given above .","title":"Mean absolute percentage error (mape*) scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_Ax_CrabNet_v1.2.1/","text":"matbench_v0.1: Ax+CrabNet v1.2.1 Algorithm description: Use Ax Bayesian adaptive design to simultaneously optimize 23 hyperparameters of CrabNet. 100 sequential design iterations were used, and parameters were chosen based on a combination of intuition and algorithm/data constraints (e.g. elemental featurizers which were missing elements contained in the dataset were removed). The first 46 iterations (23*2 parameters) were based on SOBOL sampling to create a rough initial model, while the remaining 56 iterations were Bayesian adaptive design iterations. For the inner loops (where hyperparameter optimization is performed), the average MAE across each of the five inner folds was used as Ax's objective to minimize. The best parameter set was then trained on all the inner fold data and used to predict on the test set (unknown during hyperparameter optimization). This is nested cross-validation, and is computationally expensive. Notes: A Jupyter notebook is provided which contains additional details about the run of the algorithm. If you decide to run this yourself, because it can take several days to run, be sure to set the dummy variable to True and run an initial test that it runs free of errors. Raw data download and example notebook available on the matbench repo . References (in bibtex format): ['@article{Wang2021crabnet, author = {Wang, Anthony Yu-Tung and Kauwe, Steven ' 'K. and Murdock, Ryan J. and Sparks, Taylor D.}, year = {2021}, title = ' '{Compositionally restricted attention-based network for materials property ' 'predictions}, pages = {77}, volume = {7}, number = {1}, doi = ' '{10.1038/s41524-021-00545-1}, publisher = {{Nature Publishing Group}}, ' 'shortjournal = {npj Comput. Mater.}, journal = {npj Computational ' 'Materials}', '@article{wang_kauwe_murdock_sparks_2021, place={Cambridge}, ' 'title={Compositionally-Restricted Attention-Based Network for Materials ' 'Property Prediction}, DOI={10.26434/chemrxiv.11869026.v3}, ' 'journal={ChemRxiv}, publisher={Cambridge Open Engage}, author={Wang, Anthony ' 'and Kauwe, Steven and Murdock, Ryan and Sparks, Taylor}, year={2021}} This ' 'content is a preprint and has not been peer-reviewed.'] User metadata: {'algorithm_version': '1.2.1'} Metadata: tasks recorded 1/13 complete? \u2717 composition complete? \u2717 structure complete? \u2717 regression complete? \u2717 classification complete? \u2717 Software Requirements {'python': [['ax_platform==0.2.3', 'crabnet==1.2.1', 'scikit_learn==1.0.2', 'matbench==0.5', 'kaleido==0.2.1']]} Task data: matbench_expt_gap Fold scores fold mae rmse mape* max_error fold_0 0.3465 0.8037 0.3899 5.5211 fold_1 0.4029 0.9289 0.3584 7.2696 fold_2 0.3599 0.9700 0.3834 11.0998 fold_3 0.3324 0.7836 0.3411 5.8159 fold_4 0.3412 0.8500 0.4276 7.2613 Fold score stats metric mean max min std mae 0.3566 0.4029 0.3324 0.0248 rmse 0.8673 0.9700 0.7836 0.0717 mape* 0.3801 0.4276 0.3411 0.0295 max_error 7.3935 11.0998 5.5211 1.9882 Fold parameters fold params dict fold_0 {'N': 4, 'alpha': 0.8790919451473411, 'batch_size': 69, 'betas': [0.5216069223062726, 0.7117768790338862], 'bias': True, 'criterion': 'RobustL1', 'd_model': 860, 'dim_feedforward': 3498, 'dropout': 0.... fold_1 {'N': 5, 'alpha': 0.7990423841817611, 'batch_size': 165, 'betas': [0.6461252540288698, 0.7283172840513323], 'bias': False, 'criterion': 'RobustL1', 'd_model': 516, 'dim_feedforward': 2663, 'dropout': ... fold_2 {'N': 3, 'alpha': 0.8041617902337612, 'batch_size': 63, 'betas': [0.711095287462972, 0.9476000614084613], 'bias': False, 'criterion': 'RobustL1', 'd_model': 660, 'dim_feedforward': 3469, 'dropout': 0.... fold_3 {'N': 3, 'alpha': 1.0, 'batch_size': 241, 'betas': [0.5591583071453617, 0.5830227398533708], 'bias': True, 'criterion': 'RobustL1', 'd_model': 940, 'dim_feedforward': 1981, 'dropout': 0.00347905979314... fold_4 {'N': 6, 'alpha': 0.7344910928263977, 'batch_size': 125, 'betas': [0.5574111505617741, 0.9346732886315889], 'bias': False, 'criterion': 'RobustL1', 'd_model': 288, 'dim_feedforward': 1393, 'dropout': ...","title":"matbench_v0.1: Ax+CrabNet v1.2.1"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_Ax_CrabNet_v1.2.1/#matbench_v01-axcrabnet-v121","text":"","title":"matbench_v0.1: Ax+CrabNet v1.2.1"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_Ax_CrabNet_v1.2.1/#algorithm-description","text":"Use Ax Bayesian adaptive design to simultaneously optimize 23 hyperparameters of CrabNet. 100 sequential design iterations were used, and parameters were chosen based on a combination of intuition and algorithm/data constraints (e.g. elemental featurizers which were missing elements contained in the dataset were removed). The first 46 iterations (23*2 parameters) were based on SOBOL sampling to create a rough initial model, while the remaining 56 iterations were Bayesian adaptive design iterations. For the inner loops (where hyperparameter optimization is performed), the average MAE across each of the five inner folds was used as Ax's objective to minimize. The best parameter set was then trained on all the inner fold data and used to predict on the test set (unknown during hyperparameter optimization). This is nested cross-validation, and is computationally expensive.","title":"Algorithm description:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_Ax_CrabNet_v1.2.1/#notes","text":"A Jupyter notebook is provided which contains additional details about the run of the algorithm. If you decide to run this yourself, because it can take several days to run, be sure to set the dummy variable to True and run an initial test that it runs free of errors. Raw data download and example notebook available on the matbench repo .","title":"Notes:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_Ax_CrabNet_v1.2.1/#references-in-bibtex-format","text":"['@article{Wang2021crabnet, author = {Wang, Anthony Yu-Tung and Kauwe, Steven ' 'K. and Murdock, Ryan J. and Sparks, Taylor D.}, year = {2021}, title = ' '{Compositionally restricted attention-based network for materials property ' 'predictions}, pages = {77}, volume = {7}, number = {1}, doi = ' '{10.1038/s41524-021-00545-1}, publisher = {{Nature Publishing Group}}, ' 'shortjournal = {npj Comput. Mater.}, journal = {npj Computational ' 'Materials}', '@article{wang_kauwe_murdock_sparks_2021, place={Cambridge}, ' 'title={Compositionally-Restricted Attention-Based Network for Materials ' 'Property Prediction}, DOI={10.26434/chemrxiv.11869026.v3}, ' 'journal={ChemRxiv}, publisher={Cambridge Open Engage}, author={Wang, Anthony ' 'and Kauwe, Steven and Murdock, Ryan and Sparks, Taylor}, year={2021}} This ' 'content is a preprint and has not been peer-reviewed.']","title":"References (in bibtex format):"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_Ax_CrabNet_v1.2.1/#user-metadata","text":"{'algorithm_version': '1.2.1'}","title":"User metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_Ax_CrabNet_v1.2.1/#metadata","text":"tasks recorded 1/13 complete? \u2717 composition complete? \u2717 structure complete? \u2717 regression complete? \u2717 classification complete? \u2717","title":"Metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_Ax_CrabNet_v1.2.1/#software-requirements","text":"{'python': [['ax_platform==0.2.3', 'crabnet==1.2.1', 'scikit_learn==1.0.2', 'matbench==0.5', 'kaleido==0.2.1']]}","title":"Software Requirements"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_Ax_CrabNet_v1.2.1/#task-data","text":"","title":"Task data:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_Ax_CrabNet_v1.2.1/#matbench_expt_gap","text":"","title":"matbench_expt_gap"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_Ax_CrabNet_v1.2.1/#fold-scores","text":"fold mae rmse mape* max_error fold_0 0.3465 0.8037 0.3899 5.5211 fold_1 0.4029 0.9289 0.3584 7.2696 fold_2 0.3599 0.9700 0.3834 11.0998 fold_3 0.3324 0.7836 0.3411 5.8159 fold_4 0.3412 0.8500 0.4276 7.2613","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_Ax_CrabNet_v1.2.1/#fold-score-stats","text":"metric mean max min std mae 0.3566 0.4029 0.3324 0.0248 rmse 0.8673 0.9700 0.7836 0.0717 mape* 0.3801 0.4276 0.3411 0.0295 max_error 7.3935 11.0998 5.5211 1.9882","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_Ax_CrabNet_v1.2.1/#fold-parameters","text":"fold params dict fold_0 {'N': 4, 'alpha': 0.8790919451473411, 'batch_size': 69, 'betas': [0.5216069223062726, 0.7117768790338862], 'bias': True, 'criterion': 'RobustL1', 'd_model': 860, 'dim_feedforward': 3498, 'dropout': 0.... fold_1 {'N': 5, 'alpha': 0.7990423841817611, 'batch_size': 165, 'betas': [0.6461252540288698, 0.7283172840513323], 'bias': False, 'criterion': 'RobustL1', 'd_model': 516, 'dim_feedforward': 2663, 'dropout': ... fold_2 {'N': 3, 'alpha': 0.8041617902337612, 'batch_size': 63, 'betas': [0.711095287462972, 0.9476000614084613], 'bias': False, 'criterion': 'RobustL1', 'd_model': 660, 'dim_feedforward': 3469, 'dropout': 0.... fold_3 {'N': 3, 'alpha': 1.0, 'batch_size': 241, 'betas': [0.5591583071453617, 0.5830227398533708], 'bias': True, 'criterion': 'RobustL1', 'd_model': 940, 'dim_feedforward': 1981, 'dropout': 0.00347905979314... fold_4 {'N': 6, 'alpha': 0.7344910928263977, 'batch_size': 125, 'betas': [0.5574111505617741, 0.9346732886315889], 'bias': False, 'criterion': 'RobustL1', 'd_model': 288, 'dim_feedforward': 1393, 'dropout': ...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/","text":"matbench_v0.1: CrabNet Algorithm description: Compositionally restricted attention-based network for materials property predictions. See github page for more information: https://github.com/anthony-wang/CrabNet. Notes: Raw data download and example notebook available on the matbench repo . References (in bibtex format): ('@article{Wang2021crabnet,\\n' ' author = {Wang, Anthony Yu-Tung and Kauwe, Steven K. and Murdock, Ryan J. ' 'and Sparks, Taylor D.},\\n' ' year = {2021},\\n' ' title = {Compositionally restricted attention-based network for materials ' 'property predictions},\\n' ' pages = {77},\\n' ' volume = {7},\\n' ' number = {1},\\n' ' doi = {10.1038/s41524-021-00545-1},\\n' ' publisher = {{Nature Publishing Group}},\\n' ' shortjournal = {npj Comput. Mater.},\\n' ' journal = {npj Computational Materials}\\n' ' }') User metadata: {} Metadata: tasks recorded 10/13 complete? \u2717 composition complete? \u2717 structure complete? \u2717 regression complete? \u2713 classification complete? \u2717 Software Requirements 'See GitHub page for CrabNet, CrabNet version: be89e92.' Task data: matbench_dielectric Fold scores fold mae rmse mape* max_error fold_0 0.2147 0.6794 0.0733 14.7263 fold_1 0.3048 1.1243 0.0989 19.2249 fold_2 0.4376 2.9443 0.0925 59.1583 fold_3 0.3402 2.3061 0.0797 53.8845 fold_4 0.3195 1.5900 0.0942 27.8634 Fold score stats metric mean max min std mae 0.3234 0.4376 0.2147 0.0714 rmse 1.7288 2.9443 0.6794 0.8120 mape* 0.0877 0.0989 0.0733 0.0096 max_error 34.9715 59.1583 14.7263 18.1717 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_expt_gap Fold scores fold mae rmse mape* max_error fold_0 0.3476 0.8404 0.3974 6.6728 fold_1 0.3434 0.8214 0.2866 6.3943 fold_2 0.3473 0.8680 0.3421 9.1598 fold_3 0.3329 0.8518 0.3553 9.8002 fold_4 0.3602 0.8702 0.4349 7.6012 Fold score stats metric mean max min std mae 0.3463 0.3602 0.3329 0.0088 rmse 0.8504 0.8702 0.8214 0.0181 mape* 0.3633 0.4349 0.2866 0.0504 max_error 7.9256 9.8002 6.3943 1.3459 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_jdft2d Fold scores fold mae rmse mape* max_error fold_0 36.0753 71.1404 24.8117 394.7442 fold_1 45.8800 107.0134 0.3347 669.9718 fold_2 67.1110 192.8415 0.6296 1039.2952 fold_3 31.6798 65.1904 0.2653 319.1235 fold_4 47.3058 163.8581 0.5401 1532.0118 Fold score stats metric mean max min std mae 45.6104 67.1110 31.6798 12.2491 rmse 120.0088 192.8415 65.1904 50.5756 mape* 5.3163 24.8117 0.2653 9.7486 max_error 791.0293 1532.0118 319.1235 448.3487 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_log_gvrh Fold scores fold mae rmse mape* max_error fold_0 0.0994 0.1538 0.0787 1.4432 fold_1 0.0994 0.1648 0.0794 2.4220 fold_2 0.1020 0.1594 0.0813 1.0792 fold_3 0.1034 0.1607 0.0783 1.0056 fold_4 0.1031 0.1633 0.0810 1.5313 Fold score stats metric mean max min std mae 0.1014 0.1034 0.0994 0.0017 rmse 0.1604 0.1648 0.1538 0.0038 mape* 0.0797 0.0813 0.0783 0.0012 max_error 1.4963 2.4220 1.0056 0.5051 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_log_kvrh Fold scores fold mae rmse mape* max_error fold_0 0.0748 0.1449 0.0509 1.6732 fold_1 0.0780 0.1549 0.0525 1.6914 fold_2 0.0698 0.1344 0.0463 1.3116 fold_3 0.0793 0.1508 0.0571 1.0620 fold_4 0.0773 0.1506 0.0532 1.8430 Fold score stats metric mean max min std mae 0.0758 0.0793 0.0698 0.0034 rmse 0.1471 0.1549 0.1344 0.0071 mape* 0.0520 0.0571 0.0463 0.0035 max_error 1.5162 1.8430 1.0620 0.2864 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_perovskites Fold scores fold mae rmse mape* max_error fold_0 0.4080 0.5445 0.4861 2.3726 fold_1 0.4160 0.5515 0.5261 2.1724 fold_2 0.4034 0.5363 0.4858 2.0999 fold_3 0.4096 0.5428 0.5270 2.2336 fold_4 0.3953 0.5310 0.4611 2.2192 Fold score stats metric mean max min std mae 0.4065 0.4160 0.3953 0.0069 rmse 0.5412 0.5515 0.5310 0.0070 mape* 0.4972 0.5270 0.4611 0.0256 max_error 2.2195 2.3726 2.0999 0.0896 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_phonons Fold scores fold mae rmse mape* max_error fold_0 60.8044 155.2771 0.0881 1452.7562 fold_1 58.1439 143.0602 0.0915 1207.7800 fold_2 60.2413 165.1000 0.0869 1445.4633 fold_3 47.7603 114.5270 0.0895 894.9224 fold_4 48.6072 113.9230 0.0871 1124.2209 Fold score stats metric mean max min std mae 55.1114 60.8044 47.7603 5.7317 rmse 138.3775 165.1000 113.9230 20.9212 mape* 0.0886 0.0915 0.0869 0.0017 max_error 1225.0285 1452.7562 894.9224 209.7051 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_steels Fold scores fold mae rmse mape* max_error fold_0 116.2240 176.5695 0.0774 576.3912 fold_1 88.0920 117.7789 0.0632 387.1094 fold_2 108.1233 153.4745 0.0717 485.5283 fold_3 137.4903 192.2622 0.0932 549.5977 fold_4 86.6503 124.9355 0.0654 386.2023 Fold score stats metric mean max min std mae 107.3160 137.4903 86.6503 18.9057 rmse 153.0041 192.2622 117.7789 28.7243 mape* 0.0742 0.0932 0.0632 0.0107 max_error 476.9658 576.3912 386.2023 79.4309 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_mp_gap Fold scores fold mae rmse mape* max_error fold_0 0.2653 0.5814 5.4032 6.8675 fold_1 0.2613 0.5811 2.9969 7.9829 fold_2 0.2648 0.5903 5.3833 7.7856 fold_3 0.2658 0.5954 10.1488 7.9675 fold_4 0.2704 0.6006 5.8835 6.8672 Fold score stats metric mean max min std mae 0.2655 0.2704 0.2613 0.0029 rmse 0.5898 0.6006 0.5811 0.0077 mape* 5.9631 10.1488 2.9969 2.3227 max_error 7.4941 7.9829 6.8672 0.5165 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_mp_e_form Fold scores fold mae rmse mape* max_error fold_0 0.0853 0.2492 0.5075 4.2164 fold_1 0.0857 0.2613 0.4542 6.3774 fold_2 0.0879 0.2587 0.4088 4.0334 fold_3 0.0854 0.2499 0.5596 6.2383 fold_4 0.0865 0.2532 0.4764 3.9335 Fold score stats metric mean max min std mae 0.0862 0.0879 0.0853 0.0010 rmse 0.2544 0.2613 0.2492 0.0048 mape* 0.4813 0.5596 0.4088 0.0507 max_error 4.9598 6.3774 3.9335 1.1053 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"matbench_v0.1: CrabNet"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#matbench_v01-crabnet","text":"","title":"matbench_v0.1: CrabNet"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#algorithm-description","text":"Compositionally restricted attention-based network for materials property predictions. See github page for more information: https://github.com/anthony-wang/CrabNet.","title":"Algorithm description:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#notes","text":"Raw data download and example notebook available on the matbench repo .","title":"Notes:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#references-in-bibtex-format","text":"('@article{Wang2021crabnet,\\n' ' author = {Wang, Anthony Yu-Tung and Kauwe, Steven K. and Murdock, Ryan J. ' 'and Sparks, Taylor D.},\\n' ' year = {2021},\\n' ' title = {Compositionally restricted attention-based network for materials ' 'property predictions},\\n' ' pages = {77},\\n' ' volume = {7},\\n' ' number = {1},\\n' ' doi = {10.1038/s41524-021-00545-1},\\n' ' publisher = {{Nature Publishing Group}},\\n' ' shortjournal = {npj Comput. Mater.},\\n' ' journal = {npj Computational Materials}\\n' ' }')","title":"References (in bibtex format):"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#user-metadata","text":"{}","title":"User metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#metadata","text":"tasks recorded 10/13 complete? \u2717 composition complete? \u2717 structure complete? \u2717 regression complete? \u2713 classification complete? \u2717","title":"Metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#software-requirements","text":"'See GitHub page for CrabNet, CrabNet version: be89e92.'","title":"Software Requirements"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#task-data","text":"","title":"Task data:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#matbench_dielectric","text":"","title":"matbench_dielectric"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-scores","text":"fold mae rmse mape* max_error fold_0 0.2147 0.6794 0.0733 14.7263 fold_1 0.3048 1.1243 0.0989 19.2249 fold_2 0.4376 2.9443 0.0925 59.1583 fold_3 0.3402 2.3061 0.0797 53.8845 fold_4 0.3195 1.5900 0.0942 27.8634","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-score-stats","text":"metric mean max min std mae 0.3234 0.4376 0.2147 0.0714 rmse 1.7288 2.9443 0.6794 0.8120 mape* 0.0877 0.0989 0.0733 0.0096 max_error 34.9715 59.1583 14.7263 18.1717","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-parameters","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#matbench_expt_gap","text":"","title":"matbench_expt_gap"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-scores_1","text":"fold mae rmse mape* max_error fold_0 0.3476 0.8404 0.3974 6.6728 fold_1 0.3434 0.8214 0.2866 6.3943 fold_2 0.3473 0.8680 0.3421 9.1598 fold_3 0.3329 0.8518 0.3553 9.8002 fold_4 0.3602 0.8702 0.4349 7.6012","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-score-stats_1","text":"metric mean max min std mae 0.3463 0.3602 0.3329 0.0088 rmse 0.8504 0.8702 0.8214 0.0181 mape* 0.3633 0.4349 0.2866 0.0504 max_error 7.9256 9.8002 6.3943 1.3459","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-parameters_1","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#matbench_jdft2d","text":"","title":"matbench_jdft2d"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-scores_2","text":"fold mae rmse mape* max_error fold_0 36.0753 71.1404 24.8117 394.7442 fold_1 45.8800 107.0134 0.3347 669.9718 fold_2 67.1110 192.8415 0.6296 1039.2952 fold_3 31.6798 65.1904 0.2653 319.1235 fold_4 47.3058 163.8581 0.5401 1532.0118","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-score-stats_2","text":"metric mean max min std mae 45.6104 67.1110 31.6798 12.2491 rmse 120.0088 192.8415 65.1904 50.5756 mape* 5.3163 24.8117 0.2653 9.7486 max_error 791.0293 1532.0118 319.1235 448.3487","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-parameters_2","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#matbench_log_gvrh","text":"","title":"matbench_log_gvrh"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-scores_3","text":"fold mae rmse mape* max_error fold_0 0.0994 0.1538 0.0787 1.4432 fold_1 0.0994 0.1648 0.0794 2.4220 fold_2 0.1020 0.1594 0.0813 1.0792 fold_3 0.1034 0.1607 0.0783 1.0056 fold_4 0.1031 0.1633 0.0810 1.5313","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-score-stats_3","text":"metric mean max min std mae 0.1014 0.1034 0.0994 0.0017 rmse 0.1604 0.1648 0.1538 0.0038 mape* 0.0797 0.0813 0.0783 0.0012 max_error 1.4963 2.4220 1.0056 0.5051","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-parameters_3","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#matbench_log_kvrh","text":"","title":"matbench_log_kvrh"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-scores_4","text":"fold mae rmse mape* max_error fold_0 0.0748 0.1449 0.0509 1.6732 fold_1 0.0780 0.1549 0.0525 1.6914 fold_2 0.0698 0.1344 0.0463 1.3116 fold_3 0.0793 0.1508 0.0571 1.0620 fold_4 0.0773 0.1506 0.0532 1.8430","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-score-stats_4","text":"metric mean max min std mae 0.0758 0.0793 0.0698 0.0034 rmse 0.1471 0.1549 0.1344 0.0071 mape* 0.0520 0.0571 0.0463 0.0035 max_error 1.5162 1.8430 1.0620 0.2864","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-parameters_4","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#matbench_perovskites","text":"","title":"matbench_perovskites"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-scores_5","text":"fold mae rmse mape* max_error fold_0 0.4080 0.5445 0.4861 2.3726 fold_1 0.4160 0.5515 0.5261 2.1724 fold_2 0.4034 0.5363 0.4858 2.0999 fold_3 0.4096 0.5428 0.5270 2.2336 fold_4 0.3953 0.5310 0.4611 2.2192","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-score-stats_5","text":"metric mean max min std mae 0.4065 0.4160 0.3953 0.0069 rmse 0.5412 0.5515 0.5310 0.0070 mape* 0.4972 0.5270 0.4611 0.0256 max_error 2.2195 2.3726 2.0999 0.0896","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-parameters_5","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#matbench_phonons","text":"","title":"matbench_phonons"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-scores_6","text":"fold mae rmse mape* max_error fold_0 60.8044 155.2771 0.0881 1452.7562 fold_1 58.1439 143.0602 0.0915 1207.7800 fold_2 60.2413 165.1000 0.0869 1445.4633 fold_3 47.7603 114.5270 0.0895 894.9224 fold_4 48.6072 113.9230 0.0871 1124.2209","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-score-stats_6","text":"metric mean max min std mae 55.1114 60.8044 47.7603 5.7317 rmse 138.3775 165.1000 113.9230 20.9212 mape* 0.0886 0.0915 0.0869 0.0017 max_error 1225.0285 1452.7562 894.9224 209.7051","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-parameters_6","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#matbench_steels","text":"","title":"matbench_steels"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-scores_7","text":"fold mae rmse mape* max_error fold_0 116.2240 176.5695 0.0774 576.3912 fold_1 88.0920 117.7789 0.0632 387.1094 fold_2 108.1233 153.4745 0.0717 485.5283 fold_3 137.4903 192.2622 0.0932 549.5977 fold_4 86.6503 124.9355 0.0654 386.2023","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-score-stats_7","text":"metric mean max min std mae 107.3160 137.4903 86.6503 18.9057 rmse 153.0041 192.2622 117.7789 28.7243 mape* 0.0742 0.0932 0.0632 0.0107 max_error 476.9658 576.3912 386.2023 79.4309","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-parameters_7","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#matbench_mp_gap","text":"","title":"matbench_mp_gap"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-scores_8","text":"fold mae rmse mape* max_error fold_0 0.2653 0.5814 5.4032 6.8675 fold_1 0.2613 0.5811 2.9969 7.9829 fold_2 0.2648 0.5903 5.3833 7.7856 fold_3 0.2658 0.5954 10.1488 7.9675 fold_4 0.2704 0.6006 5.8835 6.8672","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-score-stats_8","text":"metric mean max min std mae 0.2655 0.2704 0.2613 0.0029 rmse 0.5898 0.6006 0.5811 0.0077 mape* 5.9631 10.1488 2.9969 2.3227 max_error 7.4941 7.9829 6.8672 0.5165","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-parameters_8","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#matbench_mp_e_form","text":"","title":"matbench_mp_e_form"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-scores_9","text":"fold mae rmse mape* max_error fold_0 0.0853 0.2492 0.5075 4.2164 fold_1 0.0857 0.2613 0.4542 6.3774 fold_2 0.0879 0.2587 0.4088 4.0334 fold_3 0.0854 0.2499 0.5596 6.2383 fold_4 0.0865 0.2532 0.4764 3.9335","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-score-stats_9","text":"metric mean max min std mae 0.0862 0.0879 0.0853 0.0010 rmse 0.2544 0.2613 0.2492 0.0048 mape* 0.4813 0.5596 0.4088 0.0507 max_error 4.9598 6.3774 3.9335 1.1053","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet/#fold-parameters_9","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet_v1.2.1/","text":"matbench_v0.1: CrabNet v1.2.1 Algorithm description: Fit CrabNet with default hyperparameters to serve as a baseline for Ax+CrabNet v1.2.1. Notes: A Jupyter notebook is provided which contains additional details about the run of the algorithm. Raw data download and example notebook available on the matbench repo . References (in bibtex format): ['@article{Wang2021crabnet, author = {Wang, Anthony Yu-Tung and Kauwe, Steven ' 'K. and Murdock, Ryan J. and Sparks, Taylor D.}, year = {2021}, title = ' '{Compositionally restricted attention-based network for materials property ' 'predictions}, pages = {77}, volume = {7}, number = {1}, doi = ' '{10.1038/s41524-021-00545-1}, publisher = {{Nature Publishing Group}}, ' 'shortjournal = {npj Comput. Mater.}, journal = {npj Computational ' 'Materials}', '@article{wang_kauwe_murdock_sparks_2021, place={Cambridge}, ' 'title={Compositionally-Restricted Attention-Based Network for Materials ' 'Property Prediction}, DOI={10.26434/chemrxiv.11869026.v3}, ' 'journal={ChemRxiv}, publisher={Cambridge Open Engage}, author={Wang, Anthony ' 'and Kauwe, Steven and Murdock, Ryan and Sparks, Taylor}, year={2021}} This ' 'content is a preprint and has not been peer-reviewed.'] User metadata: {'algorithm_version': '1.2.1'} Metadata: tasks recorded 1/13 complete? \u2717 composition complete? \u2717 structure complete? \u2717 regression complete? \u2717 classification complete? \u2717 Software Requirements {'python': [['crabnet==1.2.1', 'scikit_learn==1.0.2', 'matbench==0.5']]} Task data: matbench_expt_gap Fold scores fold mae rmse mape* max_error fold_0 0.3489 0.8079 0.4441 5.6781 fold_1 0.3674 0.8399 0.3349 7.0404 fold_2 0.4106 1.0092 0.4539 10.2572 fold_3 0.3677 0.8437 0.4181 6.1608 fold_4 0.3839 0.9019 0.4944 7.4912 Fold score stats metric mean max min std mae 0.3757 0.4106 0.3489 0.0207 rmse 0.8805 1.0092 0.8079 0.0711 mape* 0.4291 0.4944 0.3349 0.0531 max_error 7.3256 10.2572 5.6781 1.5984 Fold parameters fold params dict fold_0 {'N': 3, 'adam': False, 'alpha': 0.5, 'base_lr': 0.0001, 'betas': [0.9, 0.999], 'bias': False, 'criterion': None, 'd_model': 512, 'dim_feedforward': 2048, 'dropout': 0.1, 'elem_prop': 'mat2vec', 'emb_... fold_1 {'N': 3, 'adam': False, 'alpha': 0.5, 'base_lr': 0.0001, 'betas': [0.9, 0.999], 'bias': False, 'criterion': None, 'd_model': 512, 'dim_feedforward': 2048, 'dropout': 0.1, 'elem_prop': 'mat2vec', 'emb_... fold_2 {'N': 3, 'adam': False, 'alpha': 0.5, 'base_lr': 0.0001, 'betas': [0.9, 0.999], 'bias': False, 'criterion': None, 'd_model': 512, 'dim_feedforward': 2048, 'dropout': 0.1, 'elem_prop': 'mat2vec', 'emb_... fold_3 {'N': 3, 'adam': False, 'alpha': 0.5, 'base_lr': 0.0001, 'betas': [0.9, 0.999], 'bias': False, 'criterion': None, 'd_model': 512, 'dim_feedforward': 2048, 'dropout': 0.1, 'elem_prop': 'mat2vec', 'emb_... fold_4 {'N': 3, 'adam': False, 'alpha': 0.5, 'base_lr': 0.0001, 'betas': [0.9, 0.999], 'bias': False, 'criterion': None, 'd_model': 512, 'dim_feedforward': 2048, 'dropout': 0.1, 'elem_prop': 'mat2vec', 'emb_...","title":"matbench_v0.1: CrabNet v1.2.1"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet_v1.2.1/#matbench_v01-crabnet-v121","text":"","title":"matbench_v0.1: CrabNet v1.2.1"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet_v1.2.1/#algorithm-description","text":"Fit CrabNet with default hyperparameters to serve as a baseline for Ax+CrabNet v1.2.1.","title":"Algorithm description:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet_v1.2.1/#notes","text":"A Jupyter notebook is provided which contains additional details about the run of the algorithm. Raw data download and example notebook available on the matbench repo .","title":"Notes:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet_v1.2.1/#references-in-bibtex-format","text":"['@article{Wang2021crabnet, author = {Wang, Anthony Yu-Tung and Kauwe, Steven ' 'K. and Murdock, Ryan J. and Sparks, Taylor D.}, year = {2021}, title = ' '{Compositionally restricted attention-based network for materials property ' 'predictions}, pages = {77}, volume = {7}, number = {1}, doi = ' '{10.1038/s41524-021-00545-1}, publisher = {{Nature Publishing Group}}, ' 'shortjournal = {npj Comput. Mater.}, journal = {npj Computational ' 'Materials}', '@article{wang_kauwe_murdock_sparks_2021, place={Cambridge}, ' 'title={Compositionally-Restricted Attention-Based Network for Materials ' 'Property Prediction}, DOI={10.26434/chemrxiv.11869026.v3}, ' 'journal={ChemRxiv}, publisher={Cambridge Open Engage}, author={Wang, Anthony ' 'and Kauwe, Steven and Murdock, Ryan and Sparks, Taylor}, year={2021}} This ' 'content is a preprint and has not been peer-reviewed.']","title":"References (in bibtex format):"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet_v1.2.1/#user-metadata","text":"{'algorithm_version': '1.2.1'}","title":"User metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet_v1.2.1/#metadata","text":"tasks recorded 1/13 complete? \u2717 composition complete? \u2717 structure complete? \u2717 regression complete? \u2717 classification complete? \u2717","title":"Metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet_v1.2.1/#software-requirements","text":"{'python': [['crabnet==1.2.1', 'scikit_learn==1.0.2', 'matbench==0.5']]}","title":"Software Requirements"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet_v1.2.1/#task-data","text":"","title":"Task data:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet_v1.2.1/#matbench_expt_gap","text":"","title":"matbench_expt_gap"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet_v1.2.1/#fold-scores","text":"fold mae rmse mape* max_error fold_0 0.3489 0.8079 0.4441 5.6781 fold_1 0.3674 0.8399 0.3349 7.0404 fold_2 0.4106 1.0092 0.4539 10.2572 fold_3 0.3677 0.8437 0.4181 6.1608 fold_4 0.3839 0.9019 0.4944 7.4912","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet_v1.2.1/#fold-score-stats","text":"metric mean max min std mae 0.3757 0.4106 0.3489 0.0207 rmse 0.8805 1.0092 0.8079 0.0711 mape* 0.4291 0.4944 0.3349 0.0531 max_error 7.3256 10.2572 5.6781 1.5984","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_CrabNet_v1.2.1/#fold-parameters","text":"fold params dict fold_0 {'N': 3, 'adam': False, 'alpha': 0.5, 'base_lr': 0.0001, 'betas': [0.9, 0.999], 'bias': False, 'criterion': None, 'd_model': 512, 'dim_feedforward': 2048, 'dropout': 0.1, 'elem_prop': 'mat2vec', 'emb_... fold_1 {'N': 3, 'adam': False, 'alpha': 0.5, 'base_lr': 0.0001, 'betas': [0.9, 0.999], 'bias': False, 'criterion': None, 'd_model': 512, 'dim_feedforward': 2048, 'dropout': 0.1, 'elem_prop': 'mat2vec', 'emb_... fold_2 {'N': 3, 'adam': False, 'alpha': 0.5, 'base_lr': 0.0001, 'betas': [0.9, 0.999], 'bias': False, 'criterion': None, 'd_model': 512, 'dim_feedforward': 2048, 'dropout': 0.1, 'elem_prop': 'mat2vec', 'emb_... fold_3 {'N': 3, 'adam': False, 'alpha': 0.5, 'base_lr': 0.0001, 'betas': [0.9, 0.999], 'bias': False, 'criterion': None, 'd_model': 512, 'dim_feedforward': 2048, 'dropout': 0.1, 'elem_prop': 'mat2vec', 'emb_... fold_4 {'N': 3, 'adam': False, 'alpha': 0.5, 'base_lr': 0.0001, 'betas': [0.9, 0.999], 'bias': False, 'criterion': None, 'd_model': 512, 'dim_feedforward': 2048, 'dropout': 0.1, 'elem_prop': 'mat2vec', 'emb_...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/","text":"matbench_v0.1: ALIGNN Algorithm description: The Atomistic Line Graph Neural Network (https://doi.org/10.1038/s41524-021-00650-1) introduces a new graph convolution layer that explicitly models both two and three body interactions in atomistic systems. This is achieved by composing two edge-gated graph convolution layers, the first applied to the atomistic line graph L(g) (representing triplet interactions) and the second applied to the atomistic bond graph g (representing pair interactions). The atomistic graph g consists of a node for each atom i (with atom/node representations hi), and one edge for each atom pair within a cutoff radius (with bond/pair representations eij). The atomistic line graph L(g) represents relationships between atom triplets: it has nodes corresponding to bonds (sharing representations eij with those in g) and edges corresponding to bond angles (with angle/triplet representations tijk).The line graph convolution updates the triplet representations and the pair representations; the direct graph convolution further updates the pair representations and the atom representations. Notes: None Raw data download and example notebook available on the matbench repo . References (in bibtex format): ('@article{choudhary2021atomistic,title={Atomistic Line Graph Neural Network ' 'for improved materials property predictions},author={Choudhary, Kamal and ' 'DeCost, Brian},journal={npj Computational ' 'Materials},volume={7},number={1},pages={1--8},year={2021},publisher={Nature ' 'Publishing Group}}') User metadata: {'algorithm': 'ALIGNN'} Metadata: tasks recorded 9/13 complete? \u2717 composition complete? \u2717 structure complete? \u2713 regression complete? \u2717 classification complete? \u2717 Software Requirements {'python': ['absl-py==1.0.0', 'alignn==2021.12.27', 'astunparse==1.6.3', 'attrs==21.3.0', 'black==21.12b0', 'cachetools==4.2.4', 'certifi==2021.10.8', 'charset-normalizer==2.0.9', 'click==8.0.3', 'cloudpickle==2.0.0', 'cycler==0.11.0', 'decorator==5.1.0', 'dgl==0.6.1', 'dgl-cu111==0.6.1', 'dm-tree==0.1.6', 'flake8==4.0.1', 'flatbuffers==2.0', 'fonttools==4.28.5', 'future==0.18.2', 'gast==0.4.0', 'google-auth==2.3.3', 'google-auth-oauthlib==0.4.6', 'google-pasta==0.2.0', 'grpcio==1.43.0', 'h5py==3.6.0', 'idna==3.3', 'importlib-metadata==4.10.0', 'importlib-resources==5.4.0', 'jarvis-tools==2021.12.16', 'joblib==1.1.0', 'jsonschema==4.3.2', 'julia==0.5.6', 'keras==2.7.0', 'Keras-Preprocessing==1.1.2', 'kiwisolver==1.3.2', 'libclang==12.0.0', 'Markdown==3.3.6', 'matbench==0.5', 'matminer==0.6.5', 'matplotlib==3.5.1', 'mccabe==0.6.1', 'modnet==0.1.11', 'monty==2021.8.17', 'mpmath==1.2.1', 'mypy-extensions==0.4.3', 'networkx==2.6.3', 'numpy==1.21.5', 'oauthlib==3.1.1', 'opt-einsum==3.3.0', 'packaging==21.3', 'palettable==3.3.0', 'pandas==1.3.5', 'pathspec==0.9.0', 'Pillow==8.4.0', 'Pint==0.18', 'platformdirs==2.4.0', 'plotly==5.5.0', 'protobuf==3.19.1', 'pyasn1==0.4.8', 'pyasn1-modules==0.2.8', 'pycodestyle==2.8.0', 'pydantic==1.8.2', 'pydocstyle==6.1.1', 'pyflakes==2.4.0', 'pymatgen==2020.8.13', 'pymongo==4.0.1', 'pyparsing==2.4.7', 'pyrsistent==0.18.0', 'python-dateutil==2.8.2', 'pytorch-ignite==0.4.7', 'pytz==2021.3', 'requests==2.26.0', 'requests-oauthlib==1.3.0', 'rsa==4.8', 'ruamel.yaml==0.17.19', 'ruamel.yaml.clib==0.2.6', 'scikit-learn==0.23.2', 'scipy==1.7.3', 'six==1.16.0', 'snowballstemmer==2.2.0', 'spglib==1.16.3', 'sympy==1.9', 'tabulate==0.8.9', 'tenacity==8.0.1', 'tensorboard==2.7.0', 'tensorboard-data-server==0.6.1', 'tensorboard-plugin-wit==1.8.0', 'tensorflow==2.7.0', 'tensorflow-estimator==2.7.0', 'tensorflow-io-gcs-filesystem==0.23.1', 'tensorflow-probability==0.15.0', 'termcolor==1.1.0', 'threadpoolctl==3.0.0', 'tomli==1.2.3', 'toolz==0.11.2', 'torch==1.10.1', 'tqdm==4.62.3', 'typing_extensions==4.0.1', 'uncertainties==3.1.6', 'urllib3==1.26.7', 'Werkzeug==2.0.2', 'wrapt==1.13.3', 'xmltodict==0.12.0', 'zipp==3.6.0']} Task data: matbench_dielectric Fold scores fold mae rmse mape* max_error fold_0 0.1974 0.6847 0.0700 13.6699 fold_1 0.3552 1.4949 0.1212 23.3832 fold_2 0.4551 3.1409 0.1035 58.7285 fold_3 0.3164 2.2413 0.0725 51.3719 fold_4 0.4005 2.2637 0.1258 36.5440 Fold score stats metric mean max min std mae 0.3449 0.4551 0.1974 0.0871 rmse 1.9651 3.1409 0.6847 0.8257 mape* 0.0986 0.1258 0.0700 0.0236 max_error 36.7395 58.7285 13.6699 16.7825 Fold parameters fold params dict fold_0 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_1 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_2 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_3 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_4 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... matbench_jdft2d Fold scores fold mae rmse mape* max_error fold_0 37.0106 85.2042 23.4970 649.7056 fold_1 45.3752 121.0872 0.4163 767.6072 fold_2 58.3624 172.0326 0.6282 973.0735 fold_3 31.9625 55.3213 0.3025 275.8517 fold_4 44.4110 153.4614 0.5874 1519.7424 Fold score stats metric mean max min std mae 43.4244 58.3624 31.9625 8.9491 rmse 117.4213 172.0326 55.3213 42.8697 mape* 5.0863 23.4970 0.3025 9.2061 max_error 837.1961 1519.7424 275.8517 409.7401 Fold parameters fold params dict fold_0 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_1 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_2 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_3 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_4 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... matbench_log_gvrh Fold scores fold mae rmse mape* max_error fold_0 0.0709 0.1093 0.0560 0.9401 fold_1 0.0725 0.1167 0.0587 1.1324 fold_2 0.0712 0.1122 0.0566 0.7799 fold_3 0.0710 0.1102 0.0558 0.8718 fold_4 0.0719 0.1133 0.0563 0.7814 Fold score stats metric mean max min std mae 0.0715 0.0725 0.0709 0.0006 rmse 0.1123 0.1167 0.1093 0.0026 mape* 0.0567 0.0587 0.0558 0.0011 max_error 0.9011 1.1324 0.7799 0.1303 Fold parameters fold params dict fold_0 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_1 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_2 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_3 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_4 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... matbench_log_kvrh Fold scores fold mae rmse mape* max_error fold_0 0.0552 0.1062 0.0361 1.6438 fold_1 0.0572 0.1158 0.0375 1.3470 fold_2 0.0531 0.1017 0.0351 1.1254 fold_3 0.0615 0.1187 0.0442 1.1145 fold_4 0.0569 0.1105 0.0379 1.3937 Fold score stats metric mean max min std mae 0.0568 0.0615 0.0531 0.0028 rmse 0.1106 0.1187 0.1017 0.0062 mape* 0.0382 0.0442 0.0351 0.0032 max_error 1.3249 1.6438 1.1145 0.1955 Fold parameters fold params dict fold_0 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_1 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_2 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_3 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_4 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... matbench_mp_e_form Fold scores fold mae rmse mape* max_error fold_0 0.0218 0.0647 0.1437 3.5487 fold_1 0.0220 0.0534 0.1299 2.9160 fold_2 0.0209 0.0494 0.1434 2.1189 fold_3 0.0219 0.0546 0.1762 1.6654 fold_4 0.0210 0.0499 0.2528 1.4116 Fold score stats metric mean max min std mae 0.0215 0.0220 0.0209 0.0005 rmse 0.0544 0.0647 0.0494 0.0055 mape* 0.1692 0.2528 0.1299 0.0445 max_error 2.3321 3.5487 1.4116 0.7948 Fold parameters fold params dict fold_0 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_1 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_2 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_3 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_4 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... matbench_mp_gap Fold scores fold mae rmse mape* max_error fold_0 0.1860 0.4624 2.3206 6.6263 fold_1 0.1852 0.4622 2.5314 7.4756 fold_2 0.1901 0.4729 4.2501 6.2931 fold_3 0.1812 0.4497 5.1591 6.9986 fold_4 0.1880 0.4703 4.8122 6.4513 Fold score stats metric mean max min std mae 0.1861 0.1901 0.1812 0.0030 rmse 0.4635 0.4729 0.4497 0.0081 mape* 3.8147 5.1591 2.3206 1.1723 max_error 6.7690 7.4756 6.2931 0.4242 Fold parameters fold params dict fold_0 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_1 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_2 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_3 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_4 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... matbench_mp_is_metal Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.9173 0.9156 0.9047 0.9156 fold_1 0.9135 0.9117 0.9003 0.9117 fold_2 0.9146 0.9125 0.9013 0.9125 fold_3 0.9135 0.9117 0.9003 0.9117 fold_4 0.9147 0.9123 0.9012 0.9123 Fold score stats metric mean max min std accuracy 0.9147 0.9173 0.9135 0.0014 balanced_accuracy 0.9128 0.9156 0.9117 0.0015 f1 0.9015 0.9047 0.9003 0.0016 rocauc 0.9128 0.9156 0.9117 0.0015 Fold parameters fold params dict fold_0 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_1 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_2 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_3 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_4 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... matbench_perovskites Fold scores fold mae rmse mape* max_error fold_0 0.0293 0.0577 0.0292 0.8306 fold_1 0.0301 0.0622 0.0291 0.9028 fold_2 0.0276 0.0509 0.0274 0.8358 fold_3 0.0286 0.0532 0.0276 0.7984 fold_4 0.0282 0.0558 0.0253 0.8666 Fold score stats metric mean max min std mae 0.0288 0.0301 0.0276 0.0009 rmse 0.0559 0.0622 0.0509 0.0039 mape* 0.0277 0.0292 0.0253 0.0014 max_error 0.8468 0.9028 0.7984 0.0354 Fold parameters fold params dict fold_0 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_1 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_2 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_3 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_4 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... matbench_phonons Fold scores fold mae rmse mape* max_error fold_0 33.4099 57.5251 0.0623 394.6285 fold_1 28.0306 44.1719 0.0571 277.0146 fold_2 29.4772 53.9163 0.0575 300.2450 fold_3 29.4931 62.0127 0.0571 615.3466 fold_4 27.2814 49.8790 0.0521 528.2511 Fold score stats metric mean max min std mae 29.5385 33.4099 27.2814 2.1148 rmse 53.5010 62.0127 44.1719 6.1476 mape* 0.0572 0.0623 0.0521 0.0032 max_error 423.0972 615.3466 277.0146 130.5836 Fold parameters fold params dict fold_0 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_1 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_2 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_3 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_4 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_...","title":"matbench_v0.1: ALIGNN"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#matbench_v01-alignn","text":"","title":"matbench_v0.1: ALIGNN"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#algorithm-description","text":"The Atomistic Line Graph Neural Network (https://doi.org/10.1038/s41524-021-00650-1) introduces a new graph convolution layer that explicitly models both two and three body interactions in atomistic systems. This is achieved by composing two edge-gated graph convolution layers, the first applied to the atomistic line graph L(g) (representing triplet interactions) and the second applied to the atomistic bond graph g (representing pair interactions). The atomistic graph g consists of a node for each atom i (with atom/node representations hi), and one edge for each atom pair within a cutoff radius (with bond/pair representations eij). The atomistic line graph L(g) represents relationships between atom triplets: it has nodes corresponding to bonds (sharing representations eij with those in g) and edges corresponding to bond angles (with angle/triplet representations tijk).The line graph convolution updates the triplet representations and the pair representations; the direct graph convolution further updates the pair representations and the atom representations.","title":"Algorithm description:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#notes","text":"None Raw data download and example notebook available on the matbench repo .","title":"Notes:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#references-in-bibtex-format","text":"('@article{choudhary2021atomistic,title={Atomistic Line Graph Neural Network ' 'for improved materials property predictions},author={Choudhary, Kamal and ' 'DeCost, Brian},journal={npj Computational ' 'Materials},volume={7},number={1},pages={1--8},year={2021},publisher={Nature ' 'Publishing Group}}')","title":"References (in bibtex format):"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#user-metadata","text":"{'algorithm': 'ALIGNN'}","title":"User metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#metadata","text":"tasks recorded 9/13 complete? \u2717 composition complete? \u2717 structure complete? \u2713 regression complete? \u2717 classification complete? \u2717","title":"Metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#software-requirements","text":"{'python': ['absl-py==1.0.0', 'alignn==2021.12.27', 'astunparse==1.6.3', 'attrs==21.3.0', 'black==21.12b0', 'cachetools==4.2.4', 'certifi==2021.10.8', 'charset-normalizer==2.0.9', 'click==8.0.3', 'cloudpickle==2.0.0', 'cycler==0.11.0', 'decorator==5.1.0', 'dgl==0.6.1', 'dgl-cu111==0.6.1', 'dm-tree==0.1.6', 'flake8==4.0.1', 'flatbuffers==2.0', 'fonttools==4.28.5', 'future==0.18.2', 'gast==0.4.0', 'google-auth==2.3.3', 'google-auth-oauthlib==0.4.6', 'google-pasta==0.2.0', 'grpcio==1.43.0', 'h5py==3.6.0', 'idna==3.3', 'importlib-metadata==4.10.0', 'importlib-resources==5.4.0', 'jarvis-tools==2021.12.16', 'joblib==1.1.0', 'jsonschema==4.3.2', 'julia==0.5.6', 'keras==2.7.0', 'Keras-Preprocessing==1.1.2', 'kiwisolver==1.3.2', 'libclang==12.0.0', 'Markdown==3.3.6', 'matbench==0.5', 'matminer==0.6.5', 'matplotlib==3.5.1', 'mccabe==0.6.1', 'modnet==0.1.11', 'monty==2021.8.17', 'mpmath==1.2.1', 'mypy-extensions==0.4.3', 'networkx==2.6.3', 'numpy==1.21.5', 'oauthlib==3.1.1', 'opt-einsum==3.3.0', 'packaging==21.3', 'palettable==3.3.0', 'pandas==1.3.5', 'pathspec==0.9.0', 'Pillow==8.4.0', 'Pint==0.18', 'platformdirs==2.4.0', 'plotly==5.5.0', 'protobuf==3.19.1', 'pyasn1==0.4.8', 'pyasn1-modules==0.2.8', 'pycodestyle==2.8.0', 'pydantic==1.8.2', 'pydocstyle==6.1.1', 'pyflakes==2.4.0', 'pymatgen==2020.8.13', 'pymongo==4.0.1', 'pyparsing==2.4.7', 'pyrsistent==0.18.0', 'python-dateutil==2.8.2', 'pytorch-ignite==0.4.7', 'pytz==2021.3', 'requests==2.26.0', 'requests-oauthlib==1.3.0', 'rsa==4.8', 'ruamel.yaml==0.17.19', 'ruamel.yaml.clib==0.2.6', 'scikit-learn==0.23.2', 'scipy==1.7.3', 'six==1.16.0', 'snowballstemmer==2.2.0', 'spglib==1.16.3', 'sympy==1.9', 'tabulate==0.8.9', 'tenacity==8.0.1', 'tensorboard==2.7.0', 'tensorboard-data-server==0.6.1', 'tensorboard-plugin-wit==1.8.0', 'tensorflow==2.7.0', 'tensorflow-estimator==2.7.0', 'tensorflow-io-gcs-filesystem==0.23.1', 'tensorflow-probability==0.15.0', 'termcolor==1.1.0', 'threadpoolctl==3.0.0', 'tomli==1.2.3', 'toolz==0.11.2', 'torch==1.10.1', 'tqdm==4.62.3', 'typing_extensions==4.0.1', 'uncertainties==3.1.6', 'urllib3==1.26.7', 'Werkzeug==2.0.2', 'wrapt==1.13.3', 'xmltodict==0.12.0', 'zipp==3.6.0']}","title":"Software Requirements"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#task-data","text":"","title":"Task data:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#matbench_dielectric","text":"","title":"matbench_dielectric"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-scores","text":"fold mae rmse mape* max_error fold_0 0.1974 0.6847 0.0700 13.6699 fold_1 0.3552 1.4949 0.1212 23.3832 fold_2 0.4551 3.1409 0.1035 58.7285 fold_3 0.3164 2.2413 0.0725 51.3719 fold_4 0.4005 2.2637 0.1258 36.5440","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-score-stats","text":"metric mean max min std mae 0.3449 0.4551 0.1974 0.0871 rmse 1.9651 3.1409 0.6847 0.8257 mape* 0.0986 0.1258 0.0700 0.0236 max_error 36.7395 58.7285 13.6699 16.7825","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-parameters","text":"fold params dict fold_0 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_1 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_2 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_3 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_4 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#matbench_jdft2d","text":"","title":"matbench_jdft2d"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-scores_1","text":"fold mae rmse mape* max_error fold_0 37.0106 85.2042 23.4970 649.7056 fold_1 45.3752 121.0872 0.4163 767.6072 fold_2 58.3624 172.0326 0.6282 973.0735 fold_3 31.9625 55.3213 0.3025 275.8517 fold_4 44.4110 153.4614 0.5874 1519.7424","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-score-stats_1","text":"metric mean max min std mae 43.4244 58.3624 31.9625 8.9491 rmse 117.4213 172.0326 55.3213 42.8697 mape* 5.0863 23.4970 0.3025 9.2061 max_error 837.1961 1519.7424 275.8517 409.7401","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-parameters_1","text":"fold params dict fold_0 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_1 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_2 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_3 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_4 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#matbench_log_gvrh","text":"","title":"matbench_log_gvrh"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-scores_2","text":"fold mae rmse mape* max_error fold_0 0.0709 0.1093 0.0560 0.9401 fold_1 0.0725 0.1167 0.0587 1.1324 fold_2 0.0712 0.1122 0.0566 0.7799 fold_3 0.0710 0.1102 0.0558 0.8718 fold_4 0.0719 0.1133 0.0563 0.7814","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-score-stats_2","text":"metric mean max min std mae 0.0715 0.0725 0.0709 0.0006 rmse 0.1123 0.1167 0.1093 0.0026 mape* 0.0567 0.0587 0.0558 0.0011 max_error 0.9011 1.1324 0.7799 0.1303","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-parameters_2","text":"fold params dict fold_0 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_1 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_2 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_3 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_4 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#matbench_log_kvrh","text":"","title":"matbench_log_kvrh"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-scores_3","text":"fold mae rmse mape* max_error fold_0 0.0552 0.1062 0.0361 1.6438 fold_1 0.0572 0.1158 0.0375 1.3470 fold_2 0.0531 0.1017 0.0351 1.1254 fold_3 0.0615 0.1187 0.0442 1.1145 fold_4 0.0569 0.1105 0.0379 1.3937","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-score-stats_3","text":"metric mean max min std mae 0.0568 0.0615 0.0531 0.0028 rmse 0.1106 0.1187 0.1017 0.0062 mape* 0.0382 0.0442 0.0351 0.0032 max_error 1.3249 1.6438 1.1145 0.1955","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-parameters_3","text":"fold params dict fold_0 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_1 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_2 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_3 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_4 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#matbench_mp_e_form","text":"","title":"matbench_mp_e_form"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-scores_4","text":"fold mae rmse mape* max_error fold_0 0.0218 0.0647 0.1437 3.5487 fold_1 0.0220 0.0534 0.1299 2.9160 fold_2 0.0209 0.0494 0.1434 2.1189 fold_3 0.0219 0.0546 0.1762 1.6654 fold_4 0.0210 0.0499 0.2528 1.4116","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-score-stats_4","text":"metric mean max min std mae 0.0215 0.0220 0.0209 0.0005 rmse 0.0544 0.0647 0.0494 0.0055 mape* 0.1692 0.2528 0.1299 0.0445 max_error 2.3321 3.5487 1.4116 0.7948","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-parameters_4","text":"fold params dict fold_0 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_1 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_2 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_3 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_4 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#matbench_mp_gap","text":"","title":"matbench_mp_gap"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-scores_5","text":"fold mae rmse mape* max_error fold_0 0.1860 0.4624 2.3206 6.6263 fold_1 0.1852 0.4622 2.5314 7.4756 fold_2 0.1901 0.4729 4.2501 6.2931 fold_3 0.1812 0.4497 5.1591 6.9986 fold_4 0.1880 0.4703 4.8122 6.4513","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-score-stats_5","text":"metric mean max min std mae 0.1861 0.1901 0.1812 0.0030 rmse 0.4635 0.4729 0.4497 0.0081 mape* 3.8147 5.1591 2.3206 1.1723 max_error 6.7690 7.4756 6.2931 0.4242","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-parameters_5","text":"fold params dict fold_0 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_1 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_2 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_3 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_4 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#matbench_mp_is_metal","text":"","title":"matbench_mp_is_metal"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-scores_6","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.9173 0.9156 0.9047 0.9156 fold_1 0.9135 0.9117 0.9003 0.9117 fold_2 0.9146 0.9125 0.9013 0.9125 fold_3 0.9135 0.9117 0.9003 0.9117 fold_4 0.9147 0.9123 0.9012 0.9123","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-score-stats_6","text":"metric mean max min std accuracy 0.9147 0.9173 0.9135 0.0014 balanced_accuracy 0.9128 0.9156 0.9117 0.0015 f1 0.9015 0.9047 0.9003 0.0016 rocauc 0.9128 0.9156 0.9117 0.0015","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-parameters_6","text":"fold params dict fold_0 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_1 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_2 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_3 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_4 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#matbench_perovskites","text":"","title":"matbench_perovskites"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-scores_7","text":"fold mae rmse mape* max_error fold_0 0.0293 0.0577 0.0292 0.8306 fold_1 0.0301 0.0622 0.0291 0.9028 fold_2 0.0276 0.0509 0.0274 0.8358 fold_3 0.0286 0.0532 0.0276 0.7984 fold_4 0.0282 0.0558 0.0253 0.8666","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-score-stats_7","text":"metric mean max min std mae 0.0288 0.0301 0.0276 0.0009 rmse 0.0559 0.0622 0.0509 0.0039 mape* 0.0277 0.0292 0.0253 0.0014 max_error 0.8468 0.9028 0.7984 0.0354","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-parameters_7","text":"fold params dict fold_0 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_1 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_2 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_3 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_4 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#matbench_phonons","text":"","title":"matbench_phonons"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-scores_8","text":"fold mae rmse mape* max_error fold_0 33.4099 57.5251 0.0623 394.6285 fold_1 28.0306 44.1719 0.0571 277.0146 fold_2 29.4772 53.9163 0.0575 300.2450 fold_3 29.4931 62.0127 0.0571 615.3466 fold_4 27.2814 49.8790 0.0521 528.2511","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-score-stats_8","text":"metric mean max min std mae 29.5385 33.4099 27.2814 2.1148 rmse 53.5010 62.0127 44.1719 6.1476 mape* 0.0572 0.0623 0.0521 0.0032 max_error 423.0972 615.3466 277.0146 130.5836","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_alignn/#fold-parameters_8","text":"fold params dict fold_0 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_1 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_2 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_3 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_... fold_4 {'atom_features': 'cgcnn', 'batch_size': 2, 'classification_threshold': None, 'criterion': 'mse', 'cutoff': 8.0, 'dataset': 'user_data', 'epochs': 3, 'filename': 'sample', 'id_tag': 'jid', 'keep_data_...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/","text":"matbench_v0.1: AMMExpress v2020 Algorithm description: Automatminer express v1.03.20200727. Based on automatic featurization, tree-based feature reduction, and genetic-algorithm based AutoML with the TPOT package. Notes: All data was generated using the same config (express, default). The automatminer version requirement specifies the versions of many dependent packages, such as matminer, which are required for the algorithm to work in your virtualenv. Raw data download and example notebook available on the matbench repo . References (in bibtex format): ('@article{Dunn2020,\\n' ' doi = {10.1038/s41524-020-00406-3},\\n' ' url = {https://doi.org/10.1038/s41524-020-00406-3},\\n' ' year = {2020},\\n' ' month = sep,\\n' ' publisher = {Springer Science and Business Media {LLC}},\\n' ' volume = {6},\\n' ' number = {1},\\n' ' author = {Alexander Dunn and Qi Wang and Alex Ganose and Daniel Dopp and ' 'Anubhav Jain},\\n' ' title = {Benchmarking materials property prediction methods: the Matbench ' 'test set and Automatminer reference algorithm},\\n' ' journal = {npj Computational Materials}\\n' '}') User metadata: {'autofeaturizer_kwargs': {'n_jobs': 10, 'preset': 'express'}, 'cleaner_kwargs': {'feature_na_method': 'drop', 'max_na_frac': 0.1, 'na_method_fit': 'mean', 'na_method_transform': 'mean'}, 'learner_kwargs': {'max_eval_time_mins': 20, 'max_time_mins': 1440, 'memory': 'auto', 'n_jobs': 10, 'population_size': 200}, 'learner_name': 'TPOTAdaptor', 'reducer_kwargs': {'reducers': ['corr', 'tree'], 'tree_importance_percentile': 0.99}} Metadata: tasks recorded 13/13 complete? \u2713 composition complete? \u2713 structure complete? \u2713 regression complete? \u2713 classification complete? \u2713 Software Requirements {'python': ['automatminer==1.0.3.20200727', 'matbench==0.1.0']} Task data: matbench_dielectric Fold scores fold mae rmse mape* max_error fold_0 0.2188 0.6855 0.0760 14.6654 fold_1 0.2844 1.0764 0.0899 19.6283 fold_2 0.4257 2.9472 0.0889 59.0112 fold_3 0.3198 2.2782 0.0720 53.5196 fold_4 0.3264 1.6137 0.0987 28.1601 Fold score stats metric mean max min std mae 0.3150 0.4257 0.2188 0.0672 rmse 1.7202 2.9472 0.6855 0.8140 mape* 0.0851 0.0987 0.0720 0.0098 max_error 34.9969 59.0112 14.6654 17.9782 Fold parameters fold params dict fold_0 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.006, score_func=<function f_regression at 0x2aaaef1a0840>))', '(robustscaler, RobustScaler(copy=true, quantile_range=(25.0, 75.0), with_centering=true... fold_1 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.0001))', '(zerocount, ZeroCount())', '(gradientboostingregressor, GradientBoostingRegressor(alpha=0.75, criterion=friedman_mse, in... fold_2 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.001))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(gradientboostingregressor, GradientBoostingRegressor(al... fold_3 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.023, score_func=<function f_regression at 0x2aaaef19f950>))', '(standardscaler, StandardScaler(copy=true, with_mean=true, with_std=true))', '(gradient... fold_4 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.034, score_func=<function f_regression at 0x2aaaf35a08c8>))', '(zerocount, ZeroCount())', '(gradientboostingregressor, GradientBoostingRegressor(alpha... matbench_expt_gap Fold scores fold mae rmse mape* max_error fold_0 0.3998 0.9435 0.3372 8.0111 fold_1 0.4061 0.9354 0.3085 8.6887 fold_2 0.4538 1.0955 0.3916 12.7533 fold_3 0.4061 1.0273 0.3019 12.6296 fold_4 0.4150 0.9573 0.4503 6.0779 Fold score stats metric mean max min std mae 0.4161 0.4538 0.3998 0.0194 rmse 0.9918 1.0955 0.9354 0.0612 mape* 0.3579 0.4503 0.3019 0.0560 max_error 9.6321 12.7533 6.0779 2.6411 Fold parameters fold params dict fold_0 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.035, score_func=<function f_regression at 0x2aaaf35a18c8>))', '(standardscaler, StandardScaler(copy=true, with_mean=true, with_std=true))', '(gradient... fold_1 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.046, score_func=<function f_regression at 0x2aaaef19f8c8>))', '(onehotencoder, OneHotEncoder(categorical_features=[false, false, false, false, false, ... fold_2 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.0005))', '(robustscaler, RobustScaler(copy=true, quantile_range=(25.0, 75.0), with_centering=true,\\n with_scaling=true... fold_3 {'best_pipeline': ['(selectpercentile, SelectPercentile(percentile=85,\\n score_func=<function f_regression at 0x2aaaf39a38c8>))', '(onehotencoder, OneHotEncoder(categorical_features=[f... fold_4 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.0005))', '(maxabsscaler, MaxAbsScaler(copy=true))', '(randomforestregressor, RandomForestRegressor(bootstrap=false, criterion=mse,... matbench_expt_is_metal Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.9218 0.9218 0.9205 0.9218 fold_1 0.9157 0.9156 0.9145 0.9156 fold_2 0.9207 0.9207 0.9193 0.9207 fold_3 0.9228 0.9228 0.9223 0.9228 fold_4 0.9238 0.9238 0.9235 0.9238 Fold score stats metric mean max min std accuracy 0.9210 0.9238 0.9157 0.0028 balanced_accuracy 0.9209 0.9238 0.9156 0.0028 f1 0.9200 0.9235 0.9145 0.0031 rocauc 0.9209 0.9238 0.9156 0.0028 Fold parameters fold params dict fold_0 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.009000000000000001,\\n score_func=<function f_classif at 0x2aaaf35a16a8>))', '(onehotencoder, OneHotEncoder(categorical_features=[false, false... fold_1 {'best_pipeline': ['(rfe, RFE(estimator=ExtraTreesClassifier(bootstrap=false, class_weight=null,\\n criterion=gini, max_depth=null,\\n ... fold_2 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.001))', '(maxabsscaler, MaxAbsScaler(copy=true))', '(gradientboostingclassifier, GradientBoostingClassifier(criterion=friedman_mse... fold_3 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.03, score_func=<function f_classif at 0x2aaaf35a0730>))', '(maxabsscaler, MaxAbsScaler(copy=true))', '(gradientboostingclassifier, GradientBoostingCla... fold_4 {'best_pipeline': ['(rfe, RFE(estimator=ExtraTreesClassifier(bootstrap=false, class_weight=null,\\n criterion=entropy, max_depth=null,\\n ... matbench_glass Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.8283 0.8441 0.8697 0.8441 fold_1 0.8125 0.8383 0.8548 0.8383 fold_2 0.8574 0.8546 0.8956 0.8546 fold_3 0.9173 0.8742 0.9437 0.8742 fold_4 0.9375 0.8921 0.9579 0.8921 Fold score stats metric mean max min std accuracy 0.8706 0.9375 0.8125 0.0490 balanced_accuracy 0.8607 0.8921 0.8383 0.0199 f1 0.9043 0.9579 0.8548 0.0404 rocauc 0.8607 0.8921 0.8383 0.0199 Fold parameters fold params dict fold_0 {'best_pipeline': ['(selectfrommodel, SelectFromModel(estimator=ExtraTreesClassifier(bootstrap=false,\\n class_weight=null,\\n ... fold_1 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.0001))', '(standardscaler, StandardScaler(copy=true, with_mean=true, with_std=true))', '(extratreesclassifier, ExtraTreesClassifie... fold_2 {'best_pipeline': ['(selectpercentile, SelectPercentile(percentile=74,\\n score_func=<function f_classif at 0x2aaaf35a0730>))', '(onehotencoder, OneHotEncoder(categorical_features=[fals... fold_3 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.0001))', '(standardscaler, StandardScaler(copy=true, with_mean=true, with_std=true))', '(gradientboostingclassifier, GradientBoost... fold_4 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.0001))', '(standardscaler, StandardScaler(copy=true, with_mean=true, with_std=true))', '(gradientboostingclassifier, GradientBoost... matbench_jdft2d Fold scores fold mae rmse mape* max_error fold_0 29.5070 57.7719 18.9726 362.2752 fold_1 44.3036 98.1137 0.3191 551.7742 fold_2 54.4690 164.0162 0.5117 847.0618 fold_3 28.0759 55.8345 0.2371 316.2185 fold_4 42.8931 156.9938 0.5429 1552.9102 Fold score stats metric mean max min std mae 39.8497 54.4690 28.0759 9.8835 rmse 106.5460 164.0162 55.8345 46.6251 mape* 4.1167 18.9726 0.2371 7.4289 max_error 726.0480 1552.9102 316.2185 453.6535 Fold parameters fold params dict fold_0 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.1))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(gradientboostingregressor, GradientBoostingRegressor(alph... fold_1 {'best_pipeline': ['(selectpercentile, SelectPercentile(percentile=40,\\n score_func=<function f_regression at 0x2aaaf35a08c8>))', '(maxabsscaler, MaxAbsScaler(copy=true))', '(gradientb... fold_2 {'best_pipeline': ['(selectpercentile, SelectPercentile(percentile=62,\\n score_func=<function f_regression at 0x2aaaf35a08c8>))', '(onehotencoder, OneHotEncoder(categorical_features=[f... fold_3 {'best_pipeline': ['(selectpercentile, SelectPercentile(percentile=82,\\n score_func=<function f_regression at 0x2aab561f6620>))', '(robustscaler, RobustScaler(copy=true, quantile_range... fold_4 {'best_pipeline': ['(selectpercentile, SelectPercentile(percentile=62,\\n score_func=<function f_regression at 0x2aaaf35a08c8>))', '(zerocount, ZeroCount())', '(gradientboostingregresso... matbench_log_gvrh Fold scores fold mae rmse mape* max_error fold_0 0.0891 0.1270 0.0692 1.1580 fold_1 0.0852 0.1261 0.0666 1.0887 fold_2 0.0849 0.1261 0.0668 0.9631 fold_3 0.0884 0.1279 0.0670 0.8959 fold_4 0.0894 0.1313 0.0690 0.9810 Fold score stats metric mean max min std mae 0.0874 0.0894 0.0849 0.0020 rmse 0.1277 0.1313 0.1261 0.0019 mape* 0.0677 0.0692 0.0666 0.0012 max_error 1.0173 1.1580 0.8959 0.0937 Fold parameters fold params dict fold_0 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.2))', '(zerocount, ZeroCount())', '(gradientboostingregressor, GradientBoostingRegressor(alpha=0.99, criterion=friedman_mse, init=... fold_1 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.01))', '(robustscaler, RobustScaler(copy=true, quantile_range=(25.0, 75.0), with_centering=true,\\n with_scaling=true))... fold_2 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.01, score_func=<function f_regression at 0x2aaaef19e8c8>))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(randomforestregressor,... fold_3 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.0001))', '(standardscaler, StandardScaler(copy=true, with_mean=true, with_std=true))', '(gradientboostingregressor, GradientBoosti... fold_4 {'best_pipeline': ['(selectpercentile, SelectPercentile(percentile=96,\\n score_func=<function f_regression at 0x2aaaf35a08c8>))', '(maxabsscaler, MaxAbsScaler(copy=true))', '(extratree... matbench_log_kvrh Fold scores fold mae rmse mape* max_error fold_0 0.0639 0.1179 0.0417 1.4823 fold_1 0.0659 0.1231 0.0432 1.2686 fold_2 0.0627 0.1115 0.0411 1.1316 fold_3 0.0668 0.1217 0.0464 1.1890 fold_4 0.0640 0.1172 0.0417 1.4335 Fold score stats metric mean max min std mae 0.0647 0.0668 0.0627 0.0015 rmse 0.1183 0.1231 0.1115 0.0041 mape* 0.0428 0.0464 0.0411 0.0019 max_error 1.3010 1.4823 1.1316 0.1362 Fold parameters fold params dict fold_0 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.032, score_func=<function f_regression at 0x2aaaf35a2840>))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(extratreesregressor, ... fold_1 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.029, score_func=<function f_regression at 0x2aaaf35a08c8>))', '(zerocount, ZeroCount())', '(gradientboostingregressor, GradientBoostingRegressor(alpha... fold_2 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.2))', '(onehotencoder, OneHotEncoder(categorical_features=[false, false, false, false, false, false,\\n ... fold_3 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.016, score_func=<function f_regression at 0x2aaaf79a28c8>))', '(onehotencoder, OneHotEncoder(categorical_features=[false, false, false, false, false, ... fold_4 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.0001))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(extratreesregressor, ExtraTreesRegressor(bootstrap=fal... matbench_mp_e_form Fold scores fold mae rmse mape* max_error fold_0 0.1586 0.2508 1.0829 4.0713 fold_1 0.2026 0.2955 0.9253 5.8108 fold_2 0.1473 0.2256 0.7722 2.7696 fold_3 0.2080 0.3062 1.3958 5.5190 fold_4 0.1467 0.2226 0.8028 3.3888 Fold score stats metric mean max min std mae 0.1726 0.2080 0.1467 0.0270 rmse 0.2602 0.3062 0.2226 0.0348 mape* 0.9958 1.3958 0.7722 0.2280 max_error 4.3119 5.8108 2.7696 1.1826 Fold parameters fold params dict fold_0 {'best_pipeline': ['(gradientboostingregressor, GradientBoostingRegressor(alpha=0.75, criterion=friedman_mse, init=null,\\n learning_rate=0.5, loss=huber, max_depth=5,\\n max_fea... fold_1 {'best_pipeline': ['(polynomialfeatures, PolynomialFeatures(degree=2, include_bias=false, interaction_only=false))', '(pca, PCA(copy=true, iterated_power=3, n_components=null, random_state=null,\\n sv... fold_2 {'best_pipeline': ['(stackingestimator, StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.9, criterion=friedman_mse, init=null,\\n learning_rate=0.5, loss=huber, max_depth=4,\\n ... fold_3 {'best_pipeline': ['(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(selectfwe, SelectFwe(alpha=0.027, score_func=<function f_regression at 0x2b2eb18422f0>))', '(stackingestimator, St... fold_4 {'best_pipeline': ['(xgbregressor, XGBRegressor(base_score=0.5, booster=gbtree, colsample_bylevel=1,\\n colsample_bytree=1, gamma=0, learning_rate=0.5, max_delta_step=0,\\n max_depth=5, min_... matbench_mp_gap Fold scores fold mae rmse mape* max_error fold_0 0.2799 0.5481 3.5712 5.4792 fold_1 0.2850 0.5671 3.1533 6.9105 fold_2 0.2724 0.5477 4.6097 6.2045 fold_3 0.2909 0.5710 10.0191 6.4590 fold_4 0.2837 0.5714 6.8322 5.5333 Fold score stats metric mean max min std mae 0.2824 0.2909 0.2724 0.0061 rmse 0.5611 0.5714 0.5477 0.0109 mape* 5.6371 10.0191 3.1533 2.5347 max_error 6.1173 6.9105 5.4792 0.5480 Fold parameters fold params dict fold_0 {'best_pipeline': ['(stackingestimator-1, StackingEstimator(estimator=RandomForestRegressor(bootstrap=false, criterion=mse, max_depth=null,\\n max_features=0.4, max_leaf_nodes=null,\\n ... fold_1 {'best_pipeline': ['(stackingestimator-1, StackingEstimator(estimator=RandomForestRegressor(bootstrap=true, criterion=mse, max_depth=null,\\n max_features=0.35000000000000003, max_leaf_nodes=... fold_2 {'best_pipeline': ['(stackingestimator-1, StackingEstimator(estimator=RandomForestRegressor(bootstrap=false, criterion=mse, max_depth=null,\\n max_features=0.45, max_leaf_nodes=null,\\n ... fold_3 {'best_pipeline': ['(stackingestimator-1, StackingEstimator(estimator=ExtraTreesRegressor(bootstrap=false, criterion=mse, max_depth=null,\\n max_features=0.45, max_leaf_nodes=null,\\n ... fold_4 {'best_pipeline': ['(stackingestimator-1, StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.85, criterion=friedman_mse, init=null,\\n learning_rate=0.01, loss=lad, max_depth=1,\\... matbench_mp_is_metal Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.9133 0.9094 0.8982 0.9094 fold_1 0.9123 0.9086 0.8972 0.9086 fold_2 0.9129 0.9089 0.8976 0.9089 fold_3 0.9146 0.9108 0.8998 0.9108 fold_4 0.9129 0.9086 0.8974 0.9086 Fold score stats metric mean max min std accuracy 0.9132 0.9146 0.9123 0.0008 balanced_accuracy 0.9093 0.9108 0.9086 0.0008 f1 0.8981 0.8998 0.8972 0.0009 rocauc 0.9093 0.9108 0.9086 0.0008 Fold parameters fold params dict fold_0 {'best_pipeline': ['(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(randomforestclassifier, RandomForestClassifier(bootstrap=false, class_weight=null,\\n criterion=entropy,... fold_1 {'best_pipeline': ['(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(randomforestclassifier, RandomForestClassifier(bootstrap=false, class_weight=null,\\n criterion=entropy,... fold_2 {'best_pipeline': ['(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(randomforestclassifier, RandomForestClassifier(bootstrap=false, class_weight=null,\\n criterion=entropy,... fold_3 {'best_pipeline': ['(stackingestimator, StackingEstimator(estimator=RandomForestClassifier(bootstrap=false, class_weight=null,\\n criterion=entropy, max_depth=null, max_features=0.5,\\n ... fold_4 {'best_pipeline': ['(featureunion, FeatureUnion(n_jobs=null,\\n transformer_list=[(functiontransformer, FunctionTransformer(accept_sparse=false, check_inverse=true,\\n func=<function copy... matbench_perovskites Fold scores fold mae rmse mape* max_error fold_0 0.2159 0.3114 0.2077 2.7651 fold_1 0.1904 0.2857 0.1944 2.6783 fold_2 0.1962 0.2869 0.1933 2.4466 fold_3 0.1992 0.2907 0.2209 3.3116 fold_4 0.2006 0.3023 0.1886 2.4386 Fold score stats metric mean max min std mae 0.2005 0.2159 0.1904 0.0085 rmse 0.2954 0.3114 0.2857 0.0099 mape* 0.2010 0.2209 0.1886 0.0118 max_error 2.7280 3.3116 2.4386 0.3186 Fold parameters fold params dict fold_0 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.1))', '(robustscaler, RobustScaler(copy=true, quantile_range=(25.0, 75.0), with_centering=true,\\n with_scaling=true))'... fold_1 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.1))', '(zerocount, ZeroCount())', '(randomforestregressor, RandomForestRegressor(bootstrap=false, criterion=mse, max_depth=null,\\n... fold_2 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.03, score_func=<function f_regression at 0x2aaaf35a08c8>))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(gradientboostingregres... fold_3 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.1))', '(robustscaler, RobustScaler(copy=true, quantile_range=(25.0, 75.0), with_centering=true,\\n with_scaling=true))'... fold_4 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.05))', '(maxabsscaler, MaxAbsScaler(copy=true))', '(randomforestregressor, RandomForestRegressor(bootstrap=false, criterion=mse, m... matbench_phonons Fold scores fold mae rmse mape* max_error fold_0 67.5727 146.7970 0.1079 1151.5570 fold_1 54.0755 100.2097 0.1048 890.4159 fold_2 50.9853 96.5991 0.0931 680.9361 fold_3 59.6458 127.8555 0.1142 926.0969 fold_4 48.5738 77.0626 0.0958 383.1912 Fold score stats metric mean max min std mae 56.1706 67.5727 48.5738 6.7981 rmse 109.7048 146.7970 77.0626 24.6280 mape* 0.1032 0.1142 0.0931 0.0078 max_error 806.4394 1151.5570 383.1912 258.9850 Fold parameters fold params dict fold_0 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.01))', '(robustscaler, RobustScaler(copy=true, quantile_range=(25.0, 75.0), with_centering=true,\\n with_scaling=true))... fold_1 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.005))', '(maxabsscaler, MaxAbsScaler(copy=true))', '(gradientboostingregressor, GradientBoostingRegressor(alpha=0.8, criterion=fri... fold_2 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.1))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(extratreesregressor, ExtraTreesRegressor(bootstrap=false,... fold_3 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.0001))', '(onehotencoder, OneHotEncoder(categorical_features=[false, false, false, false, false, false,\\n ... fold_4 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.2))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(extratreesregressor, ExtraTreesRegressor(bootstrap=false,... matbench_steels Fold scores fold mae rmse mape* max_error fold_0 109.3058 188.8049 0.0693 1082.7703 fold_1 80.4188 109.2771 0.0569 416.3620 fold_2 83.5360 120.2935 0.0607 424.5913 fold_3 98.7186 136.5898 0.0722 473.4563 fold_4 115.4851 215.1149 0.0891 1142.9223 Fold score stats metric mean max min std mae 97.4929 115.4851 80.4188 13.7919 rmse 154.0161 215.1149 109.2771 40.9531 mape* 0.0696 0.0891 0.0569 0.0112 max_error 708.0205 1142.9223 416.3620 331.6607 Fold parameters fold params dict fold_0 {'best_pipeline': ['(selectfrommodel, SelectFromModel(estimator=ExtraTreesRegressor(bootstrap=false, criterion=mse,\\n max_depth=null,\\n ... fold_1 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.1))', '(fastica, FastICA(algorithm=parallel, fun=logcosh, fun_args=null, max_iter=200,\\n n_components=null, random_state=nu... fold_2 {'best_pipeline': ['(selectpercentile, SelectPercentile(percentile=53,\\n score_func=<function f_regression at 0x2aaaf79a38c8>))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=... fold_3 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.1))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(kneighborsregressor, KNeighborsRegressor(algorithm=auto, ... fold_4 {'best_pipeline': ['(selectfrommodel, SelectFromModel(estimator=ExtraTreesRegressor(bootstrap=false, criterion=mse,\\n max_depth=null,\\n ...","title":"matbench_v0.1: AMMExpress v2020"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#matbench_v01-ammexpress-v2020","text":"","title":"matbench_v0.1: AMMExpress v2020"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#algorithm-description","text":"Automatminer express v1.03.20200727. Based on automatic featurization, tree-based feature reduction, and genetic-algorithm based AutoML with the TPOT package.","title":"Algorithm description:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#notes","text":"All data was generated using the same config (express, default). The automatminer version requirement specifies the versions of many dependent packages, such as matminer, which are required for the algorithm to work in your virtualenv. Raw data download and example notebook available on the matbench repo .","title":"Notes:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#references-in-bibtex-format","text":"('@article{Dunn2020,\\n' ' doi = {10.1038/s41524-020-00406-3},\\n' ' url = {https://doi.org/10.1038/s41524-020-00406-3},\\n' ' year = {2020},\\n' ' month = sep,\\n' ' publisher = {Springer Science and Business Media {LLC}},\\n' ' volume = {6},\\n' ' number = {1},\\n' ' author = {Alexander Dunn and Qi Wang and Alex Ganose and Daniel Dopp and ' 'Anubhav Jain},\\n' ' title = {Benchmarking materials property prediction methods: the Matbench ' 'test set and Automatminer reference algorithm},\\n' ' journal = {npj Computational Materials}\\n' '}')","title":"References (in bibtex format):"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#user-metadata","text":"{'autofeaturizer_kwargs': {'n_jobs': 10, 'preset': 'express'}, 'cleaner_kwargs': {'feature_na_method': 'drop', 'max_na_frac': 0.1, 'na_method_fit': 'mean', 'na_method_transform': 'mean'}, 'learner_kwargs': {'max_eval_time_mins': 20, 'max_time_mins': 1440, 'memory': 'auto', 'n_jobs': 10, 'population_size': 200}, 'learner_name': 'TPOTAdaptor', 'reducer_kwargs': {'reducers': ['corr', 'tree'], 'tree_importance_percentile': 0.99}}","title":"User metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#metadata","text":"tasks recorded 13/13 complete? \u2713 composition complete? \u2713 structure complete? \u2713 regression complete? \u2713 classification complete? \u2713","title":"Metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#software-requirements","text":"{'python': ['automatminer==1.0.3.20200727', 'matbench==0.1.0']}","title":"Software Requirements"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#task-data","text":"","title":"Task data:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#matbench_dielectric","text":"","title":"matbench_dielectric"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-scores","text":"fold mae rmse mape* max_error fold_0 0.2188 0.6855 0.0760 14.6654 fold_1 0.2844 1.0764 0.0899 19.6283 fold_2 0.4257 2.9472 0.0889 59.0112 fold_3 0.3198 2.2782 0.0720 53.5196 fold_4 0.3264 1.6137 0.0987 28.1601","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-score-stats","text":"metric mean max min std mae 0.3150 0.4257 0.2188 0.0672 rmse 1.7202 2.9472 0.6855 0.8140 mape* 0.0851 0.0987 0.0720 0.0098 max_error 34.9969 59.0112 14.6654 17.9782","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-parameters","text":"fold params dict fold_0 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.006, score_func=<function f_regression at 0x2aaaef1a0840>))', '(robustscaler, RobustScaler(copy=true, quantile_range=(25.0, 75.0), with_centering=true... fold_1 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.0001))', '(zerocount, ZeroCount())', '(gradientboostingregressor, GradientBoostingRegressor(alpha=0.75, criterion=friedman_mse, in... fold_2 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.001))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(gradientboostingregressor, GradientBoostingRegressor(al... fold_3 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.023, score_func=<function f_regression at 0x2aaaef19f950>))', '(standardscaler, StandardScaler(copy=true, with_mean=true, with_std=true))', '(gradient... fold_4 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.034, score_func=<function f_regression at 0x2aaaf35a08c8>))', '(zerocount, ZeroCount())', '(gradientboostingregressor, GradientBoostingRegressor(alpha...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#matbench_expt_gap","text":"","title":"matbench_expt_gap"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-scores_1","text":"fold mae rmse mape* max_error fold_0 0.3998 0.9435 0.3372 8.0111 fold_1 0.4061 0.9354 0.3085 8.6887 fold_2 0.4538 1.0955 0.3916 12.7533 fold_3 0.4061 1.0273 0.3019 12.6296 fold_4 0.4150 0.9573 0.4503 6.0779","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-score-stats_1","text":"metric mean max min std mae 0.4161 0.4538 0.3998 0.0194 rmse 0.9918 1.0955 0.9354 0.0612 mape* 0.3579 0.4503 0.3019 0.0560 max_error 9.6321 12.7533 6.0779 2.6411","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-parameters_1","text":"fold params dict fold_0 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.035, score_func=<function f_regression at 0x2aaaf35a18c8>))', '(standardscaler, StandardScaler(copy=true, with_mean=true, with_std=true))', '(gradient... fold_1 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.046, score_func=<function f_regression at 0x2aaaef19f8c8>))', '(onehotencoder, OneHotEncoder(categorical_features=[false, false, false, false, false, ... fold_2 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.0005))', '(robustscaler, RobustScaler(copy=true, quantile_range=(25.0, 75.0), with_centering=true,\\n with_scaling=true... fold_3 {'best_pipeline': ['(selectpercentile, SelectPercentile(percentile=85,\\n score_func=<function f_regression at 0x2aaaf39a38c8>))', '(onehotencoder, OneHotEncoder(categorical_features=[f... fold_4 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.0005))', '(maxabsscaler, MaxAbsScaler(copy=true))', '(randomforestregressor, RandomForestRegressor(bootstrap=false, criterion=mse,...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#matbench_expt_is_metal","text":"","title":"matbench_expt_is_metal"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-scores_2","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.9218 0.9218 0.9205 0.9218 fold_1 0.9157 0.9156 0.9145 0.9156 fold_2 0.9207 0.9207 0.9193 0.9207 fold_3 0.9228 0.9228 0.9223 0.9228 fold_4 0.9238 0.9238 0.9235 0.9238","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-score-stats_2","text":"metric mean max min std accuracy 0.9210 0.9238 0.9157 0.0028 balanced_accuracy 0.9209 0.9238 0.9156 0.0028 f1 0.9200 0.9235 0.9145 0.0031 rocauc 0.9209 0.9238 0.9156 0.0028","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-parameters_2","text":"fold params dict fold_0 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.009000000000000001,\\n score_func=<function f_classif at 0x2aaaf35a16a8>))', '(onehotencoder, OneHotEncoder(categorical_features=[false, false... fold_1 {'best_pipeline': ['(rfe, RFE(estimator=ExtraTreesClassifier(bootstrap=false, class_weight=null,\\n criterion=gini, max_depth=null,\\n ... fold_2 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.001))', '(maxabsscaler, MaxAbsScaler(copy=true))', '(gradientboostingclassifier, GradientBoostingClassifier(criterion=friedman_mse... fold_3 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.03, score_func=<function f_classif at 0x2aaaf35a0730>))', '(maxabsscaler, MaxAbsScaler(copy=true))', '(gradientboostingclassifier, GradientBoostingCla... fold_4 {'best_pipeline': ['(rfe, RFE(estimator=ExtraTreesClassifier(bootstrap=false, class_weight=null,\\n criterion=entropy, max_depth=null,\\n ...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#matbench_glass","text":"","title":"matbench_glass"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-scores_3","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.8283 0.8441 0.8697 0.8441 fold_1 0.8125 0.8383 0.8548 0.8383 fold_2 0.8574 0.8546 0.8956 0.8546 fold_3 0.9173 0.8742 0.9437 0.8742 fold_4 0.9375 0.8921 0.9579 0.8921","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-score-stats_3","text":"metric mean max min std accuracy 0.8706 0.9375 0.8125 0.0490 balanced_accuracy 0.8607 0.8921 0.8383 0.0199 f1 0.9043 0.9579 0.8548 0.0404 rocauc 0.8607 0.8921 0.8383 0.0199","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-parameters_3","text":"fold params dict fold_0 {'best_pipeline': ['(selectfrommodel, SelectFromModel(estimator=ExtraTreesClassifier(bootstrap=false,\\n class_weight=null,\\n ... fold_1 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.0001))', '(standardscaler, StandardScaler(copy=true, with_mean=true, with_std=true))', '(extratreesclassifier, ExtraTreesClassifie... fold_2 {'best_pipeline': ['(selectpercentile, SelectPercentile(percentile=74,\\n score_func=<function f_classif at 0x2aaaf35a0730>))', '(onehotencoder, OneHotEncoder(categorical_features=[fals... fold_3 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.0001))', '(standardscaler, StandardScaler(copy=true, with_mean=true, with_std=true))', '(gradientboostingclassifier, GradientBoost... fold_4 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.0001))', '(standardscaler, StandardScaler(copy=true, with_mean=true, with_std=true))', '(gradientboostingclassifier, GradientBoost...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#matbench_jdft2d","text":"","title":"matbench_jdft2d"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-scores_4","text":"fold mae rmse mape* max_error fold_0 29.5070 57.7719 18.9726 362.2752 fold_1 44.3036 98.1137 0.3191 551.7742 fold_2 54.4690 164.0162 0.5117 847.0618 fold_3 28.0759 55.8345 0.2371 316.2185 fold_4 42.8931 156.9938 0.5429 1552.9102","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-score-stats_4","text":"metric mean max min std mae 39.8497 54.4690 28.0759 9.8835 rmse 106.5460 164.0162 55.8345 46.6251 mape* 4.1167 18.9726 0.2371 7.4289 max_error 726.0480 1552.9102 316.2185 453.6535","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-parameters_4","text":"fold params dict fold_0 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.1))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(gradientboostingregressor, GradientBoostingRegressor(alph... fold_1 {'best_pipeline': ['(selectpercentile, SelectPercentile(percentile=40,\\n score_func=<function f_regression at 0x2aaaf35a08c8>))', '(maxabsscaler, MaxAbsScaler(copy=true))', '(gradientb... fold_2 {'best_pipeline': ['(selectpercentile, SelectPercentile(percentile=62,\\n score_func=<function f_regression at 0x2aaaf35a08c8>))', '(onehotencoder, OneHotEncoder(categorical_features=[f... fold_3 {'best_pipeline': ['(selectpercentile, SelectPercentile(percentile=82,\\n score_func=<function f_regression at 0x2aab561f6620>))', '(robustscaler, RobustScaler(copy=true, quantile_range... fold_4 {'best_pipeline': ['(selectpercentile, SelectPercentile(percentile=62,\\n score_func=<function f_regression at 0x2aaaf35a08c8>))', '(zerocount, ZeroCount())', '(gradientboostingregresso...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#matbench_log_gvrh","text":"","title":"matbench_log_gvrh"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-scores_5","text":"fold mae rmse mape* max_error fold_0 0.0891 0.1270 0.0692 1.1580 fold_1 0.0852 0.1261 0.0666 1.0887 fold_2 0.0849 0.1261 0.0668 0.9631 fold_3 0.0884 0.1279 0.0670 0.8959 fold_4 0.0894 0.1313 0.0690 0.9810","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-score-stats_5","text":"metric mean max min std mae 0.0874 0.0894 0.0849 0.0020 rmse 0.1277 0.1313 0.1261 0.0019 mape* 0.0677 0.0692 0.0666 0.0012 max_error 1.0173 1.1580 0.8959 0.0937","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-parameters_5","text":"fold params dict fold_0 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.2))', '(zerocount, ZeroCount())', '(gradientboostingregressor, GradientBoostingRegressor(alpha=0.99, criterion=friedman_mse, init=... fold_1 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.01))', '(robustscaler, RobustScaler(copy=true, quantile_range=(25.0, 75.0), with_centering=true,\\n with_scaling=true))... fold_2 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.01, score_func=<function f_regression at 0x2aaaef19e8c8>))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(randomforestregressor,... fold_3 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.0001))', '(standardscaler, StandardScaler(copy=true, with_mean=true, with_std=true))', '(gradientboostingregressor, GradientBoosti... fold_4 {'best_pipeline': ['(selectpercentile, SelectPercentile(percentile=96,\\n score_func=<function f_regression at 0x2aaaf35a08c8>))', '(maxabsscaler, MaxAbsScaler(copy=true))', '(extratree...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#matbench_log_kvrh","text":"","title":"matbench_log_kvrh"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-scores_6","text":"fold mae rmse mape* max_error fold_0 0.0639 0.1179 0.0417 1.4823 fold_1 0.0659 0.1231 0.0432 1.2686 fold_2 0.0627 0.1115 0.0411 1.1316 fold_3 0.0668 0.1217 0.0464 1.1890 fold_4 0.0640 0.1172 0.0417 1.4335","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-score-stats_6","text":"metric mean max min std mae 0.0647 0.0668 0.0627 0.0015 rmse 0.1183 0.1231 0.1115 0.0041 mape* 0.0428 0.0464 0.0411 0.0019 max_error 1.3010 1.4823 1.1316 0.1362","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-parameters_6","text":"fold params dict fold_0 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.032, score_func=<function f_regression at 0x2aaaf35a2840>))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(extratreesregressor, ... fold_1 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.029, score_func=<function f_regression at 0x2aaaf35a08c8>))', '(zerocount, ZeroCount())', '(gradientboostingregressor, GradientBoostingRegressor(alpha... fold_2 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.2))', '(onehotencoder, OneHotEncoder(categorical_features=[false, false, false, false, false, false,\\n ... fold_3 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.016, score_func=<function f_regression at 0x2aaaf79a28c8>))', '(onehotencoder, OneHotEncoder(categorical_features=[false, false, false, false, false, ... fold_4 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.0001))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(extratreesregressor, ExtraTreesRegressor(bootstrap=fal...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#matbench_mp_e_form","text":"","title":"matbench_mp_e_form"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-scores_7","text":"fold mae rmse mape* max_error fold_0 0.1586 0.2508 1.0829 4.0713 fold_1 0.2026 0.2955 0.9253 5.8108 fold_2 0.1473 0.2256 0.7722 2.7696 fold_3 0.2080 0.3062 1.3958 5.5190 fold_4 0.1467 0.2226 0.8028 3.3888","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-score-stats_7","text":"metric mean max min std mae 0.1726 0.2080 0.1467 0.0270 rmse 0.2602 0.3062 0.2226 0.0348 mape* 0.9958 1.3958 0.7722 0.2280 max_error 4.3119 5.8108 2.7696 1.1826","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-parameters_7","text":"fold params dict fold_0 {'best_pipeline': ['(gradientboostingregressor, GradientBoostingRegressor(alpha=0.75, criterion=friedman_mse, init=null,\\n learning_rate=0.5, loss=huber, max_depth=5,\\n max_fea... fold_1 {'best_pipeline': ['(polynomialfeatures, PolynomialFeatures(degree=2, include_bias=false, interaction_only=false))', '(pca, PCA(copy=true, iterated_power=3, n_components=null, random_state=null,\\n sv... fold_2 {'best_pipeline': ['(stackingestimator, StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.9, criterion=friedman_mse, init=null,\\n learning_rate=0.5, loss=huber, max_depth=4,\\n ... fold_3 {'best_pipeline': ['(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(selectfwe, SelectFwe(alpha=0.027, score_func=<function f_regression at 0x2b2eb18422f0>))', '(stackingestimator, St... fold_4 {'best_pipeline': ['(xgbregressor, XGBRegressor(base_score=0.5, booster=gbtree, colsample_bylevel=1,\\n colsample_bytree=1, gamma=0, learning_rate=0.5, max_delta_step=0,\\n max_depth=5, min_...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#matbench_mp_gap","text":"","title":"matbench_mp_gap"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-scores_8","text":"fold mae rmse mape* max_error fold_0 0.2799 0.5481 3.5712 5.4792 fold_1 0.2850 0.5671 3.1533 6.9105 fold_2 0.2724 0.5477 4.6097 6.2045 fold_3 0.2909 0.5710 10.0191 6.4590 fold_4 0.2837 0.5714 6.8322 5.5333","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-score-stats_8","text":"metric mean max min std mae 0.2824 0.2909 0.2724 0.0061 rmse 0.5611 0.5714 0.5477 0.0109 mape* 5.6371 10.0191 3.1533 2.5347 max_error 6.1173 6.9105 5.4792 0.5480","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-parameters_8","text":"fold params dict fold_0 {'best_pipeline': ['(stackingestimator-1, StackingEstimator(estimator=RandomForestRegressor(bootstrap=false, criterion=mse, max_depth=null,\\n max_features=0.4, max_leaf_nodes=null,\\n ... fold_1 {'best_pipeline': ['(stackingestimator-1, StackingEstimator(estimator=RandomForestRegressor(bootstrap=true, criterion=mse, max_depth=null,\\n max_features=0.35000000000000003, max_leaf_nodes=... fold_2 {'best_pipeline': ['(stackingestimator-1, StackingEstimator(estimator=RandomForestRegressor(bootstrap=false, criterion=mse, max_depth=null,\\n max_features=0.45, max_leaf_nodes=null,\\n ... fold_3 {'best_pipeline': ['(stackingestimator-1, StackingEstimator(estimator=ExtraTreesRegressor(bootstrap=false, criterion=mse, max_depth=null,\\n max_features=0.45, max_leaf_nodes=null,\\n ... fold_4 {'best_pipeline': ['(stackingestimator-1, StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.85, criterion=friedman_mse, init=null,\\n learning_rate=0.01, loss=lad, max_depth=1,\\...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#matbench_mp_is_metal","text":"","title":"matbench_mp_is_metal"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-scores_9","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.9133 0.9094 0.8982 0.9094 fold_1 0.9123 0.9086 0.8972 0.9086 fold_2 0.9129 0.9089 0.8976 0.9089 fold_3 0.9146 0.9108 0.8998 0.9108 fold_4 0.9129 0.9086 0.8974 0.9086","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-score-stats_9","text":"metric mean max min std accuracy 0.9132 0.9146 0.9123 0.0008 balanced_accuracy 0.9093 0.9108 0.9086 0.0008 f1 0.8981 0.8998 0.8972 0.0009 rocauc 0.9093 0.9108 0.9086 0.0008","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-parameters_9","text":"fold params dict fold_0 {'best_pipeline': ['(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(randomforestclassifier, RandomForestClassifier(bootstrap=false, class_weight=null,\\n criterion=entropy,... fold_1 {'best_pipeline': ['(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(randomforestclassifier, RandomForestClassifier(bootstrap=false, class_weight=null,\\n criterion=entropy,... fold_2 {'best_pipeline': ['(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(randomforestclassifier, RandomForestClassifier(bootstrap=false, class_weight=null,\\n criterion=entropy,... fold_3 {'best_pipeline': ['(stackingestimator, StackingEstimator(estimator=RandomForestClassifier(bootstrap=false, class_weight=null,\\n criterion=entropy, max_depth=null, max_features=0.5,\\n ... fold_4 {'best_pipeline': ['(featureunion, FeatureUnion(n_jobs=null,\\n transformer_list=[(functiontransformer, FunctionTransformer(accept_sparse=false, check_inverse=true,\\n func=<function copy...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#matbench_perovskites","text":"","title":"matbench_perovskites"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-scores_10","text":"fold mae rmse mape* max_error fold_0 0.2159 0.3114 0.2077 2.7651 fold_1 0.1904 0.2857 0.1944 2.6783 fold_2 0.1962 0.2869 0.1933 2.4466 fold_3 0.1992 0.2907 0.2209 3.3116 fold_4 0.2006 0.3023 0.1886 2.4386","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-score-stats_10","text":"metric mean max min std mae 0.2005 0.2159 0.1904 0.0085 rmse 0.2954 0.3114 0.2857 0.0099 mape* 0.2010 0.2209 0.1886 0.0118 max_error 2.7280 3.3116 2.4386 0.3186","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-parameters_10","text":"fold params dict fold_0 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.1))', '(robustscaler, RobustScaler(copy=true, quantile_range=(25.0, 75.0), with_centering=true,\\n with_scaling=true))'... fold_1 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.1))', '(zerocount, ZeroCount())', '(randomforestregressor, RandomForestRegressor(bootstrap=false, criterion=mse, max_depth=null,\\n... fold_2 {'best_pipeline': ['(selectfwe, SelectFwe(alpha=0.03, score_func=<function f_regression at 0x2aaaf35a08c8>))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(gradientboostingregres... fold_3 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.1))', '(robustscaler, RobustScaler(copy=true, quantile_range=(25.0, 75.0), with_centering=true,\\n with_scaling=true))'... fold_4 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.05))', '(maxabsscaler, MaxAbsScaler(copy=true))', '(randomforestregressor, RandomForestRegressor(bootstrap=false, criterion=mse, m...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#matbench_phonons","text":"","title":"matbench_phonons"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-scores_11","text":"fold mae rmse mape* max_error fold_0 67.5727 146.7970 0.1079 1151.5570 fold_1 54.0755 100.2097 0.1048 890.4159 fold_2 50.9853 96.5991 0.0931 680.9361 fold_3 59.6458 127.8555 0.1142 926.0969 fold_4 48.5738 77.0626 0.0958 383.1912","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-score-stats_11","text":"metric mean max min std mae 56.1706 67.5727 48.5738 6.7981 rmse 109.7048 146.7970 77.0626 24.6280 mape* 0.1032 0.1142 0.0931 0.0078 max_error 806.4394 1151.5570 383.1912 258.9850","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-parameters_11","text":"fold params dict fold_0 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.01))', '(robustscaler, RobustScaler(copy=true, quantile_range=(25.0, 75.0), with_centering=true,\\n with_scaling=true))... fold_1 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.005))', '(maxabsscaler, MaxAbsScaler(copy=true))', '(gradientboostingregressor, GradientBoostingRegressor(alpha=0.8, criterion=fri... fold_2 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.1))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(extratreesregressor, ExtraTreesRegressor(bootstrap=false,... fold_3 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.0001))', '(onehotencoder, OneHotEncoder(categorical_features=[false, false, false, false, false, false,\\n ... fold_4 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.2))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(extratreesregressor, ExtraTreesRegressor(bootstrap=false,...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#matbench_steels","text":"","title":"matbench_steels"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-scores_12","text":"fold mae rmse mape* max_error fold_0 109.3058 188.8049 0.0693 1082.7703 fold_1 80.4188 109.2771 0.0569 416.3620 fold_2 83.5360 120.2935 0.0607 424.5913 fold_3 98.7186 136.5898 0.0722 473.4563 fold_4 115.4851 215.1149 0.0891 1142.9223","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-score-stats_12","text":"metric mean max min std mae 97.4929 115.4851 80.4188 13.7919 rmse 154.0161 215.1149 109.2771 40.9531 mape* 0.0696 0.0891 0.0569 0.0112 max_error 708.0205 1142.9223 416.3620 331.6607","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_automatminer_expressv2020/#fold-parameters_12","text":"fold params dict fold_0 {'best_pipeline': ['(selectfrommodel, SelectFromModel(estimator=ExtraTreesRegressor(bootstrap=false, criterion=mse,\\n max_depth=null,\\n ... fold_1 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.1))', '(fastica, FastICA(algorithm=parallel, fun=logcosh, fun_args=null, max_iter=200,\\n n_components=null, random_state=nu... fold_2 {'best_pipeline': ['(selectpercentile, SelectPercentile(percentile=53,\\n score_func=<function f_regression at 0x2aaaf79a38c8>))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=... fold_3 {'best_pipeline': ['(variancethreshold, VarianceThreshold(threshold=0.1))', '(minmaxscaler, MinMaxScaler(copy=true, feature_range=(0, 1)))', '(kneighborsregressor, KNeighborsRegressor(algorithm=auto, ... fold_4 {'best_pipeline': ['(selectfrommodel, SelectFromModel(estimator=ExtraTreesRegressor(bootstrap=false, criterion=mse,\\n max_depth=null,\\n ...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/","text":"matbench_v0.1: CGCNN v2019 Algorithm description: Convolutional graph neural network, in it's original implementation as in https://github.com/txie-93/cgcnn. Utility modifications were made in order to run CGCNN without error across all structure tasks. Adapted from data originally taken from Dunn et. al 'Benchmarking materials property prediction methods: the Matbench test set and Automatminer reference algorithm' (2020). Training was performed using one NVIDIA 1080Ti GPU using CUDA (accompanied by two Intel Xeon E5-2623 CPUs with 60GB RAM). Each outer NCV training set was split 75/25 for train/validation; thus the final split for each fold was 60% train, 20% validation, 20% test. Each model is trained in epochs of 128-structure batches by optimizing according to mean squared error loss (regression) or binary cross-entropy (classification). After each epoch, the validation loss is computed with the same scoring functions as the final evaluation: MAE for regression or ROC-AUC for classification (made negative so that higher loss represents worse performance). To prevent overfitting, the training is stopped early when the validation loss does not improve over a period of at least 500 epochs. Notes: Raw data download and example notebook available on the matbench repo . References (in bibtex format): ['@article{Xie2018,\\n' ' doi = {10.1103/physrevlett.120.145301},\\n' ' url = {https://doi.org/10.1103/physrevlett.120.145301},\\n' ' year = {2018},\\n' ' month = apr,\\n' ' publisher = {American Physical Society ({APS})},\\n' ' volume = {120},\\n' ' number = {14},\\n' ' author = {Tian Xie and Jeffrey C. Grossman},\\n' ' title = {Crystal Graph Convolutional Neural Networks for an Accurate and ' 'Interpretable Prediction of Material Properties},\\n' ' journal = {Physical Review Letters}\\n' '}', '@article{Dunn2020,\\n' ' doi = {10.1038/s41524-020-00406-3},\\n' ' url = {https://doi.org/10.1038/s41524-020-00406-3},\\n' ' year = {2020},\\n' ' month = sep,\\n' ' publisher = {Springer Science and Business Media {LLC}},\\n' ' volume = {6},\\n' ' number = {1},\\n' ' author = {Alexander Dunn and Qi Wang and Alex Ganose and Daniel Dopp and ' 'Anubhav Jain},\\n' ' title = {Benchmarking materials property prediction methods: the Matbench ' 'test set and Automatminer reference algorithm},\\n' ' journal = {npj Computational Materials}\\n' '}'] User metadata: {'conv_to_fc.bias': 32, 'conv_to_fc.weight': 2048, 'convs.0.bn1.bias': 128, 'convs.0.bn1.num_batches_tracked': 1, 'convs.0.bn1.running_mean': 128, 'convs.0.bn1.running_var': 128, 'convs.0.bn1.weight': 128, 'convs.0.bn2.bias': 64, 'convs.0.bn2.num_batches_tracked': 1, 'convs.0.bn2.running_mean': 64, 'convs.0.bn2.running_var': 64, 'convs.0.bn2.weight': 64, 'convs.0.fc_full.bias': 128, 'convs.0.fc_full.weight': 21632, 'convs.1.bn1.bias': 128, 'convs.1.bn1.num_batches_tracked': 1, 'convs.1.bn1.running_mean': 128, 'convs.1.bn1.running_var': 128, 'convs.1.bn1.weight': 128, 'convs.1.bn2.bias': 64, 'convs.1.bn2.num_batches_tracked': 1, 'convs.1.bn2.running_mean': 64, 'convs.1.bn2.running_var': 64, 'convs.1.bn2.weight': 64, 'convs.1.fc_full.bias': 128, 'convs.1.fc_full.weight': 21632, 'convs.2.bn1.bias': 128, 'convs.2.bn1.num_batches_tracked': 1, 'convs.2.bn1.running_mean': 128, 'convs.2.bn1.running_var': 128, 'convs.2.bn1.weight': 128, 'convs.2.bn2.bias': 64, 'convs.2.bn2.num_batches_tracked': 1, 'convs.2.bn2.running_mean': 64, 'convs.2.bn2.running_var': 64, 'convs.2.bn2.weight': 64, 'convs.2.fc_full.bias': 128, 'convs.2.fc_full.weight': 21632, 'convs.3.bn1.bias': 128, 'convs.3.bn1.num_batches_tracked': 1, 'convs.3.bn1.running_mean': 128, 'convs.3.bn1.running_var': 128, 'convs.3.bn1.weight': 128, 'convs.3.bn2.bias': 64, 'convs.3.bn2.num_batches_tracked': 1, 'convs.3.bn2.running_mean': 64, 'convs.3.bn2.running_var': 64, 'convs.3.bn2.weight': 64, 'convs.3.fc_full.bias': 128, 'convs.3.fc_full.weight': 21632, 'embedding.bias': 64, 'embedding.weight': 5888, 'fc_out.bias': 1, 'fc_out.weight': 32} Metadata: tasks recorded 9/13 complete? \u2717 composition complete? \u2717 structure complete? \u2713 regression complete? \u2717 classification complete? \u2717 Software Requirements {'python': ['https://github.com/txie-93/cgcnn', 'numpy==1.20.1', 'matbench==0.1.0']} Task data: matbench_dielectric Fold scores fold mae rmse mape* max_error fold_0 0.4704 0.9059 0.1949 14.6895 fold_1 0.5724 1.2825 0.2222 20.3729 fold_2 0.7301 3.0600 0.2194 58.9996 fold_3 0.6111 2.4214 0.2119 53.4782 fold_4 0.6099 1.8183 0.2348 28.6714 Fold score stats metric mean max min std mae 0.5988 0.7301 0.4704 0.0833 rmse 1.8976 3.0600 0.9059 0.7738 mape* 0.2167 0.2348 0.1949 0.0131 max_error 35.2423 58.9996 14.6895 17.7969 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_jdft2d Fold scores fold mae rmse mape* max_error fold_0 34.4937 56.8278 33.7683 256.0330 fold_1 51.1167 98.1228 0.5027 407.6809 fold_2 69.4250 182.5647 0.6043 1061.5574 fold_3 42.7453 71.8811 0.4072 303.9963 fold_4 48.4396 154.4480 0.6338 1516.9120 Fold score stats metric mean max min std mae 49.2440 69.4250 34.4937 11.5865 rmse 112.7689 182.5647 56.8278 48.2169 mape* 7.1833 33.7683 0.4072 13.2927 max_error 709.2359 1516.9120 256.0330 497.3969 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_log_gvrh Fold scores fold mae rmse mape* max_error fold_0 0.0870 0.1270 0.0680 1.0473 fold_1 0.0899 0.1384 0.0714 1.4520 fold_2 0.0887 0.1323 0.0699 1.0024 fold_3 0.0902 0.1344 0.0705 0.9712 fold_4 0.0918 0.1362 0.0712 0.8430 Fold score stats metric mean max min std mae 0.0895 0.0918 0.0870 0.0016 rmse 0.1337 0.1384 0.1270 0.0039 mape* 0.0702 0.0714 0.0680 0.0012 max_error 1.0632 1.4520 0.8430 0.2059 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_log_kvrh Fold scores fold mae rmse mape* max_error fold_0 0.0702 0.1290 0.0456 1.7725 fold_1 0.0722 0.1353 0.0477 1.3813 fold_2 0.0665 0.1191 0.0423 1.1052 fold_3 0.0748 0.1341 0.0517 1.1231 fold_4 0.0724 0.1328 0.0480 1.5001 Fold score stats metric mean max min std mae 0.0712 0.0748 0.0665 0.0028 rmse 0.1301 0.1353 0.1191 0.0059 mape* 0.0471 0.0517 0.0423 0.0031 max_error 1.3765 1.7725 1.1052 0.2490 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_mp_e_form Fold scores fold mae rmse mape* max_error fold_0 0.0340 0.0714 0.4273 3.4254 fold_1 0.0340 0.0681 0.1934 2.0786 fold_2 0.0328 0.0756 0.2075 7.7205 fold_3 0.0332 0.0623 0.2258 1.3283 fold_4 0.0346 0.0633 0.2830 1.5782 Fold score stats metric mean max min std mae 0.0337 0.0346 0.0328 0.0006 rmse 0.0682 0.0756 0.0623 0.0050 mape* 0.2674 0.4273 0.1934 0.0855 max_error 3.2262 7.7205 1.3283 2.3611 Fold parameters fold params dict fold_0 {'training_mae': 0.020714398227129675, 'training_n_samples': 79650, 'validation_mae': 0.03311641346102554, 'validation_n_samples': 26551} fold_1 {'training_mae': 0.020727165395357502, 'training_n_samples': 79650, 'validation_mae': 0.03387322865233633, 'validation_n_samples': 26551} fold_2 {'training_mae': 0.020593563770909047, 'training_n_samples': 79651, 'validation_mae': 0.033246301549184044, 'validation_n_samples': 26551} fold_3 {'training_mae': 0.020858287502723834, 'training_n_samples': 79651, 'validation_mae': 0.03272479726288639, 'validation_n_samples': 26551} fold_4 {'training_mae': 0.0220815778646356, 'training_n_samples': 79651, 'validation_mae': 0.03445024667244967, 'validation_n_samples': 26551} matbench_mp_gap Fold scores fold mae rmse mape* max_error fold_0 0.2978 0.6753 3.5253 7.2169 fold_1 0.2939 0.6827 3.3933 13.6569 fold_2 0.2960 0.6653 5.5089 6.8339 fold_3 0.2947 0.6740 7.7018 7.7523 fold_4 0.3038 0.6884 5.7405 7.7166 Fold score stats metric mean max min std mae 0.2972 0.3038 0.2939 0.0035 rmse 0.6771 0.6884 0.6653 0.0079 mape* 5.1740 7.7018 3.3933 1.5945 max_error 8.6353 13.6569 6.8339 2.5336 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_mp_is_metal Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.9590 0.9578 0.9526 0.9578 fold_1 0.9450 0.9432 0.9363 0.9432 fold_2 0.9643 0.9632 0.9588 0.9632 fold_3 0.9480 0.9463 0.9398 0.9463 fold_4 0.9510 0.9494 0.9433 0.9494 Fold score stats metric mean max min std accuracy 0.9534 0.9643 0.9450 0.0072 balanced_accuracy 0.9520 0.9632 0.9432 0.0074 f1 0.9462 0.9588 0.9363 0.0083 rocauc 0.9520 0.9632 0.9432 0.0074 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_perovskites Fold scores fold mae rmse mape* max_error fold_0 0.0456 0.0753 0.0483 0.9441 fold_1 0.0462 0.0735 0.0497 0.9923 fold_2 0.0448 0.0690 0.0466 0.9840 fold_3 0.0454 0.0714 0.0482 0.7688 fold_4 0.0442 0.0718 0.0419 0.9384 Fold score stats metric mean max min std mae 0.0452 0.0462 0.0442 0.0007 rmse 0.0722 0.0753 0.0690 0.0021 mape* 0.0469 0.0497 0.0419 0.0027 max_error 0.9255 0.9923 0.7688 0.0812 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_phonons Fold scores fold mae rmse mape* max_error fold_0 81.1553 231.3233 0.1330 2504.8743 fold_1 45.0945 79.3798 0.0995 835.2144 fold_2 54.2563 132.8543 0.1081 1667.5734 fold_3 56.5819 169.7248 0.1201 2378.4055 fold_4 51.7292 95.2267 0.1087 658.0856 Fold score stats metric mean max min std mae 57.7635 81.1553 45.0945 12.3109 rmse 141.7018 231.3233 79.3798 54.6618 mape* 0.1139 0.1330 0.0995 0.0116 max_error 1608.8306 2504.8743 658.0856 761.7071 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"matbench_v0.1: CGCNN v2019"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#matbench_v01-cgcnn-v2019","text":"","title":"matbench_v0.1: CGCNN v2019"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#algorithm-description","text":"Convolutional graph neural network, in it's original implementation as in https://github.com/txie-93/cgcnn. Utility modifications were made in order to run CGCNN without error across all structure tasks. Adapted from data originally taken from Dunn et. al 'Benchmarking materials property prediction methods: the Matbench test set and Automatminer reference algorithm' (2020). Training was performed using one NVIDIA 1080Ti GPU using CUDA (accompanied by two Intel Xeon E5-2623 CPUs with 60GB RAM). Each outer NCV training set was split 75/25 for train/validation; thus the final split for each fold was 60% train, 20% validation, 20% test. Each model is trained in epochs of 128-structure batches by optimizing according to mean squared error loss (regression) or binary cross-entropy (classification). After each epoch, the validation loss is computed with the same scoring functions as the final evaluation: MAE for regression or ROC-AUC for classification (made negative so that higher loss represents worse performance). To prevent overfitting, the training is stopped early when the validation loss does not improve over a period of at least 500 epochs.","title":"Algorithm description:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#notes","text":"Raw data download and example notebook available on the matbench repo .","title":"Notes:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#references-in-bibtex-format","text":"['@article{Xie2018,\\n' ' doi = {10.1103/physrevlett.120.145301},\\n' ' url = {https://doi.org/10.1103/physrevlett.120.145301},\\n' ' year = {2018},\\n' ' month = apr,\\n' ' publisher = {American Physical Society ({APS})},\\n' ' volume = {120},\\n' ' number = {14},\\n' ' author = {Tian Xie and Jeffrey C. Grossman},\\n' ' title = {Crystal Graph Convolutional Neural Networks for an Accurate and ' 'Interpretable Prediction of Material Properties},\\n' ' journal = {Physical Review Letters}\\n' '}', '@article{Dunn2020,\\n' ' doi = {10.1038/s41524-020-00406-3},\\n' ' url = {https://doi.org/10.1038/s41524-020-00406-3},\\n' ' year = {2020},\\n' ' month = sep,\\n' ' publisher = {Springer Science and Business Media {LLC}},\\n' ' volume = {6},\\n' ' number = {1},\\n' ' author = {Alexander Dunn and Qi Wang and Alex Ganose and Daniel Dopp and ' 'Anubhav Jain},\\n' ' title = {Benchmarking materials property prediction methods: the Matbench ' 'test set and Automatminer reference algorithm},\\n' ' journal = {npj Computational Materials}\\n' '}']","title":"References (in bibtex format):"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#user-metadata","text":"{'conv_to_fc.bias': 32, 'conv_to_fc.weight': 2048, 'convs.0.bn1.bias': 128, 'convs.0.bn1.num_batches_tracked': 1, 'convs.0.bn1.running_mean': 128, 'convs.0.bn1.running_var': 128, 'convs.0.bn1.weight': 128, 'convs.0.bn2.bias': 64, 'convs.0.bn2.num_batches_tracked': 1, 'convs.0.bn2.running_mean': 64, 'convs.0.bn2.running_var': 64, 'convs.0.bn2.weight': 64, 'convs.0.fc_full.bias': 128, 'convs.0.fc_full.weight': 21632, 'convs.1.bn1.bias': 128, 'convs.1.bn1.num_batches_tracked': 1, 'convs.1.bn1.running_mean': 128, 'convs.1.bn1.running_var': 128, 'convs.1.bn1.weight': 128, 'convs.1.bn2.bias': 64, 'convs.1.bn2.num_batches_tracked': 1, 'convs.1.bn2.running_mean': 64, 'convs.1.bn2.running_var': 64, 'convs.1.bn2.weight': 64, 'convs.1.fc_full.bias': 128, 'convs.1.fc_full.weight': 21632, 'convs.2.bn1.bias': 128, 'convs.2.bn1.num_batches_tracked': 1, 'convs.2.bn1.running_mean': 128, 'convs.2.bn1.running_var': 128, 'convs.2.bn1.weight': 128, 'convs.2.bn2.bias': 64, 'convs.2.bn2.num_batches_tracked': 1, 'convs.2.bn2.running_mean': 64, 'convs.2.bn2.running_var': 64, 'convs.2.bn2.weight': 64, 'convs.2.fc_full.bias': 128, 'convs.2.fc_full.weight': 21632, 'convs.3.bn1.bias': 128, 'convs.3.bn1.num_batches_tracked': 1, 'convs.3.bn1.running_mean': 128, 'convs.3.bn1.running_var': 128, 'convs.3.bn1.weight': 128, 'convs.3.bn2.bias': 64, 'convs.3.bn2.num_batches_tracked': 1, 'convs.3.bn2.running_mean': 64, 'convs.3.bn2.running_var': 64, 'convs.3.bn2.weight': 64, 'convs.3.fc_full.bias': 128, 'convs.3.fc_full.weight': 21632, 'embedding.bias': 64, 'embedding.weight': 5888, 'fc_out.bias': 1, 'fc_out.weight': 32}","title":"User metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#metadata","text":"tasks recorded 9/13 complete? \u2717 composition complete? \u2717 structure complete? \u2713 regression complete? \u2717 classification complete? \u2717","title":"Metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#software-requirements","text":"{'python': ['https://github.com/txie-93/cgcnn', 'numpy==1.20.1', 'matbench==0.1.0']}","title":"Software Requirements"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#task-data","text":"","title":"Task data:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#matbench_dielectric","text":"","title":"matbench_dielectric"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-scores","text":"fold mae rmse mape* max_error fold_0 0.4704 0.9059 0.1949 14.6895 fold_1 0.5724 1.2825 0.2222 20.3729 fold_2 0.7301 3.0600 0.2194 58.9996 fold_3 0.6111 2.4214 0.2119 53.4782 fold_4 0.6099 1.8183 0.2348 28.6714","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-score-stats","text":"metric mean max min std mae 0.5988 0.7301 0.4704 0.0833 rmse 1.8976 3.0600 0.9059 0.7738 mape* 0.2167 0.2348 0.1949 0.0131 max_error 35.2423 58.9996 14.6895 17.7969","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-parameters","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#matbench_jdft2d","text":"","title":"matbench_jdft2d"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-scores_1","text":"fold mae rmse mape* max_error fold_0 34.4937 56.8278 33.7683 256.0330 fold_1 51.1167 98.1228 0.5027 407.6809 fold_2 69.4250 182.5647 0.6043 1061.5574 fold_3 42.7453 71.8811 0.4072 303.9963 fold_4 48.4396 154.4480 0.6338 1516.9120","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-score-stats_1","text":"metric mean max min std mae 49.2440 69.4250 34.4937 11.5865 rmse 112.7689 182.5647 56.8278 48.2169 mape* 7.1833 33.7683 0.4072 13.2927 max_error 709.2359 1516.9120 256.0330 497.3969","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-parameters_1","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#matbench_log_gvrh","text":"","title":"matbench_log_gvrh"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-scores_2","text":"fold mae rmse mape* max_error fold_0 0.0870 0.1270 0.0680 1.0473 fold_1 0.0899 0.1384 0.0714 1.4520 fold_2 0.0887 0.1323 0.0699 1.0024 fold_3 0.0902 0.1344 0.0705 0.9712 fold_4 0.0918 0.1362 0.0712 0.8430","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-score-stats_2","text":"metric mean max min std mae 0.0895 0.0918 0.0870 0.0016 rmse 0.1337 0.1384 0.1270 0.0039 mape* 0.0702 0.0714 0.0680 0.0012 max_error 1.0632 1.4520 0.8430 0.2059","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-parameters_2","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#matbench_log_kvrh","text":"","title":"matbench_log_kvrh"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-scores_3","text":"fold mae rmse mape* max_error fold_0 0.0702 0.1290 0.0456 1.7725 fold_1 0.0722 0.1353 0.0477 1.3813 fold_2 0.0665 0.1191 0.0423 1.1052 fold_3 0.0748 0.1341 0.0517 1.1231 fold_4 0.0724 0.1328 0.0480 1.5001","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-score-stats_3","text":"metric mean max min std mae 0.0712 0.0748 0.0665 0.0028 rmse 0.1301 0.1353 0.1191 0.0059 mape* 0.0471 0.0517 0.0423 0.0031 max_error 1.3765 1.7725 1.1052 0.2490","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-parameters_3","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#matbench_mp_e_form","text":"","title":"matbench_mp_e_form"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-scores_4","text":"fold mae rmse mape* max_error fold_0 0.0340 0.0714 0.4273 3.4254 fold_1 0.0340 0.0681 0.1934 2.0786 fold_2 0.0328 0.0756 0.2075 7.7205 fold_3 0.0332 0.0623 0.2258 1.3283 fold_4 0.0346 0.0633 0.2830 1.5782","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-score-stats_4","text":"metric mean max min std mae 0.0337 0.0346 0.0328 0.0006 rmse 0.0682 0.0756 0.0623 0.0050 mape* 0.2674 0.4273 0.1934 0.0855 max_error 3.2262 7.7205 1.3283 2.3611","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-parameters_4","text":"fold params dict fold_0 {'training_mae': 0.020714398227129675, 'training_n_samples': 79650, 'validation_mae': 0.03311641346102554, 'validation_n_samples': 26551} fold_1 {'training_mae': 0.020727165395357502, 'training_n_samples': 79650, 'validation_mae': 0.03387322865233633, 'validation_n_samples': 26551} fold_2 {'training_mae': 0.020593563770909047, 'training_n_samples': 79651, 'validation_mae': 0.033246301549184044, 'validation_n_samples': 26551} fold_3 {'training_mae': 0.020858287502723834, 'training_n_samples': 79651, 'validation_mae': 0.03272479726288639, 'validation_n_samples': 26551} fold_4 {'training_mae': 0.0220815778646356, 'training_n_samples': 79651, 'validation_mae': 0.03445024667244967, 'validation_n_samples': 26551}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#matbench_mp_gap","text":"","title":"matbench_mp_gap"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-scores_5","text":"fold mae rmse mape* max_error fold_0 0.2978 0.6753 3.5253 7.2169 fold_1 0.2939 0.6827 3.3933 13.6569 fold_2 0.2960 0.6653 5.5089 6.8339 fold_3 0.2947 0.6740 7.7018 7.7523 fold_4 0.3038 0.6884 5.7405 7.7166","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-score-stats_5","text":"metric mean max min std mae 0.2972 0.3038 0.2939 0.0035 rmse 0.6771 0.6884 0.6653 0.0079 mape* 5.1740 7.7018 3.3933 1.5945 max_error 8.6353 13.6569 6.8339 2.5336","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-parameters_5","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#matbench_mp_is_metal","text":"","title":"matbench_mp_is_metal"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-scores_6","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.9590 0.9578 0.9526 0.9578 fold_1 0.9450 0.9432 0.9363 0.9432 fold_2 0.9643 0.9632 0.9588 0.9632 fold_3 0.9480 0.9463 0.9398 0.9463 fold_4 0.9510 0.9494 0.9433 0.9494","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-score-stats_6","text":"metric mean max min std accuracy 0.9534 0.9643 0.9450 0.0072 balanced_accuracy 0.9520 0.9632 0.9432 0.0074 f1 0.9462 0.9588 0.9363 0.0083 rocauc 0.9520 0.9632 0.9432 0.0074","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-parameters_6","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#matbench_perovskites","text":"","title":"matbench_perovskites"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-scores_7","text":"fold mae rmse mape* max_error fold_0 0.0456 0.0753 0.0483 0.9441 fold_1 0.0462 0.0735 0.0497 0.9923 fold_2 0.0448 0.0690 0.0466 0.9840 fold_3 0.0454 0.0714 0.0482 0.7688 fold_4 0.0442 0.0718 0.0419 0.9384","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-score-stats_7","text":"metric mean max min std mae 0.0452 0.0462 0.0442 0.0007 rmse 0.0722 0.0753 0.0690 0.0021 mape* 0.0469 0.0497 0.0419 0.0027 max_error 0.9255 0.9923 0.7688 0.0812","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-parameters_7","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#matbench_phonons","text":"","title":"matbench_phonons"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-scores_8","text":"fold mae rmse mape* max_error fold_0 81.1553 231.3233 0.1330 2504.8743 fold_1 45.0945 79.3798 0.0995 835.2144 fold_2 54.2563 132.8543 0.1081 1667.5734 fold_3 56.5819 169.7248 0.1201 2378.4055 fold_4 51.7292 95.2267 0.1087 658.0856","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-score-stats_8","text":"metric mean max min std mae 57.7635 81.1553 45.0945 12.3109 rmse 141.7018 231.3233 79.3798 54.6618 mape* 0.1139 0.1330 0.0995 0.0116 max_error 1608.8306 2504.8743 658.0856 761.7071","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_cgcnnv2019/#fold-parameters_8","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/","text":"matbench_v0.1: Dummy Algorithm description: Dummy regressor, using strategy 'mean', and Dummy regressor, using strategy 'stratified'. Notes: No notes. Raw data download and example notebook available on the matbench repo . References (in bibtex format): ('@article{Dunn2020,\\n' ' doi = {10.1038/s41524-020-00406-3},\\n' ' url = {https://doi.org/10.1038/s41524-020-00406-3},\\n' ' year = {2020},\\n' ' month = sep,\\n' ' publisher = {Springer Science and Business Media {LLC}},\\n' ' volume = {6},\\n' ' number = {1},\\n' ' author = {Alexander Dunn and Qi Wang and Alex Ganose and Daniel Dopp and ' 'Anubhav Jain},\\n' ' title = {Benchmarking materials property prediction methods: the Matbench ' 'test set and Automatminer reference algorithm},\\n' ' journal = {npj Computational Materials}\\n' '}') User metadata: {'algorithm': 'dummy', 'classification_strategy': 'stratified', 'regression_strategy': 'mean'} Metadata: tasks recorded 13/13 complete? \u2713 composition complete? \u2713 structure complete? \u2713 regression complete? \u2713 classification complete? \u2713 Software Requirements {'python': ['scikit-learn==0.24.1', 'numpy==1.20.1', 'matbench==0.1.0']} Task data: matbench_dielectric Fold scores fold mae rmse mape* max_error fold_0 0.7026 1.0677 0.3201 14.9501 fold_1 0.7811 1.4374 0.3142 20.3552 fold_2 0.9218 3.1055 0.3155 59.6653 fold_3 0.8382 2.4438 0.3266 53.4563 fold_4 0.8004 1.8094 0.3222 28.5706 Fold score stats metric mean max min std mae 0.8088 0.9218 0.7026 0.0718 rmse 1.9728 3.1055 1.0677 0.7263 mape* 0.3197 0.3266 0.3142 0.0045 max_error 35.3995 59.6653 14.9501 17.9221 Fold parameters fold params dict fold_0 {'constant_': [[2.4569770107667264]]} fold_1 {'constant_': [[2.4254682439737882]]} fold_2 {'constant_': [[2.397736448888908]]} fold_3 {'constant_': [[2.429725254851624]]} fold_4 {'constant_': [[2.4316628587393003]]} matbench_expt_gap Fold scores fold mae rmse mape* max_error fold_0 1.0965 1.3397 0.8589 7.0119 fold_1 1.1922 1.5156 0.7802 8.3754 fold_2 1.1527 1.5268 1.0398 10.7354 fold_3 1.1445 1.4389 0.8373 9.5190 fold_4 1.1317 1.3979 1.2418 9.0085 Fold score stats metric mean max min std mae 1.1435 1.1922 1.0965 0.0310 rmse 1.4438 1.5268 1.3397 0.0707 mape* 0.9516 1.2418 0.7802 0.1692 max_error 8.9300 10.7354 7.0119 1.2328 Fold parameters fold params dict fold_0 {'constant_': [[0.9881156665761609]]} fold_1 {'constant_': [[0.9545533532446375]]} fold_2 {'constant_': [[0.9645506380667934]]} fold_3 {'constant_': [[0.9810371979364648]]} fold_4 {'constant_': [[0.9914956568946797]]} matbench_expt_is_metal Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.4701 0.4700 0.4540 0.4700 fold_1 0.5000 0.5001 0.5080 0.5001 fold_2 0.4878 0.4878 0.4878 0.4878 fold_3 0.5071 0.5072 0.5126 0.5072 fold_4 0.4970 0.4969 0.4944 0.4969 Fold score stats metric mean max min std accuracy 0.4924 0.5071 0.4701 0.0128 balanced_accuracy 0.4924 0.5072 0.4700 0.0128 f1 0.4913 0.5126 0.4540 0.0207 rocauc 0.4924 0.5072 0.4700 0.0128 Fold parameters fold params dict fold_0 {'class_prior_': [0.5020325203252033, 0.49796747967479676]} fold_1 {'class_prior_': [0.5019050038100076, 0.49809499618999237]} fold_2 {'class_prior_': [0.5019050038100076, 0.49809499618999237]} fold_3 {'class_prior_': [0.5019050038100076, 0.49809499618999237]} fold_4 {'class_prior_': [0.5019050038100076, 0.49809499618999237]} matbench_glass Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.6127 0.5212 0.7304 0.5212 fold_1 0.6083 0.5217 0.7251 0.5217 fold_2 0.5775 0.4848 0.7033 0.4848 fold_3 0.5731 0.4799 0.7001 0.4799 fold_4 0.5819 0.4951 0.7044 0.4951 Fold score stats metric mean max min std accuracy 0.5907 0.6127 0.5731 0.0165 balanced_accuracy 0.5005 0.5217 0.4799 0.0178 f1 0.7127 0.7304 0.7001 0.0125 rocauc 0.5005 0.5217 0.4799 0.0178 Fold parameters fold params dict fold_0 {'class_prior_': [0.289612676056338, 0.710387323943662]} fold_1 {'class_prior_': [0.289612676056338, 0.710387323943662]} fold_2 {'class_prior_': [0.289612676056338, 0.710387323943662]} fold_3 {'class_prior_': [0.289612676056338, 0.710387323943662]} fold_4 {'class_prior_': [0.289612676056338, 0.710387323943662]} matbench_jdft2d Fold scores fold mae rmse mape* max_error fold_0 53.1447 74.1060 35.3098 509.7791 fold_1 72.8118 118.0523 0.8129 642.7424 fold_2 83.1220 192.2365 0.9798 1025.0199 fold_3 61.3174 85.6603 0.7921 468.0412 fold_4 66.0295 164.1680 1.1452 1491.7993 Fold score stats metric mean max min std mae 67.2851 83.1220 53.1447 10.1832 rmse 126.8446 192.2365 74.1060 45.2193 mape* 7.8079 35.3098 0.7921 13.7515 max_error 827.4764 1491.7993 468.0412 385.9016 Fold parameters fold params dict fold_0 {'constant_': [[117.03965287603667]]} fold_1 {'constant_': [[112.91320041366653]]} fold_2 {'constant_': [[106.46511492350562]]} fold_3 {'constant_': [[114.84311227394852]]} fold_4 {'constant_': [[112.23899155170879]]} matbench_log_gvrh Fold scores fold mae rmse mape* max_error fold_0 0.2943 0.3749 0.2368 1.5529 fold_1 0.2933 0.3743 0.2359 1.5544 fold_2 0.2969 0.3736 0.2353 1.5533 fold_3 0.2875 0.3646 0.2251 1.5524 fold_4 0.2937 0.3706 0.2334 1.5552 Fold score stats metric mean max min std mae 0.2931 0.2969 0.2875 0.0031 rmse 0.3716 0.3749 0.3646 0.0038 mape* 0.2333 0.2368 0.2251 0.0042 max_error 1.5536 1.5552 1.5524 0.0010 Fold parameters fold params dict fold_0 {'constant_': [[1.5529289714161707]]} fold_1 {'constant_': [[1.554355173237515]]} fold_2 {'constant_': [[1.5532719705303168]]} fold_3 {'constant_': [[1.5523993668681186]]} fold_4 {'constant_': [[1.5552370733167413]]} matbench_log_kvrh Fold scores fold mae rmse mape* max_error fold_0 0.2935 0.3774 0.1877 1.8800 fold_1 0.2875 0.3669 0.1858 1.8809 fold_2 0.2889 0.3634 0.1825 1.8801 fold_3 0.2833 0.3635 0.1926 1.8790 fold_4 0.2953 0.3752 0.1900 1.8822 Fold score stats metric mean max min std mae 0.2897 0.2953 0.2833 0.0043 rmse 0.3693 0.3774 0.3634 0.0059 mape* 0.1877 0.1926 0.1825 0.0035 max_error 1.8804 1.8822 1.8790 0.0011 Fold parameters fold params dict fold_0 {'constant_': [[1.8800295900875317]]} fold_1 {'constant_': [[1.880914404358644]]} fold_2 {'constant_': [[1.8800659898099186]]} fold_3 {'constant_': [[1.8789962707394416]]} fold_4 {'constant_': [[1.8822230471663404]]} matbench_mp_e_form Fold scores fold mae rmse mape* max_error fold_0 1.0063 1.1626 11.6409 3.8987 fold_1 1.0036 1.1597 7.2868 3.8782 fold_2 1.0062 1.1662 8.5651 3.9096 fold_3 1.0024 1.1597 10.9729 3.8934 fold_4 1.0111 1.1675 11.2779 3.9051 Fold score stats metric mean max min std mae 1.0059 1.0111 1.0024 0.0030 rmse 1.1631 1.1675 1.1597 0.0032 mape* 9.9487 11.6409 7.2868 1.7134 max_error 3.8970 3.9096 3.8782 0.0109 Fold parameters fold params dict fold_0 {'constant_': [[-1.4071424641223964]]} fold_1 {'constant_': [[-1.4079146341783042]]} fold_2 {'constant_': [[-1.4100821676758766]]} fold_3 {'constant_': [[-1.406498235557698]]} fold_4 {'constant_': [[-1.4079540738106724]]} matbench_mp_gap Fold scores fold mae rmse mape* max_error fold_0 1.3199 1.5863 13.8283 7.1079 fold_1 1.3224 1.5888 12.1282 8.1096 fold_2 1.3252 1.5964 14.5509 7.6322 fold_3 1.3335 1.6113 19.3774 7.4334 fold_4 1.3348 1.6118 18.0392 8.5092 Fold score stats metric mean max min std mae 1.3272 1.3348 1.3199 0.0060 rmse 1.5989 1.6118 1.5863 0.0108 mape* 15.5848 19.3774 12.1282 2.7022 max_error 7.7585 8.5092 7.1079 0.4963 Fold parameters fold params dict fold_0 {'constant_': [[1.216204829779715]]} fold_1 {'constant_': [[1.2168485710920014]]} fold_2 {'constant_': [[1.2161007256449523]]} fold_3 {'constant_': [[1.2119634071927532]]} fold_4 {'constant_': [[1.2120157684560202]]} matbench_mp_is_metal Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.5158 0.5069 0.4405 0.5069 fold_1 0.5032 0.4944 0.4277 0.4944 fold_2 0.5069 0.4986 0.4342 0.4986 fold_3 0.5119 0.5032 0.4376 0.5032 fold_4 0.5118 0.5030 0.4367 0.5030 Fold score stats metric mean max min std accuracy 0.5099 0.5158 0.5032 0.0044 balanced_accuracy 0.5012 0.5069 0.4944 0.0043 f1 0.4353 0.4405 0.4277 0.0043 rocauc 0.5012 0.5069 0.4944 0.0043 Fold parameters fold params dict fold_0 {'class_prior_': [0.5650724466957239, 0.4349275533042761]} fold_1 {'class_prior_': [0.5650724466957239, 0.4349275533042761]} fold_2 {'class_prior_': [0.5650842266462481, 0.4349157733537519]} fold_3 {'class_prior_': [0.5650775700604305, 0.4349224299395696]} fold_4 {'class_prior_': [0.5650775700604305, 0.4349224299395696]} matbench_perovskites Fold scores fold mae rmse mape* max_error fold_0 0.5672 0.7361 0.7398 3.4868 fold_1 0.5742 0.7618 0.8046 3.3123 fold_2 0.5660 0.7438 0.7674 3.6873 fold_3 0.5614 0.7342 0.7738 3.3906 fold_4 0.5612 0.7362 0.7058 3.5084 Fold score stats metric mean max min std mae 0.5660 0.5742 0.5612 0.0048 rmse 0.7424 0.7618 0.7342 0.0102 mape* 0.7583 0.8046 0.7058 0.0334 max_error 3.4771 3.6873 3.3123 0.1264 Fold parameters fold params dict fold_0 {'constant_': [[1.4731871615374454]]} fold_1 {'constant_': [[1.4677308149517898]]} fold_2 {'constant_': [[1.4726720380398888]]} fold_3 {'constant_': [[1.4694433071386122]]} fold_4 {'constant_': [[1.4716264940896784]]} matbench_phonons Fold scores fold mae rmse mape* max_error fold_0 337.1003 542.7449 0.8225 3020.7169 fold_1 299.1209 452.2982 0.7977 2702.0312 fold_2 348.2576 545.4772 0.9223 3062.3450 fold_3 325.2402 480.9296 1.0268 3048.7920 fold_4 310.1921 439.3166 0.8936 1970.0884 Fold score stats metric mean max min std mae 323.9822 348.2576 299.1209 17.7269 rmse 492.1533 545.4772 439.3166 44.5176 mape* 0.8926 1.0268 0.7977 0.0810 max_error 2760.7947 3062.3450 1970.0884 417.1581 Fold parameters fold params dict fold_0 {'constant_': [[571.8686083004105]]} fold_1 {'constant_': [[583.1997247898747]]} fold_2 {'constant_': [[581.3984519265839]]} fold_3 {'constant_': [[588.7935123141577]]} fold_4 {'constant_': [[581.4972239423439]]} matbench_steels Fold scores fold mae rmse mape* max_error fold_0 241.4591 293.7245 0.1647 941.0643 fold_1 219.3770 289.2253 0.1550 1064.2831 fold_2 225.7932 291.5410 0.1600 1084.8760 fold_3 241.2035 343.9346 0.1567 1088.0568 fold_4 220.8898 287.6803 0.1576 983.3424 Fold score stats metric mean max min std mae 229.7445 241.4591 219.3770 9.6958 rmse 301.2211 343.9346 287.6803 21.4551 mape* 0.1588 0.1647 0.1550 0.0034 max_error 1032.3245 1088.0568 941.0643 59.3579 Fold parameters fold params dict fold_0 {'constant_': [[1415.3357429718874]]} fold_1 {'constant_': [[1423.0168674698796]]} fold_2 {'constant_': [[1425.424]]} fold_3 {'constant_': [[1413.0431999999998]]} fold_4 {'constant_': [[1428.1575999999998]]}","title":"matbench_v0.1: Dummy"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#matbench_v01-dummy","text":"","title":"matbench_v0.1: Dummy"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#algorithm-description","text":"Dummy regressor, using strategy 'mean', and Dummy regressor, using strategy 'stratified'.","title":"Algorithm description:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#notes","text":"No notes. Raw data download and example notebook available on the matbench repo .","title":"Notes:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#references-in-bibtex-format","text":"('@article{Dunn2020,\\n' ' doi = {10.1038/s41524-020-00406-3},\\n' ' url = {https://doi.org/10.1038/s41524-020-00406-3},\\n' ' year = {2020},\\n' ' month = sep,\\n' ' publisher = {Springer Science and Business Media {LLC}},\\n' ' volume = {6},\\n' ' number = {1},\\n' ' author = {Alexander Dunn and Qi Wang and Alex Ganose and Daniel Dopp and ' 'Anubhav Jain},\\n' ' title = {Benchmarking materials property prediction methods: the Matbench ' 'test set and Automatminer reference algorithm},\\n' ' journal = {npj Computational Materials}\\n' '}')","title":"References (in bibtex format):"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#user-metadata","text":"{'algorithm': 'dummy', 'classification_strategy': 'stratified', 'regression_strategy': 'mean'}","title":"User metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#metadata","text":"tasks recorded 13/13 complete? \u2713 composition complete? \u2713 structure complete? \u2713 regression complete? \u2713 classification complete? \u2713","title":"Metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#software-requirements","text":"{'python': ['scikit-learn==0.24.1', 'numpy==1.20.1', 'matbench==0.1.0']}","title":"Software Requirements"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#task-data","text":"","title":"Task data:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#matbench_dielectric","text":"","title":"matbench_dielectric"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-scores","text":"fold mae rmse mape* max_error fold_0 0.7026 1.0677 0.3201 14.9501 fold_1 0.7811 1.4374 0.3142 20.3552 fold_2 0.9218 3.1055 0.3155 59.6653 fold_3 0.8382 2.4438 0.3266 53.4563 fold_4 0.8004 1.8094 0.3222 28.5706","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-score-stats","text":"metric mean max min std mae 0.8088 0.9218 0.7026 0.0718 rmse 1.9728 3.1055 1.0677 0.7263 mape* 0.3197 0.3266 0.3142 0.0045 max_error 35.3995 59.6653 14.9501 17.9221","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-parameters","text":"fold params dict fold_0 {'constant_': [[2.4569770107667264]]} fold_1 {'constant_': [[2.4254682439737882]]} fold_2 {'constant_': [[2.397736448888908]]} fold_3 {'constant_': [[2.429725254851624]]} fold_4 {'constant_': [[2.4316628587393003]]}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#matbench_expt_gap","text":"","title":"matbench_expt_gap"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-scores_1","text":"fold mae rmse mape* max_error fold_0 1.0965 1.3397 0.8589 7.0119 fold_1 1.1922 1.5156 0.7802 8.3754 fold_2 1.1527 1.5268 1.0398 10.7354 fold_3 1.1445 1.4389 0.8373 9.5190 fold_4 1.1317 1.3979 1.2418 9.0085","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-score-stats_1","text":"metric mean max min std mae 1.1435 1.1922 1.0965 0.0310 rmse 1.4438 1.5268 1.3397 0.0707 mape* 0.9516 1.2418 0.7802 0.1692 max_error 8.9300 10.7354 7.0119 1.2328","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-parameters_1","text":"fold params dict fold_0 {'constant_': [[0.9881156665761609]]} fold_1 {'constant_': [[0.9545533532446375]]} fold_2 {'constant_': [[0.9645506380667934]]} fold_3 {'constant_': [[0.9810371979364648]]} fold_4 {'constant_': [[0.9914956568946797]]}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#matbench_expt_is_metal","text":"","title":"matbench_expt_is_metal"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-scores_2","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.4701 0.4700 0.4540 0.4700 fold_1 0.5000 0.5001 0.5080 0.5001 fold_2 0.4878 0.4878 0.4878 0.4878 fold_3 0.5071 0.5072 0.5126 0.5072 fold_4 0.4970 0.4969 0.4944 0.4969","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-score-stats_2","text":"metric mean max min std accuracy 0.4924 0.5071 0.4701 0.0128 balanced_accuracy 0.4924 0.5072 0.4700 0.0128 f1 0.4913 0.5126 0.4540 0.0207 rocauc 0.4924 0.5072 0.4700 0.0128","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-parameters_2","text":"fold params dict fold_0 {'class_prior_': [0.5020325203252033, 0.49796747967479676]} fold_1 {'class_prior_': [0.5019050038100076, 0.49809499618999237]} fold_2 {'class_prior_': [0.5019050038100076, 0.49809499618999237]} fold_3 {'class_prior_': [0.5019050038100076, 0.49809499618999237]} fold_4 {'class_prior_': [0.5019050038100076, 0.49809499618999237]}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#matbench_glass","text":"","title":"matbench_glass"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-scores_3","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.6127 0.5212 0.7304 0.5212 fold_1 0.6083 0.5217 0.7251 0.5217 fold_2 0.5775 0.4848 0.7033 0.4848 fold_3 0.5731 0.4799 0.7001 0.4799 fold_4 0.5819 0.4951 0.7044 0.4951","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-score-stats_3","text":"metric mean max min std accuracy 0.5907 0.6127 0.5731 0.0165 balanced_accuracy 0.5005 0.5217 0.4799 0.0178 f1 0.7127 0.7304 0.7001 0.0125 rocauc 0.5005 0.5217 0.4799 0.0178","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-parameters_3","text":"fold params dict fold_0 {'class_prior_': [0.289612676056338, 0.710387323943662]} fold_1 {'class_prior_': [0.289612676056338, 0.710387323943662]} fold_2 {'class_prior_': [0.289612676056338, 0.710387323943662]} fold_3 {'class_prior_': [0.289612676056338, 0.710387323943662]} fold_4 {'class_prior_': [0.289612676056338, 0.710387323943662]}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#matbench_jdft2d","text":"","title":"matbench_jdft2d"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-scores_4","text":"fold mae rmse mape* max_error fold_0 53.1447 74.1060 35.3098 509.7791 fold_1 72.8118 118.0523 0.8129 642.7424 fold_2 83.1220 192.2365 0.9798 1025.0199 fold_3 61.3174 85.6603 0.7921 468.0412 fold_4 66.0295 164.1680 1.1452 1491.7993","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-score-stats_4","text":"metric mean max min std mae 67.2851 83.1220 53.1447 10.1832 rmse 126.8446 192.2365 74.1060 45.2193 mape* 7.8079 35.3098 0.7921 13.7515 max_error 827.4764 1491.7993 468.0412 385.9016","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-parameters_4","text":"fold params dict fold_0 {'constant_': [[117.03965287603667]]} fold_1 {'constant_': [[112.91320041366653]]} fold_2 {'constant_': [[106.46511492350562]]} fold_3 {'constant_': [[114.84311227394852]]} fold_4 {'constant_': [[112.23899155170879]]}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#matbench_log_gvrh","text":"","title":"matbench_log_gvrh"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-scores_5","text":"fold mae rmse mape* max_error fold_0 0.2943 0.3749 0.2368 1.5529 fold_1 0.2933 0.3743 0.2359 1.5544 fold_2 0.2969 0.3736 0.2353 1.5533 fold_3 0.2875 0.3646 0.2251 1.5524 fold_4 0.2937 0.3706 0.2334 1.5552","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-score-stats_5","text":"metric mean max min std mae 0.2931 0.2969 0.2875 0.0031 rmse 0.3716 0.3749 0.3646 0.0038 mape* 0.2333 0.2368 0.2251 0.0042 max_error 1.5536 1.5552 1.5524 0.0010","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-parameters_5","text":"fold params dict fold_0 {'constant_': [[1.5529289714161707]]} fold_1 {'constant_': [[1.554355173237515]]} fold_2 {'constant_': [[1.5532719705303168]]} fold_3 {'constant_': [[1.5523993668681186]]} fold_4 {'constant_': [[1.5552370733167413]]}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#matbench_log_kvrh","text":"","title":"matbench_log_kvrh"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-scores_6","text":"fold mae rmse mape* max_error fold_0 0.2935 0.3774 0.1877 1.8800 fold_1 0.2875 0.3669 0.1858 1.8809 fold_2 0.2889 0.3634 0.1825 1.8801 fold_3 0.2833 0.3635 0.1926 1.8790 fold_4 0.2953 0.3752 0.1900 1.8822","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-score-stats_6","text":"metric mean max min std mae 0.2897 0.2953 0.2833 0.0043 rmse 0.3693 0.3774 0.3634 0.0059 mape* 0.1877 0.1926 0.1825 0.0035 max_error 1.8804 1.8822 1.8790 0.0011","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-parameters_6","text":"fold params dict fold_0 {'constant_': [[1.8800295900875317]]} fold_1 {'constant_': [[1.880914404358644]]} fold_2 {'constant_': [[1.8800659898099186]]} fold_3 {'constant_': [[1.8789962707394416]]} fold_4 {'constant_': [[1.8822230471663404]]}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#matbench_mp_e_form","text":"","title":"matbench_mp_e_form"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-scores_7","text":"fold mae rmse mape* max_error fold_0 1.0063 1.1626 11.6409 3.8987 fold_1 1.0036 1.1597 7.2868 3.8782 fold_2 1.0062 1.1662 8.5651 3.9096 fold_3 1.0024 1.1597 10.9729 3.8934 fold_4 1.0111 1.1675 11.2779 3.9051","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-score-stats_7","text":"metric mean max min std mae 1.0059 1.0111 1.0024 0.0030 rmse 1.1631 1.1675 1.1597 0.0032 mape* 9.9487 11.6409 7.2868 1.7134 max_error 3.8970 3.9096 3.8782 0.0109","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-parameters_7","text":"fold params dict fold_0 {'constant_': [[-1.4071424641223964]]} fold_1 {'constant_': [[-1.4079146341783042]]} fold_2 {'constant_': [[-1.4100821676758766]]} fold_3 {'constant_': [[-1.406498235557698]]} fold_4 {'constant_': [[-1.4079540738106724]]}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#matbench_mp_gap","text":"","title":"matbench_mp_gap"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-scores_8","text":"fold mae rmse mape* max_error fold_0 1.3199 1.5863 13.8283 7.1079 fold_1 1.3224 1.5888 12.1282 8.1096 fold_2 1.3252 1.5964 14.5509 7.6322 fold_3 1.3335 1.6113 19.3774 7.4334 fold_4 1.3348 1.6118 18.0392 8.5092","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-score-stats_8","text":"metric mean max min std mae 1.3272 1.3348 1.3199 0.0060 rmse 1.5989 1.6118 1.5863 0.0108 mape* 15.5848 19.3774 12.1282 2.7022 max_error 7.7585 8.5092 7.1079 0.4963","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-parameters_8","text":"fold params dict fold_0 {'constant_': [[1.216204829779715]]} fold_1 {'constant_': [[1.2168485710920014]]} fold_2 {'constant_': [[1.2161007256449523]]} fold_3 {'constant_': [[1.2119634071927532]]} fold_4 {'constant_': [[1.2120157684560202]]}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#matbench_mp_is_metal","text":"","title":"matbench_mp_is_metal"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-scores_9","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.5158 0.5069 0.4405 0.5069 fold_1 0.5032 0.4944 0.4277 0.4944 fold_2 0.5069 0.4986 0.4342 0.4986 fold_3 0.5119 0.5032 0.4376 0.5032 fold_4 0.5118 0.5030 0.4367 0.5030","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-score-stats_9","text":"metric mean max min std accuracy 0.5099 0.5158 0.5032 0.0044 balanced_accuracy 0.5012 0.5069 0.4944 0.0043 f1 0.4353 0.4405 0.4277 0.0043 rocauc 0.5012 0.5069 0.4944 0.0043","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-parameters_9","text":"fold params dict fold_0 {'class_prior_': [0.5650724466957239, 0.4349275533042761]} fold_1 {'class_prior_': [0.5650724466957239, 0.4349275533042761]} fold_2 {'class_prior_': [0.5650842266462481, 0.4349157733537519]} fold_3 {'class_prior_': [0.5650775700604305, 0.4349224299395696]} fold_4 {'class_prior_': [0.5650775700604305, 0.4349224299395696]}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#matbench_perovskites","text":"","title":"matbench_perovskites"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-scores_10","text":"fold mae rmse mape* max_error fold_0 0.5672 0.7361 0.7398 3.4868 fold_1 0.5742 0.7618 0.8046 3.3123 fold_2 0.5660 0.7438 0.7674 3.6873 fold_3 0.5614 0.7342 0.7738 3.3906 fold_4 0.5612 0.7362 0.7058 3.5084","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-score-stats_10","text":"metric mean max min std mae 0.5660 0.5742 0.5612 0.0048 rmse 0.7424 0.7618 0.7342 0.0102 mape* 0.7583 0.8046 0.7058 0.0334 max_error 3.4771 3.6873 3.3123 0.1264","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-parameters_10","text":"fold params dict fold_0 {'constant_': [[1.4731871615374454]]} fold_1 {'constant_': [[1.4677308149517898]]} fold_2 {'constant_': [[1.4726720380398888]]} fold_3 {'constant_': [[1.4694433071386122]]} fold_4 {'constant_': [[1.4716264940896784]]}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#matbench_phonons","text":"","title":"matbench_phonons"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-scores_11","text":"fold mae rmse mape* max_error fold_0 337.1003 542.7449 0.8225 3020.7169 fold_1 299.1209 452.2982 0.7977 2702.0312 fold_2 348.2576 545.4772 0.9223 3062.3450 fold_3 325.2402 480.9296 1.0268 3048.7920 fold_4 310.1921 439.3166 0.8936 1970.0884","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-score-stats_11","text":"metric mean max min std mae 323.9822 348.2576 299.1209 17.7269 rmse 492.1533 545.4772 439.3166 44.5176 mape* 0.8926 1.0268 0.7977 0.0810 max_error 2760.7947 3062.3450 1970.0884 417.1581","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-parameters_11","text":"fold params dict fold_0 {'constant_': [[571.8686083004105]]} fold_1 {'constant_': [[583.1997247898747]]} fold_2 {'constant_': [[581.3984519265839]]} fold_3 {'constant_': [[588.7935123141577]]} fold_4 {'constant_': [[581.4972239423439]]}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#matbench_steels","text":"","title":"matbench_steels"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-scores_12","text":"fold mae rmse mape* max_error fold_0 241.4591 293.7245 0.1647 941.0643 fold_1 219.3770 289.2253 0.1550 1064.2831 fold_2 225.7932 291.5410 0.1600 1084.8760 fold_3 241.2035 343.9346 0.1567 1088.0568 fold_4 220.8898 287.6803 0.1576 983.3424","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-score-stats_12","text":"metric mean max min std mae 229.7445 241.4591 219.3770 9.6958 rmse 301.2211 343.9346 287.6803 21.4551 mape* 0.1588 0.1647 0.1550 0.0034 max_error 1032.3245 1088.0568 941.0643 59.3579","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_dummy/#fold-parameters_12","text":"fold params dict fold_0 {'constant_': [[1415.3357429718874]]} fold_1 {'constant_': [[1423.0168674698796]]} fold_2 {'constant_': [[1425.424]]} fold_3 {'constant_': [[1413.0431999999998]]} fold_4 {'constant_': [[1428.1575999999998]]}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/","text":"matbench_v0.1: fictitious_compound Algorithm description: Fictitious compound ensemble where the best prediction from each of the models on a compound-wise basis was taken. Notes: No notes. Raw data download and example notebook available on the matbench repo . References (in bibtex format): ('@article{Dunn2020,\\n' ' doi = {10.1038/s41524-020-00406-3},\\n' ' url = {https://doi.org/10.1038/s41524-020-00406-3},\\n' ' year = {2020},\\n' ' month = sep,\\n' ' publisher = {Springer Science and Business Media {LLC}},\\n' ' volume = {6},\\n' ' number = {1},\\n' ' author = {Alexander Dunn and Qi Wang and Alex Ganose and Daniel Dopp and ' 'Anubhav Jain},\\n' ' title = {Benchmarking materials property prediction methods: the Matbench ' 'test set and Automatminer reference algorithm},\\n' ' journal = {npj Computational Materials}\\n' '}') User metadata: {} Metadata: tasks recorded 13/13 complete? \u2713 composition complete? \u2713 structure complete? \u2713 regression complete? \u2713 classification complete? \u2713 Software Requirements {'python': ['scikit-learn==0.24.1', 'numpy==1.20.1', 'matbench==0.1.0']} Task data: matbench_dielectric Fold scores fold mae rmse mape* max_error fold_0 0.0991 0.5482 0.0281 13.6699 fold_1 0.1590 0.9421 0.0430 19.2249 fold_2 0.3185 2.8847 0.0484 58.7285 fold_3 0.2157 2.1932 0.0330 51.3719 fold_4 0.1985 1.5144 0.0481 27.8634 Fold score stats metric mean max min std mae 0.1981 0.3185 0.0991 0.0723 rmse 1.6165 2.8847 0.5482 0.8423 mape* 0.0401 0.0484 0.0281 0.0082 max_error 34.1717 58.7285 13.6699 17.7899 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_expt_gap Fold scores fold mae rmse mape* max_error fold_0 0.0951 0.2945 0.1159 4.5021 fold_1 0.1009 0.3217 0.1049 6.3544 fold_2 0.1144 0.4185 0.1328 7.8869 fold_3 0.0926 0.2750 0.1130 4.2857 fold_4 0.0977 0.3129 0.1525 3.9071 Fold score stats metric mean max min std mae 0.1001 0.1144 0.0926 0.0076 rmse 0.3245 0.4185 0.2750 0.0497 mape* 0.1238 0.1525 0.1049 0.0170 max_error 5.3872 7.8869 3.9071 1.5081 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_expt_is_metal Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.9919 0.9919 0.9918 0.9919 fold_1 0.9929 0.9929 0.9928 0.9929 fold_2 0.9970 0.9969 0.9969 0.9969 fold_3 0.9929 0.9929 0.9928 0.9929 fold_4 0.9939 0.9939 0.9939 0.9939 Fold score stats metric mean max min std accuracy 0.9937 0.9970 0.9919 0.0017 balanced_accuracy 0.9937 0.9969 0.9919 0.0017 f1 0.9937 0.9969 0.9918 0.0018 rocauc 0.9937 0.9969 0.9919 0.0017 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_glass Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.9798 0.9758 0.9857 0.9758 fold_1 0.9868 0.9799 0.9908 0.9799 fold_2 0.9762 0.9680 0.9833 0.9680 fold_3 0.9771 0.9623 0.9841 0.9623 fold_4 0.9762 0.9590 0.9835 0.9590 Fold score stats metric mean max min std accuracy 0.9792 0.9868 0.9762 0.0040 balanced_accuracy 0.9690 0.9799 0.9590 0.0079 f1 0.9855 0.9908 0.9833 0.0028 rocauc 0.9690 0.9799 0.9590 0.0079 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_jdft2d Fold scores fold mae rmse mape* max_error fold_0 12.9983 26.1572 18.6985 121.9589 fold_1 16.9571 49.9781 0.1214 364.1909 fold_2 38.9201 136.4774 0.4100 845.7528 fold_3 14.0195 38.0753 0.1062 275.8517 fold_4 26.9229 145.8185 0.3647 1519.7424 Fold score stats metric mean max min std mae 21.9636 38.9201 12.9983 9.8048 rmse 79.3013 145.8185 26.1572 51.1417 mape* 3.9402 18.6985 0.1062 7.3802 max_error 625.4994 1519.7424 121.9589 508.3839 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_log_gvrh Fold scores fold mae rmse mape* max_error fold_0 0.0381 0.0721 0.0313 0.8695 fold_1 0.0402 0.0770 0.0328 1.0887 fold_2 0.0386 0.0769 0.0326 0.7799 fold_3 0.0396 0.0755 0.0317 0.8718 fold_4 0.0401 0.0771 0.0320 0.6013 Fold score stats metric mean max min std mae 0.0393 0.0402 0.0381 0.0008 rmse 0.0757 0.0771 0.0721 0.0019 mape* 0.0321 0.0328 0.0313 0.0005 max_error 0.8422 1.0887 0.6013 0.1577 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_log_kvrh Fold scores fold mae rmse mape* max_error fold_0 0.0290 0.0749 0.0199 1.4823 fold_1 0.0312 0.0830 0.0212 1.2273 fold_2 0.0280 0.0691 0.0193 1.0919 fold_3 0.0334 0.0846 0.0255 1.0491 fold_4 0.0293 0.0750 0.0200 1.3125 Fold score stats metric mean max min std mae 0.0302 0.0334 0.0280 0.0019 rmse 0.0773 0.0846 0.0691 0.0057 mape* 0.0212 0.0255 0.0193 0.0023 max_error 1.2326 1.4823 1.0491 0.1563 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_mp_e_form Fold scores fold mae rmse mape* max_error fold_0 0.0133 0.0429 0.0961 3.5487 fold_1 0.0139 0.0393 0.0839 2.3544 fold_2 0.0133 0.0346 0.0833 1.4278 fold_3 0.0138 0.0373 0.1080 1.2195 fold_4 0.0132 0.0347 0.1057 1.0446 Fold score stats metric mean max min std mae 0.0135 0.0139 0.0132 0.0003 rmse 0.0378 0.0429 0.0346 0.0031 mape* 0.0954 0.1080 0.0833 0.0104 max_error 1.9190 3.5487 1.0446 0.9317 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_mp_gap Fold scores fold mae rmse mape* max_error fold_0 0.0794 0.2179 0.9938 4.9652 fold_1 0.0776 0.2125 1.1647 4.9121 fold_2 0.0776 0.2063 2.5293 4.1238 fold_3 0.0776 0.2014 2.9035 4.9277 fold_4 0.0812 0.2210 2.2283 4.0632 Fold score stats metric mean max min std mae 0.0787 0.0812 0.0776 0.0014 rmse 0.2118 0.2210 0.2014 0.0072 mape* 1.9639 2.9035 0.9938 0.7553 max_error 4.5984 4.9652 4.0632 0.4131 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_mp_is_metal Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.9683 0.9662 0.9630 0.9662 fold_1 0.9614 0.9589 0.9549 0.9589 fold_2 0.9710 0.9666 0.9655 0.9666 fold_3 0.9641 0.9616 0.9581 0.9616 fold_4 0.9633 0.9603 0.9570 0.9603 Fold score stats metric mean max min std accuracy 0.9656 0.9710 0.9614 0.0035 balanced_accuracy 0.9627 0.9666 0.9589 0.0031 f1 0.9597 0.9655 0.9549 0.0039 rocauc 0.9627 0.9666 0.9589 0.0031 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_perovskites Fold scores fold mae rmse mape* max_error fold_0 0.0226 0.0455 0.0230 0.7848 fold_1 0.0234 0.0478 0.0237 0.7967 fold_2 0.0208 0.0358 0.0211 0.6265 fold_3 0.0211 0.0379 0.0221 0.5825 fold_4 0.0214 0.0408 0.0201 0.7664 Fold score stats metric mean max min std mae 0.0218 0.0234 0.0208 0.0010 rmse 0.0416 0.0478 0.0358 0.0045 mape* 0.0220 0.0237 0.0201 0.0013 max_error 0.7114 0.7967 0.5825 0.0889 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_phonons Fold scores fold mae rmse mape* max_error fold_0 13.9451 31.5923 0.0250 394.6285 fold_1 13.4878 28.3736 0.0260 277.0146 fold_2 14.8111 33.7098 0.0261 226.4931 fold_3 14.1058 32.4059 0.0273 252.1160 fold_4 12.1421 21.0252 0.0241 190.2821 Fold score stats metric mean max min std mae 13.6984 14.8111 12.1421 0.8867 rmse 29.4214 33.7098 21.0252 4.5520 mape* 0.0257 0.0273 0.0241 0.0011 max_error 268.1069 394.6285 190.2821 69.4711 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {} matbench_steels Fold scores fold mae rmse mape* max_error fold_0 65.0725 119.1001 0.0403 553.2987 fold_1 46.6963 71.5273 0.0348 350.5612 fold_2 54.3692 91.1689 0.0365 370.1786 fold_3 50.6138 76.2675 0.0362 295.1635 fold_4 50.1129 85.8563 0.0391 356.5724 Fold score stats metric mean max min std mae 53.3729 65.0725 46.6963 6.3355 rmse 88.7840 119.1001 71.5273 16.6600 mape* 0.0374 0.0403 0.0348 0.0020 max_error 385.1549 553.2987 295.1635 87.8735 Fold parameters fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"matbench_v0.1: fictitious_compound"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#matbench_v01-fictitious_compound","text":"","title":"matbench_v0.1: fictitious_compound"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#algorithm-description","text":"Fictitious compound ensemble where the best prediction from each of the models on a compound-wise basis was taken.","title":"Algorithm description:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#notes","text":"No notes. Raw data download and example notebook available on the matbench repo .","title":"Notes:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#references-in-bibtex-format","text":"('@article{Dunn2020,\\n' ' doi = {10.1038/s41524-020-00406-3},\\n' ' url = {https://doi.org/10.1038/s41524-020-00406-3},\\n' ' year = {2020},\\n' ' month = sep,\\n' ' publisher = {Springer Science and Business Media {LLC}},\\n' ' volume = {6},\\n' ' number = {1},\\n' ' author = {Alexander Dunn and Qi Wang and Alex Ganose and Daniel Dopp and ' 'Anubhav Jain},\\n' ' title = {Benchmarking materials property prediction methods: the Matbench ' 'test set and Automatminer reference algorithm},\\n' ' journal = {npj Computational Materials}\\n' '}')","title":"References (in bibtex format):"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#user-metadata","text":"{}","title":"User metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#metadata","text":"tasks recorded 13/13 complete? \u2713 composition complete? \u2713 structure complete? \u2713 regression complete? \u2713 classification complete? \u2713","title":"Metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#software-requirements","text":"{'python': ['scikit-learn==0.24.1', 'numpy==1.20.1', 'matbench==0.1.0']}","title":"Software Requirements"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#task-data","text":"","title":"Task data:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#matbench_dielectric","text":"","title":"matbench_dielectric"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-scores","text":"fold mae rmse mape* max_error fold_0 0.0991 0.5482 0.0281 13.6699 fold_1 0.1590 0.9421 0.0430 19.2249 fold_2 0.3185 2.8847 0.0484 58.7285 fold_3 0.2157 2.1932 0.0330 51.3719 fold_4 0.1985 1.5144 0.0481 27.8634","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-score-stats","text":"metric mean max min std mae 0.1981 0.3185 0.0991 0.0723 rmse 1.6165 2.8847 0.5482 0.8423 mape* 0.0401 0.0484 0.0281 0.0082 max_error 34.1717 58.7285 13.6699 17.7899","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-parameters","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#matbench_expt_gap","text":"","title":"matbench_expt_gap"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-scores_1","text":"fold mae rmse mape* max_error fold_0 0.0951 0.2945 0.1159 4.5021 fold_1 0.1009 0.3217 0.1049 6.3544 fold_2 0.1144 0.4185 0.1328 7.8869 fold_3 0.0926 0.2750 0.1130 4.2857 fold_4 0.0977 0.3129 0.1525 3.9071","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-score-stats_1","text":"metric mean max min std mae 0.1001 0.1144 0.0926 0.0076 rmse 0.3245 0.4185 0.2750 0.0497 mape* 0.1238 0.1525 0.1049 0.0170 max_error 5.3872 7.8869 3.9071 1.5081","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-parameters_1","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#matbench_expt_is_metal","text":"","title":"matbench_expt_is_metal"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-scores_2","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.9919 0.9919 0.9918 0.9919 fold_1 0.9929 0.9929 0.9928 0.9929 fold_2 0.9970 0.9969 0.9969 0.9969 fold_3 0.9929 0.9929 0.9928 0.9929 fold_4 0.9939 0.9939 0.9939 0.9939","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-score-stats_2","text":"metric mean max min std accuracy 0.9937 0.9970 0.9919 0.0017 balanced_accuracy 0.9937 0.9969 0.9919 0.0017 f1 0.9937 0.9969 0.9918 0.0018 rocauc 0.9937 0.9969 0.9919 0.0017","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-parameters_2","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#matbench_glass","text":"","title":"matbench_glass"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-scores_3","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.9798 0.9758 0.9857 0.9758 fold_1 0.9868 0.9799 0.9908 0.9799 fold_2 0.9762 0.9680 0.9833 0.9680 fold_3 0.9771 0.9623 0.9841 0.9623 fold_4 0.9762 0.9590 0.9835 0.9590","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-score-stats_3","text":"metric mean max min std accuracy 0.9792 0.9868 0.9762 0.0040 balanced_accuracy 0.9690 0.9799 0.9590 0.0079 f1 0.9855 0.9908 0.9833 0.0028 rocauc 0.9690 0.9799 0.9590 0.0079","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-parameters_3","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#matbench_jdft2d","text":"","title":"matbench_jdft2d"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-scores_4","text":"fold mae rmse mape* max_error fold_0 12.9983 26.1572 18.6985 121.9589 fold_1 16.9571 49.9781 0.1214 364.1909 fold_2 38.9201 136.4774 0.4100 845.7528 fold_3 14.0195 38.0753 0.1062 275.8517 fold_4 26.9229 145.8185 0.3647 1519.7424","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-score-stats_4","text":"metric mean max min std mae 21.9636 38.9201 12.9983 9.8048 rmse 79.3013 145.8185 26.1572 51.1417 mape* 3.9402 18.6985 0.1062 7.3802 max_error 625.4994 1519.7424 121.9589 508.3839","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-parameters_4","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#matbench_log_gvrh","text":"","title":"matbench_log_gvrh"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-scores_5","text":"fold mae rmse mape* max_error fold_0 0.0381 0.0721 0.0313 0.8695 fold_1 0.0402 0.0770 0.0328 1.0887 fold_2 0.0386 0.0769 0.0326 0.7799 fold_3 0.0396 0.0755 0.0317 0.8718 fold_4 0.0401 0.0771 0.0320 0.6013","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-score-stats_5","text":"metric mean max min std mae 0.0393 0.0402 0.0381 0.0008 rmse 0.0757 0.0771 0.0721 0.0019 mape* 0.0321 0.0328 0.0313 0.0005 max_error 0.8422 1.0887 0.6013 0.1577","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-parameters_5","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#matbench_log_kvrh","text":"","title":"matbench_log_kvrh"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-scores_6","text":"fold mae rmse mape* max_error fold_0 0.0290 0.0749 0.0199 1.4823 fold_1 0.0312 0.0830 0.0212 1.2273 fold_2 0.0280 0.0691 0.0193 1.0919 fold_3 0.0334 0.0846 0.0255 1.0491 fold_4 0.0293 0.0750 0.0200 1.3125","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-score-stats_6","text":"metric mean max min std mae 0.0302 0.0334 0.0280 0.0019 rmse 0.0773 0.0846 0.0691 0.0057 mape* 0.0212 0.0255 0.0193 0.0023 max_error 1.2326 1.4823 1.0491 0.1563","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-parameters_6","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#matbench_mp_e_form","text":"","title":"matbench_mp_e_form"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-scores_7","text":"fold mae rmse mape* max_error fold_0 0.0133 0.0429 0.0961 3.5487 fold_1 0.0139 0.0393 0.0839 2.3544 fold_2 0.0133 0.0346 0.0833 1.4278 fold_3 0.0138 0.0373 0.1080 1.2195 fold_4 0.0132 0.0347 0.1057 1.0446","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-score-stats_7","text":"metric mean max min std mae 0.0135 0.0139 0.0132 0.0003 rmse 0.0378 0.0429 0.0346 0.0031 mape* 0.0954 0.1080 0.0833 0.0104 max_error 1.9190 3.5487 1.0446 0.9317","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-parameters_7","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#matbench_mp_gap","text":"","title":"matbench_mp_gap"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-scores_8","text":"fold mae rmse mape* max_error fold_0 0.0794 0.2179 0.9938 4.9652 fold_1 0.0776 0.2125 1.1647 4.9121 fold_2 0.0776 0.2063 2.5293 4.1238 fold_3 0.0776 0.2014 2.9035 4.9277 fold_4 0.0812 0.2210 2.2283 4.0632","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-score-stats_8","text":"metric mean max min std mae 0.0787 0.0812 0.0776 0.0014 rmse 0.2118 0.2210 0.2014 0.0072 mape* 1.9639 2.9035 0.9938 0.7553 max_error 4.5984 4.9652 4.0632 0.4131","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-parameters_8","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#matbench_mp_is_metal","text":"","title":"matbench_mp_is_metal"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-scores_9","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.9683 0.9662 0.9630 0.9662 fold_1 0.9614 0.9589 0.9549 0.9589 fold_2 0.9710 0.9666 0.9655 0.9666 fold_3 0.9641 0.9616 0.9581 0.9616 fold_4 0.9633 0.9603 0.9570 0.9603","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-score-stats_9","text":"metric mean max min std accuracy 0.9656 0.9710 0.9614 0.0035 balanced_accuracy 0.9627 0.9666 0.9589 0.0031 f1 0.9597 0.9655 0.9549 0.0039 rocauc 0.9627 0.9666 0.9589 0.0031","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-parameters_9","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#matbench_perovskites","text":"","title":"matbench_perovskites"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-scores_10","text":"fold mae rmse mape* max_error fold_0 0.0226 0.0455 0.0230 0.7848 fold_1 0.0234 0.0478 0.0237 0.7967 fold_2 0.0208 0.0358 0.0211 0.6265 fold_3 0.0211 0.0379 0.0221 0.5825 fold_4 0.0214 0.0408 0.0201 0.7664","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-score-stats_10","text":"metric mean max min std mae 0.0218 0.0234 0.0208 0.0010 rmse 0.0416 0.0478 0.0358 0.0045 mape* 0.0220 0.0237 0.0201 0.0013 max_error 0.7114 0.7967 0.5825 0.0889","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-parameters_10","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#matbench_phonons","text":"","title":"matbench_phonons"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-scores_11","text":"fold mae rmse mape* max_error fold_0 13.9451 31.5923 0.0250 394.6285 fold_1 13.4878 28.3736 0.0260 277.0146 fold_2 14.8111 33.7098 0.0261 226.4931 fold_3 14.1058 32.4059 0.0273 252.1160 fold_4 12.1421 21.0252 0.0241 190.2821","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-score-stats_11","text":"metric mean max min std mae 13.6984 14.8111 12.1421 0.8867 rmse 29.4214 33.7098 21.0252 4.5520 mape* 0.0257 0.0273 0.0241 0.0011 max_error 268.1069 394.6285 190.2821 69.4711","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-parameters_11","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#matbench_steels","text":"","title":"matbench_steels"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-scores_12","text":"fold mae rmse mape* max_error fold_0 65.0725 119.1001 0.0403 553.2987 fold_1 46.6963 71.5273 0.0348 350.5612 fold_2 54.3692 91.1689 0.0365 370.1786 fold_3 50.6138 76.2675 0.0362 295.1635 fold_4 50.1129 85.8563 0.0391 356.5724","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-score-stats_12","text":"metric mean max min std mae 53.3729 65.0725 46.6963 6.3355 rmse 88.7840 119.1001 71.5273 16.6600 mape* 0.0374 0.0403 0.0348 0.0020 max_error 385.1549 553.2987 295.1635 87.8735","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_compound_wise/#fold-parameters_12","text":"fold params dict fold_0 {} fold_1 {} fold_2 {} fold_3 {} fold_4 {}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/","text":"matbench_v0.1: fictitious_model Algorithm description: Fictitious model ensemble where (intentionally p-hacked) optimal weights were found based on ~130k SOBOL-sampled weight combinations of the various models for each of the task/fold combinations. Also referred to as a weighted ensemble average. Notes: No notes. Raw data download and example notebook available on the matbench repo . References (in bibtex format): ('@article{Dunn2020,\\n' ' doi = {10.1038/s41524-020-00406-3},\\n' ' url = {https://doi.org/10.1038/s41524-020-00406-3},\\n' ' year = {2020},\\n' ' month = sep,\\n' ' publisher = {Springer Science and Business Media {LLC}},\\n' ' volume = {6},\\n' ' number = {1},\\n' ' author = {Alexander Dunn and Qi Wang and Alex Ganose and Daniel Dopp and ' 'Anubhav Jain},\\n' ' title = {Benchmarking materials property prediction methods: the Matbench ' 'test set and Automatminer reference algorithm},\\n' ' journal = {npj Computational Materials}\\n' '}') User metadata: {} Metadata: tasks recorded 13/13 complete? \u2713 composition complete? \u2713 structure complete? \u2713 regression complete? \u2713 classification complete? \u2713 Software Requirements {'python': ['scikit-learn==0.24.1', 'numpy==1.20.1', 'matbench==0.1.0']} Task data: matbench_dielectric Fold scores fold mae rmse mape* max_error fold_0 0.1736 0.6474 0.0587 14.0358 fold_1 0.2564 1.0394 0.0836 19.4483 fold_2 0.3997 2.9327 0.0801 59.0262 fold_3 0.2760 2.2449 0.0551 52.4163 fold_4 0.2931 1.5846 0.0887 28.0063 Fold score stats metric mean max min std mae 0.2798 0.3997 0.1736 0.0726 rmse 1.6898 2.9327 0.6474 0.8214 mape* 0.0733 0.0887 0.0551 0.0136 max_error 34.5866 59.0262 14.0358 17.9443 Fold parameters fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.20575117871423193, 'matbench_v0.1_alignn': 0.36503077350276164, 'matbench_v0.1_automatminer_expressv2020': 0.03693776452273196, 'matbench_v0.1_modnet_v0.1.10': 0.3922802832... fold_1 {'matbench_v0.1_CrabNet': 0.15967213462626922, 'matbench_v0.1_alignn': 0.019957298927968287, 'matbench_v0.1_automatminer_expressv2020': 0.22950980783361521, 'matbench_v0.1_modnet_v0.1.10': 0.590860758... fold_2 {'matbench_v0.1_CrabNet': 0.26752136323219017, 'matbench_v0.1_alignn': 0.00022466871513461952, 'matbench_v0.1_automatminer_expressv2020': 0.32256934842943974, 'matbench_v0.1_modnet_v0.1.10': 0.4096846... fold_3 {'matbench_v0.1_CrabNet': 0.07708241986797697, 'matbench_v0.1_alignn': 0.22901497809874913, 'matbench_v0.1_automatminer_expressv2020': 0.08756035007079653, 'matbench_v0.1_modnet_v0.1.10': 0.6063422519... fold_4 {'matbench_v0.1_CrabNet': 0.40492091223579596, 'matbench_v0.1_alignn': 0.01928753160915624, 'matbench_v0.1_automatminer_expressv2020': 0.21645513538860056, 'matbench_v0.1_modnet_v0.1.10': 0.3593364207... matbench_expt_gap Fold scores fold mae rmse mape* max_error fold_0 0.3135 0.6275 0.3085 5.1756 fold_1 0.3282 0.6640 0.2527 6.7164 fold_2 0.3390 0.7782 0.3133 8.8678 fold_3 0.3086 0.6647 0.2919 5.4599 fold_4 0.3199 0.6526 0.3583 5.6826 Fold score stats metric mean max min std mae 0.3218 0.3390 0.3086 0.0108 rmse 0.6774 0.7782 0.6275 0.0522 mape* 0.3049 0.3583 0.2527 0.0341 max_error 6.3805 8.8678 5.1756 1.3480 Fold parameters fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.25328331229913154, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.19279284485612228, 'matbench_v0.1_modnet_v0.1.10': 0.5539238428447462, 'target'... fold_1 {'matbench_v0.1_CrabNet': 0.5107847473559838, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.1463424463386866, 'matbench_v0.1_modnet_v0.1.10': 0.3428728063053297, 'target': ... fold_2 {'matbench_v0.1_CrabNet': 0.6865386216198438, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.12547388312292732, 'matbench_v0.1_modnet_v0.1.10': 0.18798749525722885, 'target'... fold_3 {'matbench_v0.1_CrabNet': 0.4973765410211031, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.07069597545395823, 'matbench_v0.1_modnet_v0.1.10': 0.43192748352493865, 'target'... fold_4 {'matbench_v0.1_CrabNet': 0.24897538653903542, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.14684490360565391, 'matbench_v0.1_modnet_v0.1.10': 0.6041797098553107, 'target'... matbench_expt_is_metal Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.9218 0.9218 0.9205 0.9218 fold_1 0.9157 0.9156 0.9145 0.9156 fold_2 0.9207 0.9207 0.9193 0.9207 fold_3 0.9228 0.9228 0.9223 0.9228 fold_4 0.9238 0.9238 0.9235 0.9238 Fold score stats metric mean max min std accuracy 0.9210 0.9238 0.9157 0.0028 balanced_accuracy 0.9209 0.9238 0.9156 0.0028 f1 0.9200 0.9235 0.9145 0.0031 rocauc 0.9209 0.9238 0.9156 0.0028 Fold parameters fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999994636097223, 'matbench_v0.1_modnet_v0.1.10': 5.363902777989782e-07, 'target': 0.0} fold_1 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999975241578845, 'matbench_v0.1_modnet_v0.1.10': 2.4758421155135045e-06, 'target': 0.0} fold_2 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999913986660386, 'matbench_v0.1_modnet_v0.1.10': 8.601333961434646e-06, 'target': 0.0} fold_3 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999938365545721, 'matbench_v0.1_modnet_v0.1.10': 6.16344542788228e-06, 'target': 0.0} fold_4 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999802552040563, 'matbench_v0.1_modnet_v0.1.10': 1.974479594366695e-05, 'target': 0.0} matbench_glass Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.8283 0.8441 0.8697 0.8441 fold_1 0.8125 0.8383 0.8548 0.8383 fold_2 0.8574 0.8546 0.8956 0.8546 fold_3 0.9173 0.8742 0.9437 0.8742 fold_4 0.9375 0.8921 0.9579 0.8921 Fold score stats metric mean max min std accuracy 0.8706 0.9375 0.8125 0.0490 balanced_accuracy 0.8607 0.8921 0.8383 0.0199 f1 0.9043 0.9579 0.8548 0.0404 rocauc 0.8607 0.8921 0.8383 0.0199 Fold parameters fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999959766363284, 'matbench_v0.1_modnet_v0.1.10': 4.023363671606479e-06, 'target': 0.0} fold_1 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999933773656292, 'matbench_v0.1_modnet_v0.1.10': 6.622634370803024e-06, 'target': 0.0} fold_2 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999955134857657, 'matbench_v0.1_modnet_v0.1.10': 4.486514234348761e-06, 'target': 0.0} fold_3 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999873715803423, 'matbench_v0.1_modnet_v0.1.10': 1.2628419657670107e-05, 'target': 0.0} fold_4 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999923643049671, 'matbench_v0.1_modnet_v0.1.10': 7.635695032953968e-06, 'target': 0.0} matbench_jdft2d Fold scores fold mae rmse mape* max_error fold_0 25.6817 45.6908 20.8585 201.4303 fold_1 27.7426 63.3403 0.2270 367.3741 fold_2 50.8337 147.0971 0.5943 846.5039 fold_3 24.9264 50.6733 0.2358 302.0936 fold_4 37.7280 151.3688 0.4895 1533.5975 Fold score stats metric mean max min std mae 33.3825 50.8337 24.9264 9.8594 rmse 91.6341 151.3688 45.6908 47.3993 mape* 4.4810 20.8585 0.2270 8.1900 max_error 650.1999 1533.5975 201.4303 494.2650 Fold parameters fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.15342414366645282, 'matbench_v0.1_alignn': 0.03181279451221846, 'matbench_v0.1_automatminer_expressv2020': 0.446224463518545, 'matbench_v0.1_modnet_v0.1.10': 0.368538598302... fold_1 {'matbench_v0.1_CrabNet': 0.01712548352219769, 'matbench_v0.1_alignn': 0.0008425195852314279, 'matbench_v0.1_automatminer_expressv2020': 0.028448329464033657, 'matbench_v0.1_modnet_v0.1.10': 0.9535836... fold_2 {'matbench_v0.1_CrabNet': 0.0005454254242677881, 'matbench_v0.1_alignn': 0.18050122343170372, 'matbench_v0.1_automatminer_expressv2020': 0.02001052727974478, 'matbench_v0.1_modnet_v0.1.10': 0.79894282... fold_3 {'matbench_v0.1_CrabNet': 0.17966803253404068, 'matbench_v0.1_alignn': 0.1828427612410428, 'matbench_v0.1_automatminer_expressv2020': 0.2321150227627915, 'matbench_v0.1_modnet_v0.1.10': 0.405374183462... fold_4 {'matbench_v0.1_CrabNet': 0.0004628740793511039, 'matbench_v0.1_alignn': 0.25457773015822455, 'matbench_v0.1_automatminer_expressv2020': 0.13933131094614346, 'matbench_v0.1_modnet_v0.1.10': 0.60562808... matbench_log_gvrh Fold scores fold mae rmse mape* max_error fold_0 0.0671 0.1030 0.0533 0.9089 fold_1 0.0685 0.1081 0.0551 1.1515 fold_2 0.0676 0.1055 0.0543 0.8141 fold_3 0.0676 0.1046 0.0528 0.8935 fold_4 0.0669 0.1052 0.0525 0.7486 Fold score stats metric mean max min std mae 0.0675 0.0685 0.0669 0.0006 rmse 0.1053 0.1081 0.1030 0.0017 mape* 0.0536 0.0551 0.0525 0.0010 max_error 0.9033 1.1515 0.7486 0.1368 Fold parameters fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.0005469296890628186, 'matbench_v0.1_alignn': 0.5790363091121159, 'matbench_v0.1_automatminer_expressv2020': 0.016475670548405624, 'matbench_v0.1_modnet_v0.1.10': 0.40394109... fold_1 {'matbench_v0.1_CrabNet': 0.0008720753148568775, 'matbench_v0.1_alignn': 0.5398068953552143, 'matbench_v0.1_automatminer_expressv2020': 0.00293608420564664, 'matbench_v0.1_modnet_v0.1.10': 0.456384945... fold_2 {'matbench_v0.1_CrabNet': 0.00397918306233581, 'matbench_v0.1_alignn': 0.5905921519766839, 'matbench_v0.1_automatminer_expressv2020': 0.0013491061186301629, 'matbench_v0.1_modnet_v0.1.10': 0.404079558... fold_3 {'matbench_v0.1_CrabNet': 0.0036578251362713926, 'matbench_v0.1_alignn': 0.5745243458793965, 'matbench_v0.1_automatminer_expressv2020': 0.0007068555241777903, 'matbench_v0.1_modnet_v0.1.10': 0.4211109... fold_4 {'matbench_v0.1_CrabNet': 0.004858865465360498, 'matbench_v0.1_alignn': 0.510039374033992, 'matbench_v0.1_automatminer_expressv2020': 0.0015554463592616025, 'matbench_v0.1_modnet_v0.1.10': 0.483546314... matbench_log_kvrh Fold scores fold mae rmse mape* max_error fold_0 0.0505 0.0987 0.0335 1.5820 fold_1 0.0522 0.1061 0.0343 1.3186 fold_2 0.0477 0.0922 0.0318 1.1371 fold_3 0.0554 0.1091 0.0400 1.0836 fold_4 0.0514 0.1001 0.0345 1.3635 Fold score stats metric mean max min std mae 0.0514 0.0554 0.0477 0.0025 rmse 0.1012 0.1091 0.0922 0.0059 mape* 0.0348 0.0400 0.0318 0.0027 max_error 1.2970 1.5820 1.0836 0.1773 Fold parameters fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.0021430310672584164, 'matbench_v0.1_alignn': 0.44012112653432134, 'matbench_v0.1_automatminer_expressv2020': 0.037587235031681716, 'matbench_v0.1_modnet_v0.1.10': 0.5201486... fold_1 {'matbench_v0.1_CrabNet': 0.011125181281904034, 'matbench_v0.1_alignn': 0.4454571849332936, 'matbench_v0.1_automatminer_expressv2020': 0.04526796006356993, 'matbench_v0.1_modnet_v0.1.10': 0.4981496737... fold_2 {'matbench_v0.1_CrabNet': 0.035214279444113504, 'matbench_v0.1_alignn': 0.4542466438434961, 'matbench_v0.1_automatminer_expressv2020': 0.002799658586453697, 'matbench_v0.1_modnet_v0.1.10': 0.507739418... fold_3 {'matbench_v0.1_CrabNet': 0.001317193865459212, 'matbench_v0.1_alignn': 0.3612906935102406, 'matbench_v0.1_automatminer_expressv2020': 0.12549697847265245, 'matbench_v0.1_modnet_v0.1.10': 0.5118951341... fold_4 {'matbench_v0.1_CrabNet': 0.0013550637888710693, 'matbench_v0.1_alignn': 0.3972992478093793, 'matbench_v0.1_automatminer_expressv2020': 0.1239877770792496, 'matbench_v0.1_modnet_v0.1.10': 0.4773579113... matbench_mp_e_form Fold scores fold mae rmse mape* max_error fold_0 0.0217 0.0631 0.1491 3.5983 fold_1 0.0220 0.0533 0.1330 2.8653 fold_2 0.0210 0.0496 0.1395 2.0082 fold_3 0.0219 0.0543 0.1831 1.6367 fold_4 0.0211 0.0500 0.2482 1.3868 Fold score stats metric mean max min std mae 0.0215 0.0220 0.0210 0.0004 rmse 0.0541 0.0631 0.0496 0.0049 mape* 0.1706 0.2482 0.1330 0.0425 max_error 2.2991 3.5983 1.3868 0.8203 Fold parameters fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.008499291160418577, 'matbench_v0.1_alignn': 0.9032029811240486, 'matbench_v0.1_automatminer_expressv2020': 0.007458152631024239, 'matbench_v0.1_modnet_v0.1.10': 0.080839575... fold_1 {'matbench_v0.1_CrabNet': 0.004977249815968251, 'matbench_v0.1_alignn': 0.9004650742942951, 'matbench_v0.1_automatminer_expressv2020': 0.002291709303718508, 'matbench_v0.1_modnet_v0.1.10': 0.092265966... fold_2 {'matbench_v0.1_CrabNet': 0.017394782923472796, 'matbench_v0.1_alignn': 0.9120658902185469, 'matbench_v0.1_automatminer_expressv2020': 0.0015317067352738955, 'matbench_v0.1_modnet_v0.1.10': 0.06900762... fold_3 {'matbench_v0.1_CrabNet': 0.008004151413010018, 'matbench_v0.1_alignn': 0.9272266729313643, 'matbench_v0.1_automatminer_expressv2020': 0.006611717219836468, 'matbench_v0.1_modnet_v0.1.10': 0.058157458... fold_4 {'matbench_v0.1_CrabNet': 0.006685517382035754, 'matbench_v0.1_alignn': 0.9329780645736975, 'matbench_v0.1_automatminer_expressv2020': 0.01291754489989797, 'matbench_v0.1_modnet_v0.1.10': 0.0474188731... matbench_mp_gap Fold scores fold mae rmse mape* max_error fold_0 0.1770 0.4025 2.4955 6.1745 fold_1 0.1774 0.4018 2.5452 6.3716 fold_2 0.1802 0.4059 4.1836 5.5312 fold_3 0.1756 0.4091 5.2175 6.0348 fold_4 0.1803 0.4188 4.8511 5.2131 Fold score stats metric mean max min std mae 0.1781 0.1803 0.1756 0.0018 rmse 0.4076 0.4188 0.4018 0.0062 mape* 3.8586 5.2175 2.4955 1.1420 max_error 5.8650 6.3716 5.2131 0.4284 Fold parameters fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.03134890898326961, 'matbench_v0.1_alignn': 0.7111209827754947, 'matbench_v0.1_automatminer_expressv2020': 0.005786178299136875, 'matbench_v0.1_modnet_v0.1.10': 0.2517439299... fold_1 {'matbench_v0.1_CrabNet': 0.0407591733831172, 'matbench_v0.1_alignn': 0.6975505799075838, 'matbench_v0.1_automatminer_expressv2020': 0.003201954717881077, 'matbench_v0.1_modnet_v0.1.10': 0.25848829199... fold_2 {'matbench_v0.1_CrabNet': 0.036284616969791905, 'matbench_v0.1_alignn': 0.6942018172396998, 'matbench_v0.1_automatminer_expressv2020': 0.00041732299008234003, 'matbench_v0.1_modnet_v0.1.10': 0.2690962... fold_3 {'matbench_v0.1_CrabNet': 0.03522104460336443, 'matbench_v0.1_alignn': 0.8009824659470643, 'matbench_v0.1_automatminer_expressv2020': 0.002089889799254658, 'matbench_v0.1_modnet_v0.1.10': 0.1617065996... fold_4 {'matbench_v0.1_CrabNet': 0.04105720156963323, 'matbench_v0.1_alignn': 0.7549700155809074, 'matbench_v0.1_automatminer_expressv2020': 0.0048447390836097535, 'matbench_v0.1_modnet_v0.1.10': 0.199128043... matbench_mp_is_metal Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.9173 0.9156 0.9047 0.9156 fold_1 0.9135 0.9117 0.9003 0.9117 fold_2 0.9146 0.9125 0.9013 0.9125 fold_3 0.9146 0.9108 0.8998 0.9108 fold_4 0.9147 0.9123 0.9012 0.9123 Fold score stats metric mean max min std accuracy 0.9149 0.9173 0.9135 0.0013 balanced_accuracy 0.9126 0.9156 0.9108 0.0016 f1 0.9015 0.9047 0.8998 0.0017 rocauc 0.9126 0.9156 0.9108 0.0016 Fold parameters fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.9953164044264534, 'matbench_v0.1_automatminer_expressv2020': 0.003963882720028548, 'matbench_v0.1_modnet_v0.1.10': 0.0007197128535180265, 'targ... fold_1 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.93404698679432, 'matbench_v0.1_automatminer_expressv2020': 0.06591254576545874, 'matbench_v0.1_modnet_v0.1.10': 4.0467440221240175e-05, 'target... fold_2 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.935298137056003, 'matbench_v0.1_automatminer_expressv2020': 0.06459068443951696, 'matbench_v0.1_modnet_v0.1.10': 0.00011117850448007413, 'targe... fold_3 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 3.765834317927802e-05, 'matbench_v0.1_automatminer_expressv2020': 0.9998516565467143, 'matbench_v0.1_modnet_v0.1.10': 0.00011068511010648864, 'ta... fold_4 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.9606082727476839, 'matbench_v0.1_automatminer_expressv2020': 0.03919544417457493, 'matbench_v0.1_modnet_v0.1.10': 0.00019628307774114157, 'targ... matbench_perovskites Fold scores fold mae rmse mape* max_error fold_0 0.0300 0.0583 0.0307 0.8302 fold_1 0.0332 0.0644 0.0338 0.9146 fold_2 0.0295 0.0519 0.0296 0.8127 fold_3 0.0302 0.0545 0.0298 0.7940 fold_4 0.0305 0.0569 0.0283 0.8359 Fold score stats metric mean max min std mae 0.0307 0.0332 0.0295 0.0013 rmse 0.0572 0.0644 0.0519 0.0042 mape* 0.0305 0.0338 0.0283 0.0018 max_error 0.8375 0.9146 0.7940 0.0412 Fold parameters fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.009547584180147844, 'matbench_v0.1_alignn': 0.9848694217426553, 'matbench_v0.1_automatminer_expressv2020': 0.0016440523111006303, 'matbench_v0.1_modnet_v0.1.10': 0.00393894... fold_1 {'matbench_v0.1_CrabNet': 0.01535995631615898, 'matbench_v0.1_alignn': 0.9490765455180638, 'matbench_v0.1_automatminer_expressv2020': 0.02139321931280439, 'matbench_v0.1_modnet_v0.1.10': 0.01417027885... fold_2 {'matbench_v0.1_CrabNet': 0.004227415890322362, 'matbench_v0.1_alignn': 0.9472138966649386, 'matbench_v0.1_automatminer_expressv2020': 0.027487021706203058, 'matbench_v0.1_modnet_v0.1.10': 0.021071665... fold_3 {'matbench_v0.1_CrabNet': 0.0011844250315664847, 'matbench_v0.1_alignn': 0.9684742126005914, 'matbench_v0.1_automatminer_expressv2020': 0.029884743936693152, 'matbench_v0.1_modnet_v0.1.10': 0.00045661... fold_4 {'matbench_v0.1_CrabNet': 0.005623378454120186, 'matbench_v0.1_alignn': 0.9059313272448253, 'matbench_v0.1_automatminer_expressv2020': 0.011448435884939327, 'matbench_v0.1_modnet_v0.1.10': 0.076996858... matbench_phonons Fold scores fold mae rmse mape* max_error fold_0 31.6557 65.3053 0.0564 629.1442 fold_1 26.9097 45.6052 0.0542 353.3826 fold_2 28.6332 51.7256 0.0536 294.6454 fold_3 28.1886 61.4775 0.0549 513.0511 fold_4 23.9047 44.1742 0.0447 449.6660 Fold score stats metric mean max min std mae 27.8584 31.6557 23.9047 2.5164 rmse 53.6576 65.3053 44.1742 8.4300 mape* 0.0527 0.0564 0.0447 0.0041 max_error 447.9778 629.1442 294.6454 117.9133 Fold parameters fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.0018203153633914477, 'matbench_v0.1_alignn': 0.6486914675578309, 'matbench_v0.1_automatminer_expressv2020': 0.08593035173170938, 'matbench_v0.1_modnet_v0.1.10': 0.263557865... fold_1 {'matbench_v0.1_CrabNet': 5.294311262819687e-05, 'matbench_v0.1_alignn': 0.7044278252813406, 'matbench_v0.1_automatminer_expressv2020': 0.11910972769429452, 'matbench_v0.1_modnet_v0.1.10': 0.176409503... fold_2 {'matbench_v0.1_CrabNet': 0.003740616371621964, 'matbench_v0.1_alignn': 0.7214054796467301, 'matbench_v0.1_automatminer_expressv2020': 0.008481957920670979, 'matbench_v0.1_modnet_v0.1.10': 0.266371946... fold_3 {'matbench_v0.1_CrabNet': 0.08617214017244104, 'matbench_v0.1_alignn': 0.7045203684732354, 'matbench_v0.1_automatminer_expressv2020': 0.0006473946247970298, 'matbench_v0.1_modnet_v0.1.10': 0.208660096... fold_4 {'matbench_v0.1_CrabNet': 0.0061383217259627495, 'matbench_v0.1_alignn': 0.6551412667140507, 'matbench_v0.1_automatminer_expressv2020': 0.053142546033970374, 'matbench_v0.1_modnet_v0.1.10': 0.28557786... matbench_steels Fold scores fold mae rmse mape* max_error fold_0 100.9349 168.5655 0.0653 846.0036 fold_1 70.6172 96.7591 0.0512 382.8586 fold_2 80.5541 119.5353 0.0581 419.7060 fold_3 82.7723 130.3167 0.0616 653.9549 fold_4 82.6796 125.2501 0.0627 382.7175 Fold score stats metric mean max min std mae 83.5116 100.9349 70.6172 9.7961 rmse 128.0854 168.5655 96.7591 23.2641 mape* 0.0598 0.0653 0.0512 0.0049 max_error 537.0481 846.0036 382.7175 184.6522 Fold parameters fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.39234828151308304, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.41596749600021377, 'matbench_v0.1_modnet_v0.1.10': 0.1916842224867032, 'target'... fold_1 {'matbench_v0.1_CrabNet': 0.3889308341485257, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.2748090726773708, 'matbench_v0.1_modnet_v0.1.10': 0.3362600931741035, 'target': ... fold_2 {'matbench_v0.1_CrabNet': 0.026986533299291782, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.7075583926926007, 'matbench_v0.1_modnet_v0.1.10': 0.26545507400810747, 'target... fold_3 {'matbench_v0.1_CrabNet': 0.11192047688071322, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.22618555757912648, 'matbench_v0.1_modnet_v0.1.10': 0.6618939655401603, 'target'... fold_4 {'matbench_v0.1_CrabNet': 0.6529073222418661, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 8.321664685660583e-05, 'matbench_v0.1_modnet_v0.1.10': 0.34700946111127734, 'targe...","title":"matbench_v0.1: fictitious_model"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#matbench_v01-fictitious_model","text":"","title":"matbench_v0.1: fictitious_model"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#algorithm-description","text":"Fictitious model ensemble where (intentionally p-hacked) optimal weights were found based on ~130k SOBOL-sampled weight combinations of the various models for each of the task/fold combinations. Also referred to as a weighted ensemble average.","title":"Algorithm description:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#notes","text":"No notes. Raw data download and example notebook available on the matbench repo .","title":"Notes:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#references-in-bibtex-format","text":"('@article{Dunn2020,\\n' ' doi = {10.1038/s41524-020-00406-3},\\n' ' url = {https://doi.org/10.1038/s41524-020-00406-3},\\n' ' year = {2020},\\n' ' month = sep,\\n' ' publisher = {Springer Science and Business Media {LLC}},\\n' ' volume = {6},\\n' ' number = {1},\\n' ' author = {Alexander Dunn and Qi Wang and Alex Ganose and Daniel Dopp and ' 'Anubhav Jain},\\n' ' title = {Benchmarking materials property prediction methods: the Matbench ' 'test set and Automatminer reference algorithm},\\n' ' journal = {npj Computational Materials}\\n' '}')","title":"References (in bibtex format):"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#user-metadata","text":"{}","title":"User metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#metadata","text":"tasks recorded 13/13 complete? \u2713 composition complete? \u2713 structure complete? \u2713 regression complete? \u2713 classification complete? \u2713","title":"Metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#software-requirements","text":"{'python': ['scikit-learn==0.24.1', 'numpy==1.20.1', 'matbench==0.1.0']}","title":"Software Requirements"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#task-data","text":"","title":"Task data:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#matbench_dielectric","text":"","title":"matbench_dielectric"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-scores","text":"fold mae rmse mape* max_error fold_0 0.1736 0.6474 0.0587 14.0358 fold_1 0.2564 1.0394 0.0836 19.4483 fold_2 0.3997 2.9327 0.0801 59.0262 fold_3 0.2760 2.2449 0.0551 52.4163 fold_4 0.2931 1.5846 0.0887 28.0063","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-score-stats","text":"metric mean max min std mae 0.2798 0.3997 0.1736 0.0726 rmse 1.6898 2.9327 0.6474 0.8214 mape* 0.0733 0.0887 0.0551 0.0136 max_error 34.5866 59.0262 14.0358 17.9443","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-parameters","text":"fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.20575117871423193, 'matbench_v0.1_alignn': 0.36503077350276164, 'matbench_v0.1_automatminer_expressv2020': 0.03693776452273196, 'matbench_v0.1_modnet_v0.1.10': 0.3922802832... fold_1 {'matbench_v0.1_CrabNet': 0.15967213462626922, 'matbench_v0.1_alignn': 0.019957298927968287, 'matbench_v0.1_automatminer_expressv2020': 0.22950980783361521, 'matbench_v0.1_modnet_v0.1.10': 0.590860758... fold_2 {'matbench_v0.1_CrabNet': 0.26752136323219017, 'matbench_v0.1_alignn': 0.00022466871513461952, 'matbench_v0.1_automatminer_expressv2020': 0.32256934842943974, 'matbench_v0.1_modnet_v0.1.10': 0.4096846... fold_3 {'matbench_v0.1_CrabNet': 0.07708241986797697, 'matbench_v0.1_alignn': 0.22901497809874913, 'matbench_v0.1_automatminer_expressv2020': 0.08756035007079653, 'matbench_v0.1_modnet_v0.1.10': 0.6063422519... fold_4 {'matbench_v0.1_CrabNet': 0.40492091223579596, 'matbench_v0.1_alignn': 0.01928753160915624, 'matbench_v0.1_automatminer_expressv2020': 0.21645513538860056, 'matbench_v0.1_modnet_v0.1.10': 0.3593364207...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#matbench_expt_gap","text":"","title":"matbench_expt_gap"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-scores_1","text":"fold mae rmse mape* max_error fold_0 0.3135 0.6275 0.3085 5.1756 fold_1 0.3282 0.6640 0.2527 6.7164 fold_2 0.3390 0.7782 0.3133 8.8678 fold_3 0.3086 0.6647 0.2919 5.4599 fold_4 0.3199 0.6526 0.3583 5.6826","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-score-stats_1","text":"metric mean max min std mae 0.3218 0.3390 0.3086 0.0108 rmse 0.6774 0.7782 0.6275 0.0522 mape* 0.3049 0.3583 0.2527 0.0341 max_error 6.3805 8.8678 5.1756 1.3480","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-parameters_1","text":"fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.25328331229913154, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.19279284485612228, 'matbench_v0.1_modnet_v0.1.10': 0.5539238428447462, 'target'... fold_1 {'matbench_v0.1_CrabNet': 0.5107847473559838, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.1463424463386866, 'matbench_v0.1_modnet_v0.1.10': 0.3428728063053297, 'target': ... fold_2 {'matbench_v0.1_CrabNet': 0.6865386216198438, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.12547388312292732, 'matbench_v0.1_modnet_v0.1.10': 0.18798749525722885, 'target'... fold_3 {'matbench_v0.1_CrabNet': 0.4973765410211031, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.07069597545395823, 'matbench_v0.1_modnet_v0.1.10': 0.43192748352493865, 'target'... fold_4 {'matbench_v0.1_CrabNet': 0.24897538653903542, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.14684490360565391, 'matbench_v0.1_modnet_v0.1.10': 0.6041797098553107, 'target'...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#matbench_expt_is_metal","text":"","title":"matbench_expt_is_metal"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-scores_2","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.9218 0.9218 0.9205 0.9218 fold_1 0.9157 0.9156 0.9145 0.9156 fold_2 0.9207 0.9207 0.9193 0.9207 fold_3 0.9228 0.9228 0.9223 0.9228 fold_4 0.9238 0.9238 0.9235 0.9238","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-score-stats_2","text":"metric mean max min std accuracy 0.9210 0.9238 0.9157 0.0028 balanced_accuracy 0.9209 0.9238 0.9156 0.0028 f1 0.9200 0.9235 0.9145 0.0031 rocauc 0.9209 0.9238 0.9156 0.0028","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-parameters_2","text":"fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999994636097223, 'matbench_v0.1_modnet_v0.1.10': 5.363902777989782e-07, 'target': 0.0} fold_1 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999975241578845, 'matbench_v0.1_modnet_v0.1.10': 2.4758421155135045e-06, 'target': 0.0} fold_2 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999913986660386, 'matbench_v0.1_modnet_v0.1.10': 8.601333961434646e-06, 'target': 0.0} fold_3 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999938365545721, 'matbench_v0.1_modnet_v0.1.10': 6.16344542788228e-06, 'target': 0.0} fold_4 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999802552040563, 'matbench_v0.1_modnet_v0.1.10': 1.974479594366695e-05, 'target': 0.0}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#matbench_glass","text":"","title":"matbench_glass"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-scores_3","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.8283 0.8441 0.8697 0.8441 fold_1 0.8125 0.8383 0.8548 0.8383 fold_2 0.8574 0.8546 0.8956 0.8546 fold_3 0.9173 0.8742 0.9437 0.8742 fold_4 0.9375 0.8921 0.9579 0.8921","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-score-stats_3","text":"metric mean max min std accuracy 0.8706 0.9375 0.8125 0.0490 balanced_accuracy 0.8607 0.8921 0.8383 0.0199 f1 0.9043 0.9579 0.8548 0.0404 rocauc 0.8607 0.8921 0.8383 0.0199","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-parameters_3","text":"fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999959766363284, 'matbench_v0.1_modnet_v0.1.10': 4.023363671606479e-06, 'target': 0.0} fold_1 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999933773656292, 'matbench_v0.1_modnet_v0.1.10': 6.622634370803024e-06, 'target': 0.0} fold_2 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999955134857657, 'matbench_v0.1_modnet_v0.1.10': 4.486514234348761e-06, 'target': 0.0} fold_3 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999873715803423, 'matbench_v0.1_modnet_v0.1.10': 1.2628419657670107e-05, 'target': 0.0} fold_4 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.9999923643049671, 'matbench_v0.1_modnet_v0.1.10': 7.635695032953968e-06, 'target': 0.0}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#matbench_jdft2d","text":"","title":"matbench_jdft2d"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-scores_4","text":"fold mae rmse mape* max_error fold_0 25.6817 45.6908 20.8585 201.4303 fold_1 27.7426 63.3403 0.2270 367.3741 fold_2 50.8337 147.0971 0.5943 846.5039 fold_3 24.9264 50.6733 0.2358 302.0936 fold_4 37.7280 151.3688 0.4895 1533.5975","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-score-stats_4","text":"metric mean max min std mae 33.3825 50.8337 24.9264 9.8594 rmse 91.6341 151.3688 45.6908 47.3993 mape* 4.4810 20.8585 0.2270 8.1900 max_error 650.1999 1533.5975 201.4303 494.2650","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-parameters_4","text":"fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.15342414366645282, 'matbench_v0.1_alignn': 0.03181279451221846, 'matbench_v0.1_automatminer_expressv2020': 0.446224463518545, 'matbench_v0.1_modnet_v0.1.10': 0.368538598302... fold_1 {'matbench_v0.1_CrabNet': 0.01712548352219769, 'matbench_v0.1_alignn': 0.0008425195852314279, 'matbench_v0.1_automatminer_expressv2020': 0.028448329464033657, 'matbench_v0.1_modnet_v0.1.10': 0.9535836... fold_2 {'matbench_v0.1_CrabNet': 0.0005454254242677881, 'matbench_v0.1_alignn': 0.18050122343170372, 'matbench_v0.1_automatminer_expressv2020': 0.02001052727974478, 'matbench_v0.1_modnet_v0.1.10': 0.79894282... fold_3 {'matbench_v0.1_CrabNet': 0.17966803253404068, 'matbench_v0.1_alignn': 0.1828427612410428, 'matbench_v0.1_automatminer_expressv2020': 0.2321150227627915, 'matbench_v0.1_modnet_v0.1.10': 0.405374183462... fold_4 {'matbench_v0.1_CrabNet': 0.0004628740793511039, 'matbench_v0.1_alignn': 0.25457773015822455, 'matbench_v0.1_automatminer_expressv2020': 0.13933131094614346, 'matbench_v0.1_modnet_v0.1.10': 0.60562808...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#matbench_log_gvrh","text":"","title":"matbench_log_gvrh"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-scores_5","text":"fold mae rmse mape* max_error fold_0 0.0671 0.1030 0.0533 0.9089 fold_1 0.0685 0.1081 0.0551 1.1515 fold_2 0.0676 0.1055 0.0543 0.8141 fold_3 0.0676 0.1046 0.0528 0.8935 fold_4 0.0669 0.1052 0.0525 0.7486","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-score-stats_5","text":"metric mean max min std mae 0.0675 0.0685 0.0669 0.0006 rmse 0.1053 0.1081 0.1030 0.0017 mape* 0.0536 0.0551 0.0525 0.0010 max_error 0.9033 1.1515 0.7486 0.1368","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-parameters_5","text":"fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.0005469296890628186, 'matbench_v0.1_alignn': 0.5790363091121159, 'matbench_v0.1_automatminer_expressv2020': 0.016475670548405624, 'matbench_v0.1_modnet_v0.1.10': 0.40394109... fold_1 {'matbench_v0.1_CrabNet': 0.0008720753148568775, 'matbench_v0.1_alignn': 0.5398068953552143, 'matbench_v0.1_automatminer_expressv2020': 0.00293608420564664, 'matbench_v0.1_modnet_v0.1.10': 0.456384945... fold_2 {'matbench_v0.1_CrabNet': 0.00397918306233581, 'matbench_v0.1_alignn': 0.5905921519766839, 'matbench_v0.1_automatminer_expressv2020': 0.0013491061186301629, 'matbench_v0.1_modnet_v0.1.10': 0.404079558... fold_3 {'matbench_v0.1_CrabNet': 0.0036578251362713926, 'matbench_v0.1_alignn': 0.5745243458793965, 'matbench_v0.1_automatminer_expressv2020': 0.0007068555241777903, 'matbench_v0.1_modnet_v0.1.10': 0.4211109... fold_4 {'matbench_v0.1_CrabNet': 0.004858865465360498, 'matbench_v0.1_alignn': 0.510039374033992, 'matbench_v0.1_automatminer_expressv2020': 0.0015554463592616025, 'matbench_v0.1_modnet_v0.1.10': 0.483546314...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#matbench_log_kvrh","text":"","title":"matbench_log_kvrh"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-scores_6","text":"fold mae rmse mape* max_error fold_0 0.0505 0.0987 0.0335 1.5820 fold_1 0.0522 0.1061 0.0343 1.3186 fold_2 0.0477 0.0922 0.0318 1.1371 fold_3 0.0554 0.1091 0.0400 1.0836 fold_4 0.0514 0.1001 0.0345 1.3635","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-score-stats_6","text":"metric mean max min std mae 0.0514 0.0554 0.0477 0.0025 rmse 0.1012 0.1091 0.0922 0.0059 mape* 0.0348 0.0400 0.0318 0.0027 max_error 1.2970 1.5820 1.0836 0.1773","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-parameters_6","text":"fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.0021430310672584164, 'matbench_v0.1_alignn': 0.44012112653432134, 'matbench_v0.1_automatminer_expressv2020': 0.037587235031681716, 'matbench_v0.1_modnet_v0.1.10': 0.5201486... fold_1 {'matbench_v0.1_CrabNet': 0.011125181281904034, 'matbench_v0.1_alignn': 0.4454571849332936, 'matbench_v0.1_automatminer_expressv2020': 0.04526796006356993, 'matbench_v0.1_modnet_v0.1.10': 0.4981496737... fold_2 {'matbench_v0.1_CrabNet': 0.035214279444113504, 'matbench_v0.1_alignn': 0.4542466438434961, 'matbench_v0.1_automatminer_expressv2020': 0.002799658586453697, 'matbench_v0.1_modnet_v0.1.10': 0.507739418... fold_3 {'matbench_v0.1_CrabNet': 0.001317193865459212, 'matbench_v0.1_alignn': 0.3612906935102406, 'matbench_v0.1_automatminer_expressv2020': 0.12549697847265245, 'matbench_v0.1_modnet_v0.1.10': 0.5118951341... fold_4 {'matbench_v0.1_CrabNet': 0.0013550637888710693, 'matbench_v0.1_alignn': 0.3972992478093793, 'matbench_v0.1_automatminer_expressv2020': 0.1239877770792496, 'matbench_v0.1_modnet_v0.1.10': 0.4773579113...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#matbench_mp_e_form","text":"","title":"matbench_mp_e_form"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-scores_7","text":"fold mae rmse mape* max_error fold_0 0.0217 0.0631 0.1491 3.5983 fold_1 0.0220 0.0533 0.1330 2.8653 fold_2 0.0210 0.0496 0.1395 2.0082 fold_3 0.0219 0.0543 0.1831 1.6367 fold_4 0.0211 0.0500 0.2482 1.3868","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-score-stats_7","text":"metric mean max min std mae 0.0215 0.0220 0.0210 0.0004 rmse 0.0541 0.0631 0.0496 0.0049 mape* 0.1706 0.2482 0.1330 0.0425 max_error 2.2991 3.5983 1.3868 0.8203","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-parameters_7","text":"fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.008499291160418577, 'matbench_v0.1_alignn': 0.9032029811240486, 'matbench_v0.1_automatminer_expressv2020': 0.007458152631024239, 'matbench_v0.1_modnet_v0.1.10': 0.080839575... fold_1 {'matbench_v0.1_CrabNet': 0.004977249815968251, 'matbench_v0.1_alignn': 0.9004650742942951, 'matbench_v0.1_automatminer_expressv2020': 0.002291709303718508, 'matbench_v0.1_modnet_v0.1.10': 0.092265966... fold_2 {'matbench_v0.1_CrabNet': 0.017394782923472796, 'matbench_v0.1_alignn': 0.9120658902185469, 'matbench_v0.1_automatminer_expressv2020': 0.0015317067352738955, 'matbench_v0.1_modnet_v0.1.10': 0.06900762... fold_3 {'matbench_v0.1_CrabNet': 0.008004151413010018, 'matbench_v0.1_alignn': 0.9272266729313643, 'matbench_v0.1_automatminer_expressv2020': 0.006611717219836468, 'matbench_v0.1_modnet_v0.1.10': 0.058157458... fold_4 {'matbench_v0.1_CrabNet': 0.006685517382035754, 'matbench_v0.1_alignn': 0.9329780645736975, 'matbench_v0.1_automatminer_expressv2020': 0.01291754489989797, 'matbench_v0.1_modnet_v0.1.10': 0.0474188731...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#matbench_mp_gap","text":"","title":"matbench_mp_gap"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-scores_8","text":"fold mae rmse mape* max_error fold_0 0.1770 0.4025 2.4955 6.1745 fold_1 0.1774 0.4018 2.5452 6.3716 fold_2 0.1802 0.4059 4.1836 5.5312 fold_3 0.1756 0.4091 5.2175 6.0348 fold_4 0.1803 0.4188 4.8511 5.2131","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-score-stats_8","text":"metric mean max min std mae 0.1781 0.1803 0.1756 0.0018 rmse 0.4076 0.4188 0.4018 0.0062 mape* 3.8586 5.2175 2.4955 1.1420 max_error 5.8650 6.3716 5.2131 0.4284","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-parameters_8","text":"fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.03134890898326961, 'matbench_v0.1_alignn': 0.7111209827754947, 'matbench_v0.1_automatminer_expressv2020': 0.005786178299136875, 'matbench_v0.1_modnet_v0.1.10': 0.2517439299... fold_1 {'matbench_v0.1_CrabNet': 0.0407591733831172, 'matbench_v0.1_alignn': 0.6975505799075838, 'matbench_v0.1_automatminer_expressv2020': 0.003201954717881077, 'matbench_v0.1_modnet_v0.1.10': 0.25848829199... fold_2 {'matbench_v0.1_CrabNet': 0.036284616969791905, 'matbench_v0.1_alignn': 0.6942018172396998, 'matbench_v0.1_automatminer_expressv2020': 0.00041732299008234003, 'matbench_v0.1_modnet_v0.1.10': 0.2690962... fold_3 {'matbench_v0.1_CrabNet': 0.03522104460336443, 'matbench_v0.1_alignn': 0.8009824659470643, 'matbench_v0.1_automatminer_expressv2020': 0.002089889799254658, 'matbench_v0.1_modnet_v0.1.10': 0.1617065996... fold_4 {'matbench_v0.1_CrabNet': 0.04105720156963323, 'matbench_v0.1_alignn': 0.7549700155809074, 'matbench_v0.1_automatminer_expressv2020': 0.0048447390836097535, 'matbench_v0.1_modnet_v0.1.10': 0.199128043...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#matbench_mp_is_metal","text":"","title":"matbench_mp_is_metal"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-scores_9","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.9173 0.9156 0.9047 0.9156 fold_1 0.9135 0.9117 0.9003 0.9117 fold_2 0.9146 0.9125 0.9013 0.9125 fold_3 0.9146 0.9108 0.8998 0.9108 fold_4 0.9147 0.9123 0.9012 0.9123","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-score-stats_9","text":"metric mean max min std accuracy 0.9149 0.9173 0.9135 0.0013 balanced_accuracy 0.9126 0.9156 0.9108 0.0016 f1 0.9015 0.9047 0.8998 0.0017 rocauc 0.9126 0.9156 0.9108 0.0016","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-parameters_9","text":"fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.9953164044264534, 'matbench_v0.1_automatminer_expressv2020': 0.003963882720028548, 'matbench_v0.1_modnet_v0.1.10': 0.0007197128535180265, 'targ... fold_1 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.93404698679432, 'matbench_v0.1_automatminer_expressv2020': 0.06591254576545874, 'matbench_v0.1_modnet_v0.1.10': 4.0467440221240175e-05, 'target... fold_2 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.935298137056003, 'matbench_v0.1_automatminer_expressv2020': 0.06459068443951696, 'matbench_v0.1_modnet_v0.1.10': 0.00011117850448007413, 'targe... fold_3 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 3.765834317927802e-05, 'matbench_v0.1_automatminer_expressv2020': 0.9998516565467143, 'matbench_v0.1_modnet_v0.1.10': 0.00011068511010648864, 'ta... fold_4 {'matbench_v0.1_CrabNet': 0.0, 'matbench_v0.1_alignn': 0.9606082727476839, 'matbench_v0.1_automatminer_expressv2020': 0.03919544417457493, 'matbench_v0.1_modnet_v0.1.10': 0.00019628307774114157, 'targ...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#matbench_perovskites","text":"","title":"matbench_perovskites"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-scores_10","text":"fold mae rmse mape* max_error fold_0 0.0300 0.0583 0.0307 0.8302 fold_1 0.0332 0.0644 0.0338 0.9146 fold_2 0.0295 0.0519 0.0296 0.8127 fold_3 0.0302 0.0545 0.0298 0.7940 fold_4 0.0305 0.0569 0.0283 0.8359","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-score-stats_10","text":"metric mean max min std mae 0.0307 0.0332 0.0295 0.0013 rmse 0.0572 0.0644 0.0519 0.0042 mape* 0.0305 0.0338 0.0283 0.0018 max_error 0.8375 0.9146 0.7940 0.0412","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-parameters_10","text":"fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.009547584180147844, 'matbench_v0.1_alignn': 0.9848694217426553, 'matbench_v0.1_automatminer_expressv2020': 0.0016440523111006303, 'matbench_v0.1_modnet_v0.1.10': 0.00393894... fold_1 {'matbench_v0.1_CrabNet': 0.01535995631615898, 'matbench_v0.1_alignn': 0.9490765455180638, 'matbench_v0.1_automatminer_expressv2020': 0.02139321931280439, 'matbench_v0.1_modnet_v0.1.10': 0.01417027885... fold_2 {'matbench_v0.1_CrabNet': 0.004227415890322362, 'matbench_v0.1_alignn': 0.9472138966649386, 'matbench_v0.1_automatminer_expressv2020': 0.027487021706203058, 'matbench_v0.1_modnet_v0.1.10': 0.021071665... fold_3 {'matbench_v0.1_CrabNet': 0.0011844250315664847, 'matbench_v0.1_alignn': 0.9684742126005914, 'matbench_v0.1_automatminer_expressv2020': 0.029884743936693152, 'matbench_v0.1_modnet_v0.1.10': 0.00045661... fold_4 {'matbench_v0.1_CrabNet': 0.005623378454120186, 'matbench_v0.1_alignn': 0.9059313272448253, 'matbench_v0.1_automatminer_expressv2020': 0.011448435884939327, 'matbench_v0.1_modnet_v0.1.10': 0.076996858...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#matbench_phonons","text":"","title":"matbench_phonons"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-scores_11","text":"fold mae rmse mape* max_error fold_0 31.6557 65.3053 0.0564 629.1442 fold_1 26.9097 45.6052 0.0542 353.3826 fold_2 28.6332 51.7256 0.0536 294.6454 fold_3 28.1886 61.4775 0.0549 513.0511 fold_4 23.9047 44.1742 0.0447 449.6660","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-score-stats_11","text":"metric mean max min std mae 27.8584 31.6557 23.9047 2.5164 rmse 53.6576 65.3053 44.1742 8.4300 mape* 0.0527 0.0564 0.0447 0.0041 max_error 447.9778 629.1442 294.6454 117.9133","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-parameters_11","text":"fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.0018203153633914477, 'matbench_v0.1_alignn': 0.6486914675578309, 'matbench_v0.1_automatminer_expressv2020': 0.08593035173170938, 'matbench_v0.1_modnet_v0.1.10': 0.263557865... fold_1 {'matbench_v0.1_CrabNet': 5.294311262819687e-05, 'matbench_v0.1_alignn': 0.7044278252813406, 'matbench_v0.1_automatminer_expressv2020': 0.11910972769429452, 'matbench_v0.1_modnet_v0.1.10': 0.176409503... fold_2 {'matbench_v0.1_CrabNet': 0.003740616371621964, 'matbench_v0.1_alignn': 0.7214054796467301, 'matbench_v0.1_automatminer_expressv2020': 0.008481957920670979, 'matbench_v0.1_modnet_v0.1.10': 0.266371946... fold_3 {'matbench_v0.1_CrabNet': 0.08617214017244104, 'matbench_v0.1_alignn': 0.7045203684732354, 'matbench_v0.1_automatminer_expressv2020': 0.0006473946247970298, 'matbench_v0.1_modnet_v0.1.10': 0.208660096... fold_4 {'matbench_v0.1_CrabNet': 0.0061383217259627495, 'matbench_v0.1_alignn': 0.6551412667140507, 'matbench_v0.1_automatminer_expressv2020': 0.053142546033970374, 'matbench_v0.1_modnet_v0.1.10': 0.28557786...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#matbench_steels","text":"","title":"matbench_steels"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-scores_12","text":"fold mae rmse mape* max_error fold_0 100.9349 168.5655 0.0653 846.0036 fold_1 70.6172 96.7591 0.0512 382.8586 fold_2 80.5541 119.5353 0.0581 419.7060 fold_3 82.7723 130.3167 0.0616 653.9549 fold_4 82.6796 125.2501 0.0627 382.7175","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-score-stats_12","text":"metric mean max min std mae 83.5116 100.9349 70.6172 9.7961 rmse 128.0854 168.5655 96.7591 23.2641 mape* 0.0598 0.0653 0.0512 0.0049 max_error 537.0481 846.0036 382.7175 184.6522","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_fictitious_model_wise/#fold-parameters_12","text":"fold params dict fold_0 {'matbench_v0.1_CrabNet': 0.39234828151308304, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.41596749600021377, 'matbench_v0.1_modnet_v0.1.10': 0.1916842224867032, 'target'... fold_1 {'matbench_v0.1_CrabNet': 0.3889308341485257, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.2748090726773708, 'matbench_v0.1_modnet_v0.1.10': 0.3362600931741035, 'target': ... fold_2 {'matbench_v0.1_CrabNet': 0.026986533299291782, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.7075583926926007, 'matbench_v0.1_modnet_v0.1.10': 0.26545507400810747, 'target... fold_3 {'matbench_v0.1_CrabNet': 0.11192047688071322, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 0.22618555757912648, 'matbench_v0.1_modnet_v0.1.10': 0.6618939655401603, 'target'... fold_4 {'matbench_v0.1_CrabNet': 0.6529073222418661, 'matbench_v0.1_alignn': 0.0, 'matbench_v0.1_automatminer_expressv2020': 8.321664685660583e-05, 'matbench_v0.1_modnet_v0.1.10': 0.34700946111127734, 'targe...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/","text":"matbench_v0.1: MODNet (v0.1.10) Algorithm description: MODNet, the Materials Optimal Descriptor Network (v0.1.10). A feed-forward neural network, using all compatible matminer features and a relevance-redundancy based feature selection algorithm. Hyperparameter optimisation is performed with a nested grid search for the 9 smaller tasks, and with a genetic algorithm for the 4 larger tasks ( matbench_perovskites , matbench_mp_gap , matbench_mp_is_metal , matbench_mp_eform . Benchmark results were loaded from https://github.com/ml-evs/modnet-matbench/releases/tag/v0.3.0, archived at 10.5281/zenodo.5562338 . Notes: None Raw data download and example notebook available on the matbench repo . References (in bibtex format): ('@article{De_Breuck_2021, doi = {10.1088/1361-648x/ac1280}, url = ' '{https://doi.org/10.1088/1361-648x/ac1280}, year = 2021, month = {jul}, ' 'publisher = {{IOP} Publishing}, volume = {33}, number = {40}, pages = ' '{404002}, author = {Pierre-Paul De Breuck and Matthew L Evans and Gian-Marco ' 'Rignanese}, title = {Robust model benchmarking and bias-imbalance in ' 'data-driven materials science: a case study on {MODNet}}, journal = {Journal ' 'of Physics: Condensed Matter}, abstract = {As the number of novel ' 'data-driven approaches to material science continues to grow, it is crucial ' 'to perform consistent quality, reliability and applicability assessments of ' 'model performance. In this paper, we benchmark the Materials Optimal ' 'Descriptor Network (MODNet) method and architecture against the recently ' 'released MatBench v0.1, a curated test suite of materials datasets. MODNet ' 'is shown to outperform current leaders on 6 of the 13 tasks, while closely ' 'matching the current leaders on a further 2 tasks; MODNet performs ' 'particularly well when the number of samples is below 10\\xa0000. Attention ' 'is paid to two topics of concern when benchmarking models. First, we ' 'encourage the reporting of a more diverse set of metrics as it leads to a ' 'more comprehensive and holistic comparison of model performance. Second, an ' 'equally important task is the uncertainty assessment of a model towards a ' 'target domain. Significant variations in validation errors can be observed, ' 'depending on the imbalance and bias in the training set (i.e., similarity ' 'between training and application space). By using an ensemble MODNet model, ' 'confidence intervals can be built and the uncertainty on individual ' 'predictions can be quantified. Imbalance and bias issues are often ' 'overlooked, and yet are important for successful real-world applications of ' 'machine learning in materials science and condensed matter.}}, ' '@article{DeBreuck2021, doi = {10.1038/s41524-021-00552-2}, url = ' '{https://doi.org/10.1038/s41524-021-00552-2}, year = {2021}, month = jun, ' 'publisher = {Springer Science and Business Media {LLC}}, volume = {7}, ' 'number = {1}, author = {Pierre-Paul De Breuck and Geoffroy Hautier and ' 'Gian-Marco Rignanese}, title = {Materials property prediction for limited ' 'datasets enabled by feature selection and joint learning with {MODNet}}, ' 'journal = {npj Computational Materials}}') User metadata: {} Metadata: tasks recorded 13/13 complete? \u2713 composition complete? \u2713 structure complete? \u2713 regression complete? \u2713 classification complete? \u2713 Software Requirements {'python': ['modnet==0.1.10', 'matbench==0.2.0']} Task data: matbench_dielectric Fold scores fold mae rmse mape* max_error fold_0 0.1939 0.7043 0.0657 13.9549 fold_1 0.2669 1.0559 0.0897 19.4132 fold_2 0.4138 2.9360 0.0873 58.9519 fold_3 0.2880 2.2447 0.0593 52.4648 fold_4 0.3223 1.6518 0.1040 28.0662 Fold score stats metric mean max min std mae 0.2970 0.4138 0.1939 0.0720 rmse 1.7185 2.9360 0.7043 0.8039 mape* 0.0812 0.1040 0.0593 0.0164 max_error 34.5702 58.9519 13.9549 17.9539 Fold parameters fold params dict fold_0 {'std': [[0.06733036786317825], [0.0744570940732956], [0.4551847279071808], [0.6979547739028931], [0.05083160847425461], [0.09550821781158447], [0.19389964640140533], [0.45796462893486023], [0.0457437... fold_1 {'std': [[0.072625070810318], [0.20568081736564636], [0.06122368574142456], [0.1985706239938736], [0.13508345186710358], [0.5729472637176514], [0.18508531153202057], [0.36901697516441345], [0.15902499... fold_2 {'std': [[0.07455231994390488], [0.40818431973457336], [0.17178542912006378], [0.1756463497877121], [0.05205323547124863], [0.18280482292175293], [0.029979035258293152], [0.13881000876426697], [0.1849... fold_3 {'std': [[0.07038000971078873], [0.052069056779146194], [0.11985469609498978], [0.2917916774749756], [0.07153098285198212], [0.16248461604118347], [0.03178466856479645], [0.06994788348674774], [0.0775... fold_4 {'std': [[0.062172286212444305], [0.1020524799823761], [0.046170447021722794], [0.2582769989967346], [0.5205304622650146], [0.09285475313663483], [0.04396134242415428], [0.1190856322646141], [0.159720... matbench_expt_gap Fold scores fold mae rmse mape* max_error fold_0 0.3272 0.7062 0.3510 6.3096 fold_1 0.3594 0.7340 0.3187 6.3544 fold_2 0.3845 0.8563 0.3841 9.8567 fold_3 0.3259 0.6888 0.3231 5.1081 fold_4 0.3382 0.7334 0.4075 6.5141 Fold score stats metric mean max min std mae 0.3470 0.3845 0.3259 0.0222 rmse 0.7437 0.8563 0.6888 0.0588 mape* 0.3569 0.4075 0.3187 0.0345 max_error 6.8286 9.8567 5.1081 1.5952 Fold parameters fold params dict fold_0 {'std': [[0.3934377431869507], [0.6812934875488281], [0.46057939529418945], [0.21048687398433685], [0.39122024178504944], [2.184469699859619], [1.0323148965835571], [1.1202787160873413], [0.8973723649... fold_1 {'std': [[0.42153677344322205], [0.10141757875680923], [0.2717689573764801], [0.0055972738191485405], [0.12942852079868317], [0.19773989915847778], [0.1537284404039383], [0.152150496840477], [0.127241... fold_2 {'std': [[0.5310537219047546], [0.09472547471523285], [0.6016039848327637], [0.7606176137924194], [0.2108703851699829], [0.3892253637313843], [0.8048807382583618], [0.058867525309324265], [0.236589178... fold_3 {'std': [[0.4957612454891205], [0.24328213930130005], [1.1489912271499634], [0.4026401937007904], [0.38004472851753235], [0.235760897397995], [0.2802310287952423], [0.23525512218475342], [0.9116547703... fold_4 {'std': [[0.17024394869804382], [1.0889294147491455], [0.0037015366833657026], [0.45974406599998474], [0.7000935673713684], [1.2791191339492798], [1.037060260772705], [1.1216192245483398], [1.26752507... matbench_expt_is_metal Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.9269 0.9269 0.9255 0.9269 fold_1 0.9136 0.9136 0.9121 0.9136 fold_2 0.9177 0.9177 0.9173 0.9177 fold_3 0.9177 0.9177 0.9169 0.9177 fold_4 0.9045 0.9045 0.9049 0.9045 Fold score stats metric mean max min std accuracy 0.9161 0.9269 0.9045 0.0073 balanced_accuracy 0.9161 0.9269 0.9045 0.0072 f1 0.9153 0.9255 0.9049 0.0068 rocauc 0.9161 0.9269 0.9045 0.0072 Fold parameters fold params dict fold_0 {'std': [[0.17845642566680908, 0.1784563958644867], [0.38880154490470886, 0.38880154490470886], [0.11527526378631592, 0.11527524888515472], [0.29507318139076233, 0.29507318139076233], [0.4294841885566... fold_1 {'std': [[0.250985711812973, 0.250985711812973], [0.15300564467906952, 0.15300562977790833], [0.11072004586458206, 0.11072004586458206], [0.07669822871685028, 0.07669822126626968], [0.1074658855795860... fold_2 {'std': [[0.16936197876930237, 0.16936197876930237], [0.28381362557411194, 0.2838136553764343], [0.3199393153190613, 0.3199393153190613], [0.14618311822414398, 0.1461830586194992], [0.1249608173966407... fold_3 {'std': [[0.13702887296676636, 0.13702887296676636], [0.1351342797279358, 0.1351342648267746], [0.40080583095550537, 0.40080583095550537], [0.1579451709985733, 0.1579451709985733], [0.1655253022909164... fold_4 {'std': [[0.28618890047073364, 0.28618890047073364], [0.27044594287872314, 0.27044594287872314], [0.29565274715423584, 0.29565274715423584], [0.29736053943634033, 0.29736053943634033], [0.237021714448... matbench_glass Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.8759 0.8262 0.9153 0.8262 fold_1 0.8539 0.7783 0.9030 0.7783 fold_2 0.8565 0.8063 0.9016 0.8063 fold_3 0.8856 0.8402 0.9217 0.8402 fold_4 0.8662 0.8023 0.9102 0.8023 Fold score stats metric mean max min std accuracy 0.8676 0.8856 0.8539 0.0119 balanced_accuracy 0.8107 0.8402 0.7783 0.0212 f1 0.9104 0.9217 0.9016 0.0075 rocauc 0.8107 0.8402 0.7783 0.0212 Fold parameters fold params dict fold_0 {'std': [[0.1912706196308136, 0.1912706196308136], [0.3220626413822174, 0.3220626413822174], [0.38618433475494385, 0.38618433475494385], [0.3689897954463959, 0.36898982524871826], [0.296833336353302, ... fold_1 {'std': [[0.3554098606109619, 0.3554098606109619], [0.19303181767463684, 0.19303181767463684], [0.2967971861362457, 0.2967972159385681], [0.3302050232887268, 0.3302050232887268], [0.2770985960960388, ... fold_2 {'std': [[0.3961077332496643, 0.39610767364501953], [0.19110238552093506, 0.19110238552093506], [0.22688446938991547, 0.22688449919223785], [0.34065985679626465, 0.3406599164009094], [0.25127366185188... fold_3 {'std': [[0.18866945803165436, 0.18866944313049316], [0.17435620725154877, 0.17435620725154877], [0.18012933433055878, 0.1801293045282364], [0.11812251806259155, 0.11812251806259155], [0.1196716576814... fold_4 {'std': [[0.16088004410266876, 0.16088005900382996], [0.2377340942621231, 0.2377340942621231], [0.3177623748779297, 0.3177623748779297], [0.3061956465244293, 0.3061956465244293], [0.2921837270259857, ... matbench_jdft2d Fold scores fold mae rmse mape* max_error fold_0 27.5769 49.7512 21.3632 243.2504 fold_1 27.9722 63.3103 0.2282 364.1909 fold_2 51.3402 142.7963 0.6111 845.7528 fold_3 26.9141 52.8447 0.2724 311.7558 fold_4 38.8806 152.4413 0.4853 1534.9797 Fold score stats metric mean max min std mae 34.5368 51.3402 26.9141 9.4959 rmse 92.2288 152.4413 49.7512 45.5508 mape* 4.5920 21.3632 0.2282 8.3868 max_error 659.9859 1534.9797 243.2504 486.3231 Fold parameters fold params dict fold_0 {'std': [[9.497343063354492], [15.862295150756836], [74.97210693359375], [25.96040916442871], [47.26897048950195], [14.80854606628418], [22.77548599243164], [10.362432479858398], [8.255328178405762], ... fold_1 {'std': [[5.382687568664551], [25.67997932434082], [16.605792999267578], [7.763948917388916], [10.631340026855469], [36.21831512451172], [12.867671012878418], [3.2303359508514404], [38.958377838134766... fold_2 {'std': [[71.73993682861328], [18.688243865966797], [7.084332466125488], [16.097488403320312], [83.72747802734375], [12.528894424438477], [16.004690170288086], [14.574416160583496], [7.346397399902344... fold_3 {'std': [[2.2033019065856934], [17.148666381835938], [6.929365634918213], [3.3733177185058594], [19.175621032714844], [9.659783363342285], [2.456592321395874], [13.089242935180664], [44.94028091430664... fold_4 {'std': [[24.92951202392578], [17.333660125732422], [10.269680976867676], [4.752265453338623], [3.5876128673553467], [4.854499340057373], [12.900960922241211], [6.644251823425293], [9.120869636535645]... matbench_log_gvrh Fold scores fold mae rmse mape* max_error fold_0 0.0731 0.1089 0.0576 0.9014 fold_1 0.0738 0.1111 0.0579 1.1745 fold_2 0.0731 0.1101 0.0587 0.9076 fold_3 0.0738 0.1115 0.0567 0.9225 fold_4 0.0718 0.1101 0.0560 0.8007 Fold score stats metric mean max min std mae 0.0731 0.0738 0.0718 0.0007 rmse 0.1103 0.1115 0.1089 0.0009 mape* 0.0574 0.0587 0.0560 0.0009 max_error 0.9413 1.1745 0.8007 0.1243 Fold parameters fold params dict fold_0 {'std': [0.06334669888019562, 0.04876908287405968, 0.0713210254907608, 0.06149518862366676, 0.05233978480100632, 0.053833525627851486, 0.045166339725255966, 0.09107258170843124, 0.08312246203422546, 0... fold_1 {'std': [0.04058562591671944, 0.09664303809404373, 0.06196340546011925, 0.07074710726737976, 0.05361659824848175, 0.05300111323595047, 0.04533914476633072, 0.060226064175367355, 0.15155699849128723, 0... fold_2 {'std': [0.04066888242959976, 0.05564378947019577, 0.05513373762369156, 0.03629153221845627, 0.08530019223690033, 0.0363982692360878, 0.07014258950948715, 0.07834821194410324, 0.056601572781801224, 0.... fold_3 {'std': [0.06376504898071289, 0.2838374972343445, 0.025865282863378525, 0.04888685792684555, 0.18576562404632568, 0.045733798295259476, 0.047175027430057526, 0.04196206107735634, 0.06469003856182098, ... fold_4 {'std': [0.06425822526216507, 0.07589271664619446, 0.04857879504561424, 0.07567392289638519, 0.07976284623146057, 0.05443073436617851, 0.0474713109433651, 0.08143744617700577, 0.10169852524995804, 0.0... matbench_log_kvrh Fold scores fold mae rmse mape* max_error fold_0 0.0536 0.1013 0.0356 1.5366 fold_1 0.0559 0.1079 0.0366 1.2998 fold_2 0.0510 0.0949 0.0340 1.1808 fold_3 0.0585 0.1126 0.0418 1.1355 fold_4 0.0549 0.1046 0.0370 1.3202 Fold score stats metric mean max min std mae 0.0548 0.0585 0.0510 0.0025 rmse 0.1043 0.1126 0.0949 0.0060 mape* 0.0370 0.0418 0.0340 0.0026 max_error 1.2946 1.5366 1.1355 0.1397 Fold parameters fold params dict fold_0 {'std': [0.03473027050495148, 0.05344022810459137, 0.04392522946000099, 0.09693300724029541, 0.0621185339987278, 0.0515923835337162, 0.034392938017845154, 0.0368841215968132, 0.09843463450670242, 0.03... fold_1 {'std': [0.037284620106220245, 0.0660589188337326, 0.05483892932534218, 0.05504067987203598, 0.045397065579891205, 0.053156472742557526, 0.04068203642964363, 0.04492218419909477, 0.1503378003835678, 0... fold_2 {'std': [0.04053986072540283, 0.039209164679050446, 0.04213540256023407, 0.036292385309934616, 0.06385202705860138, 0.032488591969013214, 0.0784469023346901, 0.0694998949766159, 0.050309233367443085, ... fold_3 {'std': [0.04948587343096733, 0.11705353856086731, 0.025648461654782295, 0.03585298731923103, 0.11334579437971115, 0.03046250157058239, 0.040365662425756454, 0.03331249952316284, 0.038164108991622925,... fold_4 {'std': [0.04571979492902756, 0.0366676039993763, 0.036114685237407684, 0.06556463986635208, 0.07480020076036453, 0.03638936206698418, 0.05547630041837692, 0.10959770530462265, 0.16662324965000153, 0.... matbench_mp_e_form Fold scores fold mae rmse mape* max_error fold_0 0.0402 0.0817 0.3786 4.0438 fold_1 0.0497 0.1018 0.3121 4.8803 fold_2 0.0475 0.0905 0.2562 1.6230 fold_3 0.0464 0.0889 0.3515 1.5189 fold_4 0.0400 0.0812 0.2882 3.3787 Fold score stats metric mean max min std mae 0.0448 0.0497 0.0400 0.0039 rmse 0.0888 0.1018 0.0812 0.0075 mape* 0.3173 0.3786 0.2562 0.0436 max_error 3.0889 4.8803 1.5189 1.3281 Fold parameters fold params dict fold_0 {'std': [[0.0636366754770279], [0.02924380451440811], [0.11916627734899521], [0.11677506566047668], [0.1760452687740326], [0.1811746507883072], [0.08742818236351013], [0.1322329342365265], [0.14015400... fold_1 {'std': [[0.10467445850372314], [0.2568114399909973], [0.08591523766517639], [0.11847899109125137], [1.0217572450637817], [0.2770746350288391], [0.20971281826496124], [0.19037210941314697], [0.0730839... fold_2 {'std': [[0.07034026086330414], [0.07515157759189606], [0.1308293640613556], [0.23308764398097992], [0.2118426114320755], [0.1338074803352356], [0.17896589636802673], [0.09289371967315674], [0.0988285... fold_3 {'std': [[0.17922396957874298], [0.21060164272785187], [0.04639369249343872], [0.0925942063331604], [0.06210273131728172], [0.28422462940216064], [0.2840571105480194], [0.2760363817214966], [0.1231188... fold_4 {'std': [[0.1517249494791031], [0.13391436636447906], [0.40770843625068665], [0.13683228194713593], [0.124815434217453], [0.042988162487745285], [0.13916344940662384], [0.06709353625774384], [0.053676... matbench_mp_gap Fold scores fold mae rmse mape* max_error fold_0 0.2147 0.4441 2.8966 5.0558 fold_1 0.2161 0.4484 2.6899 6.2874 fold_2 0.2165 0.4433 4.1912 7.5685 fold_3 0.2309 0.4705 4.6749 6.9325 fold_4 0.2211 0.4564 4.9590 4.9406 Fold score stats metric mean max min std mae 0.2199 0.2309 0.2147 0.0059 rmse 0.4525 0.4705 0.4433 0.0101 mape* 3.8823 4.9590 2.6899 0.9248 max_error 6.1570 7.5685 4.9406 1.0299 Fold parameters fold params dict fold_0 {'std': [[0.2779600918292999], [0.1588134467601776], [0.0013879217440262437], [0.0013879217440262437], [0.0013879217440262437], [0.15315547585487366], [0.4301016926765442], [0.19215451180934906], [0.0... fold_1 {'std': [[0.0023871688172221184], [0.0023871688172221184], [0.0023871688172221184], [0.0023871688172221184], [0.0023871688172221184], [0.0023871688172221184], [0.35058045387268066], [0.439932823181152... fold_2 {'std': [[0.17874807119369507], [0.0015997957671061158], [0.0015997957671061158], [0.35170578956604004], [0.0015997957671061158], [0.0015997957671061158], [0.0015997957671061158], [0.00156632636208087... fold_3 {'std': [[0.004137647803872824], [0.004137647803872824], [0.004137647803872824], [0.004137647803872824], [0.004137647803872824], [0.004137647803872824], [0.004137647803872824], [0.004137647803872824],... fold_4 {'std': [[0.4364481568336487], [0.0038157568778842688], [0.003807253669947386], [0.003807792905718088], [0.003815052565187216], [0.0038899907376617193], [0.0038147747982293367], [0.19303250312805176],... matbench_mp_is_metal Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.8515 0.8415 0.8175 0.8415 fold_1 0.8824 0.8709 0.8526 0.8709 fold_2 0.5650 0.5000 0.0000 0.5000 fold_3 0.8575 0.8447 0.8200 0.8447 fold_4 0.8588 0.8453 0.8203 0.8453 Fold score stats metric mean max min std accuracy 0.8031 0.8824 0.5650 0.1195 balanced_accuracy 0.7805 0.8709 0.5000 0.1406 f1 0.6621 0.8526 0.0000 0.3313 rocauc 0.7805 0.8709 0.5000 0.1406 Fold parameters fold params dict fold_0 {'std': [[0.0416233129799366, 0.041623327881097794], [0.06803157180547714, 0.06803156435489655], [0.008189204148948193, 0.008189203217625618], [0.0037693644408136606, 0.003769365604966879], [0.0107376... fold_1 {'std': [[0.2837996482849121, 0.2837996482849121], [0.28376519680023193, 0.28376519680023193], [0.2669283449649811, 0.2669283449649811], [0.2666773200035095, 0.2666773200035095], [0.3405170142650604, ... fold_2 {'std': [[0.1777520626783371, 0.1777520775794983], [0.1777520626783371, 0.1777520775794983], [0.1777520626783371, 0.1777520775794983], [0.1777520626783371, 0.1777520775794983], [0.1777520626783371, 0.... fold_3 {'std': [[0.22140955924987793, 0.22140958905220032], [0.2370903342962265, 0.2370903342962265], [0.2370903342962265, 0.2370903342962265], [0.2370903342962265, 0.2370903342962265], [0.2370903342962265, ... fold_4 {'std': [[0.09707242995500565, 0.09707243740558624], [0.259949266910553, 0.2599492371082306], [0.09707242995500565, 0.09707243740558624], [0.23583407700061798, 0.2358340471982956], [0.2358340770006179... matbench_perovskites Fold scores fold mae rmse mape* max_error fold_0 0.0932 0.1304 0.0970 0.8705 fold_1 0.0939 0.1283 0.1058 1.0063 fold_2 0.0861 0.1216 0.0939 0.9432 fold_3 0.0892 0.1274 0.0995 0.8501 fold_4 0.0914 0.1310 0.0894 1.1780 Fold score stats metric mean max min std mae 0.0908 0.0939 0.0861 0.0028 rmse 0.1277 0.1310 0.1216 0.0033 mape* 0.0971 0.1058 0.0894 0.0055 max_error 0.9696 1.1780 0.8501 0.1180 Fold parameters fold params dict fold_0 {'std': [[0.10714199393987656], [0.0770525336265564], [0.07103670388460159], [0.047520048916339874], [0.09854762256145477], [0.054435212165117264], [0.06670265644788742], [0.12760771811008453], [0.110... fold_1 {'std': [[0.06410787999629974], [0.09309504926204681], [0.07608579844236374], [0.08194778114557266], [0.11951383203268051], [0.07481898367404938], [0.04808051139116287], [0.08761747926473618], [0.0603... fold_2 {'std': [[0.10084810853004456], [0.10042519122362137], [0.10863561928272247], [0.11654899269342422], [0.08363119512796402], [0.11726558208465576], [0.12616018950939178], [0.104669950902462], [0.083491... fold_3 {'std': [[0.0883394405245781], [0.0751652866601944], [0.07409299165010452], [0.12206761538982391], [0.10416710376739502], [0.11867869645357132], [0.15680250525474548], [0.07212385535240173], [0.066893... fold_4 {'std': [[0.15121375024318695], [0.09383570402860641], [0.10639220476150513], [0.09952569007873535], [0.060146454721689224], [0.0721314549446106], [0.09489388763904572], [0.0831003338098526], [0.09796... matbench_phonons Fold scores fold mae rmse mape* max_error fold_0 40.2218 99.9366 0.0661 1031.8168 fold_1 41.1190 83.0600 0.0680 721.2376 fold_2 38.8526 70.0409 0.0705 452.0254 fold_3 37.1039 78.3636 0.0710 662.8152 fold_4 36.4648 59.7092 0.0665 342.3226 Fold score stats metric mean max min std mae 38.7524 41.1190 36.4648 1.7732 rmse 78.2220 99.9366 59.7092 13.4507 mape* 0.0684 0.0710 0.0661 0.0020 max_error 642.0435 1031.8168 342.3226 238.5648 Fold parameters fold params dict fold_0 {'std': [[26.26407814025879], [175.91537475585938], [15.736849784851074], [16.808921813964844], [18.036314010620117], [16.40987777709961], [31.393617630004883], [26.229381561279297], [16.6152362823486... fold_1 {'std': [[32.547828674316406], [18.21637535095215], [34.24558639526367], [27.42135238647461], [23.881690979003906], [151.67642211914062], [18.94878578186035], [19.094982147216797], [16.469425201416016... fold_2 {'std': [[21.542694091796875], [21.627588272094727], [15.593945503234863], [67.26934814453125], [11.872613906860352], [11.879145622253418], [12.56555461883545], [24.557519912719727], [22.6404819488525... fold_3 {'std': [[30.841398239135742], [17.576093673706055], [20.379390716552734], [24.910297393798828], [13.184524536132812], [24.256025314331055], [26.51211166381836], [18.35163116455078], [17.3094120025634... fold_4 {'std': [[11.17216968536377], [16.963390350341797], [32.19032287597656], [16.677236557006836], [27.273052215576172], [28.90708351135254], [25.442333221435547], [21.135835647583008], [16.36865615844726... matbench_steels Fold scores fold mae rmse mape* max_error fold_0 112.2905 189.8130 0.0707 931.3261 fold_1 81.9908 115.9188 0.0604 404.5644 fold_2 99.3739 139.4921 0.0699 411.7195 fold_3 93.2877 152.1443 0.0672 827.5305 fold_4 94.1265 152.3995 0.0709 672.9292 Fold score stats metric mean max min std mae 96.2139 112.2905 81.9908 9.8352 rmse 149.9535 189.8130 115.9188 23.9473 mape* 0.0678 0.0709 0.0604 0.0039 max_error 649.6139 931.3261 404.5644 213.6365 Fold parameters fold params dict fold_0 {'std': [[181.11865234375], [192.23825073242188], [42.135902404785156], [54.896053314208984], [164.36167907714844], [53.29085159301758], [41.6890869140625], [68.6667709350586], [96.96005249023438], [3... fold_1 {'std': [[69.18898010253906], [72.29112243652344], [54.083961486816406], [66.05774688720703], [51.890892028808594], [41.604408264160156], [207.0948028564453], [56.53154373168945], [111.53943634033203]... fold_2 {'std': [[179.28602600097656], [267.37554931640625], [125.63412475585938], [117.67745971679688], [43.56736755371094], [56.37009811401367], [37.02374267578125], [116.51256561279297], [162.7328186035156... fold_3 {'std': [[264.0724182128906], [320.3443603515625], [69.12999725341797], [98.54374694824219], [102.41849517822266], [85.94804382324219], [39.834190368652344], [159.82240295410156], [59.77300262451172],... fold_4 {'std': [[172.47418212890625], [83.7674560546875], [355.5929260253906], [119.87616729736328], [59.057350158691406], [119.39688873291016], [50.491127014160156], [79.44507598876953], [93.9663314819336],...","title":"matbench_v0.1: MODNet (v0.1.10)"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#matbench_v01-modnet-v0110","text":"","title":"matbench_v0.1: MODNet (v0.1.10)"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#algorithm-description","text":"MODNet, the Materials Optimal Descriptor Network (v0.1.10). A feed-forward neural network, using all compatible matminer features and a relevance-redundancy based feature selection algorithm. Hyperparameter optimisation is performed with a nested grid search for the 9 smaller tasks, and with a genetic algorithm for the 4 larger tasks ( matbench_perovskites , matbench_mp_gap , matbench_mp_is_metal , matbench_mp_eform . Benchmark results were loaded from https://github.com/ml-evs/modnet-matbench/releases/tag/v0.3.0, archived at 10.5281/zenodo.5562338 .","title":"Algorithm description:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#notes","text":"None Raw data download and example notebook available on the matbench repo .","title":"Notes:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#references-in-bibtex-format","text":"('@article{De_Breuck_2021, doi = {10.1088/1361-648x/ac1280}, url = ' '{https://doi.org/10.1088/1361-648x/ac1280}, year = 2021, month = {jul}, ' 'publisher = {{IOP} Publishing}, volume = {33}, number = {40}, pages = ' '{404002}, author = {Pierre-Paul De Breuck and Matthew L Evans and Gian-Marco ' 'Rignanese}, title = {Robust model benchmarking and bias-imbalance in ' 'data-driven materials science: a case study on {MODNet}}, journal = {Journal ' 'of Physics: Condensed Matter}, abstract = {As the number of novel ' 'data-driven approaches to material science continues to grow, it is crucial ' 'to perform consistent quality, reliability and applicability assessments of ' 'model performance. In this paper, we benchmark the Materials Optimal ' 'Descriptor Network (MODNet) method and architecture against the recently ' 'released MatBench v0.1, a curated test suite of materials datasets. MODNet ' 'is shown to outperform current leaders on 6 of the 13 tasks, while closely ' 'matching the current leaders on a further 2 tasks; MODNet performs ' 'particularly well when the number of samples is below 10\\xa0000. Attention ' 'is paid to two topics of concern when benchmarking models. First, we ' 'encourage the reporting of a more diverse set of metrics as it leads to a ' 'more comprehensive and holistic comparison of model performance. Second, an ' 'equally important task is the uncertainty assessment of a model towards a ' 'target domain. Significant variations in validation errors can be observed, ' 'depending on the imbalance and bias in the training set (i.e., similarity ' 'between training and application space). By using an ensemble MODNet model, ' 'confidence intervals can be built and the uncertainty on individual ' 'predictions can be quantified. Imbalance and bias issues are often ' 'overlooked, and yet are important for successful real-world applications of ' 'machine learning in materials science and condensed matter.}}, ' '@article{DeBreuck2021, doi = {10.1038/s41524-021-00552-2}, url = ' '{https://doi.org/10.1038/s41524-021-00552-2}, year = {2021}, month = jun, ' 'publisher = {Springer Science and Business Media {LLC}}, volume = {7}, ' 'number = {1}, author = {Pierre-Paul De Breuck and Geoffroy Hautier and ' 'Gian-Marco Rignanese}, title = {Materials property prediction for limited ' 'datasets enabled by feature selection and joint learning with {MODNet}}, ' 'journal = {npj Computational Materials}}')","title":"References (in bibtex format):"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#user-metadata","text":"{}","title":"User metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#metadata","text":"tasks recorded 13/13 complete? \u2713 composition complete? \u2713 structure complete? \u2713 regression complete? \u2713 classification complete? \u2713","title":"Metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#software-requirements","text":"{'python': ['modnet==0.1.10', 'matbench==0.2.0']}","title":"Software Requirements"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#task-data","text":"","title":"Task data:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#matbench_dielectric","text":"","title":"matbench_dielectric"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-scores","text":"fold mae rmse mape* max_error fold_0 0.1939 0.7043 0.0657 13.9549 fold_1 0.2669 1.0559 0.0897 19.4132 fold_2 0.4138 2.9360 0.0873 58.9519 fold_3 0.2880 2.2447 0.0593 52.4648 fold_4 0.3223 1.6518 0.1040 28.0662","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-score-stats","text":"metric mean max min std mae 0.2970 0.4138 0.1939 0.0720 rmse 1.7185 2.9360 0.7043 0.8039 mape* 0.0812 0.1040 0.0593 0.0164 max_error 34.5702 58.9519 13.9549 17.9539","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-parameters","text":"fold params dict fold_0 {'std': [[0.06733036786317825], [0.0744570940732956], [0.4551847279071808], [0.6979547739028931], [0.05083160847425461], [0.09550821781158447], [0.19389964640140533], [0.45796462893486023], [0.0457437... fold_1 {'std': [[0.072625070810318], [0.20568081736564636], [0.06122368574142456], [0.1985706239938736], [0.13508345186710358], [0.5729472637176514], [0.18508531153202057], [0.36901697516441345], [0.15902499... fold_2 {'std': [[0.07455231994390488], [0.40818431973457336], [0.17178542912006378], [0.1756463497877121], [0.05205323547124863], [0.18280482292175293], [0.029979035258293152], [0.13881000876426697], [0.1849... fold_3 {'std': [[0.07038000971078873], [0.052069056779146194], [0.11985469609498978], [0.2917916774749756], [0.07153098285198212], [0.16248461604118347], [0.03178466856479645], [0.06994788348674774], [0.0775... fold_4 {'std': [[0.062172286212444305], [0.1020524799823761], [0.046170447021722794], [0.2582769989967346], [0.5205304622650146], [0.09285475313663483], [0.04396134242415428], [0.1190856322646141], [0.159720...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#matbench_expt_gap","text":"","title":"matbench_expt_gap"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-scores_1","text":"fold mae rmse mape* max_error fold_0 0.3272 0.7062 0.3510 6.3096 fold_1 0.3594 0.7340 0.3187 6.3544 fold_2 0.3845 0.8563 0.3841 9.8567 fold_3 0.3259 0.6888 0.3231 5.1081 fold_4 0.3382 0.7334 0.4075 6.5141","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-score-stats_1","text":"metric mean max min std mae 0.3470 0.3845 0.3259 0.0222 rmse 0.7437 0.8563 0.6888 0.0588 mape* 0.3569 0.4075 0.3187 0.0345 max_error 6.8286 9.8567 5.1081 1.5952","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-parameters_1","text":"fold params dict fold_0 {'std': [[0.3934377431869507], [0.6812934875488281], [0.46057939529418945], [0.21048687398433685], [0.39122024178504944], [2.184469699859619], [1.0323148965835571], [1.1202787160873413], [0.8973723649... fold_1 {'std': [[0.42153677344322205], [0.10141757875680923], [0.2717689573764801], [0.0055972738191485405], [0.12942852079868317], [0.19773989915847778], [0.1537284404039383], [0.152150496840477], [0.127241... fold_2 {'std': [[0.5310537219047546], [0.09472547471523285], [0.6016039848327637], [0.7606176137924194], [0.2108703851699829], [0.3892253637313843], [0.8048807382583618], [0.058867525309324265], [0.236589178... fold_3 {'std': [[0.4957612454891205], [0.24328213930130005], [1.1489912271499634], [0.4026401937007904], [0.38004472851753235], [0.235760897397995], [0.2802310287952423], [0.23525512218475342], [0.9116547703... fold_4 {'std': [[0.17024394869804382], [1.0889294147491455], [0.0037015366833657026], [0.45974406599998474], [0.7000935673713684], [1.2791191339492798], [1.037060260772705], [1.1216192245483398], [1.26752507...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#matbench_expt_is_metal","text":"","title":"matbench_expt_is_metal"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-scores_2","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.9269 0.9269 0.9255 0.9269 fold_1 0.9136 0.9136 0.9121 0.9136 fold_2 0.9177 0.9177 0.9173 0.9177 fold_3 0.9177 0.9177 0.9169 0.9177 fold_4 0.9045 0.9045 0.9049 0.9045","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-score-stats_2","text":"metric mean max min std accuracy 0.9161 0.9269 0.9045 0.0073 balanced_accuracy 0.9161 0.9269 0.9045 0.0072 f1 0.9153 0.9255 0.9049 0.0068 rocauc 0.9161 0.9269 0.9045 0.0072","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-parameters_2","text":"fold params dict fold_0 {'std': [[0.17845642566680908, 0.1784563958644867], [0.38880154490470886, 0.38880154490470886], [0.11527526378631592, 0.11527524888515472], [0.29507318139076233, 0.29507318139076233], [0.4294841885566... fold_1 {'std': [[0.250985711812973, 0.250985711812973], [0.15300564467906952, 0.15300562977790833], [0.11072004586458206, 0.11072004586458206], [0.07669822871685028, 0.07669822126626968], [0.1074658855795860... fold_2 {'std': [[0.16936197876930237, 0.16936197876930237], [0.28381362557411194, 0.2838136553764343], [0.3199393153190613, 0.3199393153190613], [0.14618311822414398, 0.1461830586194992], [0.1249608173966407... fold_3 {'std': [[0.13702887296676636, 0.13702887296676636], [0.1351342797279358, 0.1351342648267746], [0.40080583095550537, 0.40080583095550537], [0.1579451709985733, 0.1579451709985733], [0.1655253022909164... fold_4 {'std': [[0.28618890047073364, 0.28618890047073364], [0.27044594287872314, 0.27044594287872314], [0.29565274715423584, 0.29565274715423584], [0.29736053943634033, 0.29736053943634033], [0.237021714448...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#matbench_glass","text":"","title":"matbench_glass"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-scores_3","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.8759 0.8262 0.9153 0.8262 fold_1 0.8539 0.7783 0.9030 0.7783 fold_2 0.8565 0.8063 0.9016 0.8063 fold_3 0.8856 0.8402 0.9217 0.8402 fold_4 0.8662 0.8023 0.9102 0.8023","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-score-stats_3","text":"metric mean max min std accuracy 0.8676 0.8856 0.8539 0.0119 balanced_accuracy 0.8107 0.8402 0.7783 0.0212 f1 0.9104 0.9217 0.9016 0.0075 rocauc 0.8107 0.8402 0.7783 0.0212","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-parameters_3","text":"fold params dict fold_0 {'std': [[0.1912706196308136, 0.1912706196308136], [0.3220626413822174, 0.3220626413822174], [0.38618433475494385, 0.38618433475494385], [0.3689897954463959, 0.36898982524871826], [0.296833336353302, ... fold_1 {'std': [[0.3554098606109619, 0.3554098606109619], [0.19303181767463684, 0.19303181767463684], [0.2967971861362457, 0.2967972159385681], [0.3302050232887268, 0.3302050232887268], [0.2770985960960388, ... fold_2 {'std': [[0.3961077332496643, 0.39610767364501953], [0.19110238552093506, 0.19110238552093506], [0.22688446938991547, 0.22688449919223785], [0.34065985679626465, 0.3406599164009094], [0.25127366185188... fold_3 {'std': [[0.18866945803165436, 0.18866944313049316], [0.17435620725154877, 0.17435620725154877], [0.18012933433055878, 0.1801293045282364], [0.11812251806259155, 0.11812251806259155], [0.1196716576814... fold_4 {'std': [[0.16088004410266876, 0.16088005900382996], [0.2377340942621231, 0.2377340942621231], [0.3177623748779297, 0.3177623748779297], [0.3061956465244293, 0.3061956465244293], [0.2921837270259857, ...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#matbench_jdft2d","text":"","title":"matbench_jdft2d"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-scores_4","text":"fold mae rmse mape* max_error fold_0 27.5769 49.7512 21.3632 243.2504 fold_1 27.9722 63.3103 0.2282 364.1909 fold_2 51.3402 142.7963 0.6111 845.7528 fold_3 26.9141 52.8447 0.2724 311.7558 fold_4 38.8806 152.4413 0.4853 1534.9797","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-score-stats_4","text":"metric mean max min std mae 34.5368 51.3402 26.9141 9.4959 rmse 92.2288 152.4413 49.7512 45.5508 mape* 4.5920 21.3632 0.2282 8.3868 max_error 659.9859 1534.9797 243.2504 486.3231","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-parameters_4","text":"fold params dict fold_0 {'std': [[9.497343063354492], [15.862295150756836], [74.97210693359375], [25.96040916442871], [47.26897048950195], [14.80854606628418], [22.77548599243164], [10.362432479858398], [8.255328178405762], ... fold_1 {'std': [[5.382687568664551], [25.67997932434082], [16.605792999267578], [7.763948917388916], [10.631340026855469], [36.21831512451172], [12.867671012878418], [3.2303359508514404], [38.958377838134766... fold_2 {'std': [[71.73993682861328], [18.688243865966797], [7.084332466125488], [16.097488403320312], [83.72747802734375], [12.528894424438477], [16.004690170288086], [14.574416160583496], [7.346397399902344... fold_3 {'std': [[2.2033019065856934], [17.148666381835938], [6.929365634918213], [3.3733177185058594], [19.175621032714844], [9.659783363342285], [2.456592321395874], [13.089242935180664], [44.94028091430664... fold_4 {'std': [[24.92951202392578], [17.333660125732422], [10.269680976867676], [4.752265453338623], [3.5876128673553467], [4.854499340057373], [12.900960922241211], [6.644251823425293], [9.120869636535645]...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#matbench_log_gvrh","text":"","title":"matbench_log_gvrh"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-scores_5","text":"fold mae rmse mape* max_error fold_0 0.0731 0.1089 0.0576 0.9014 fold_1 0.0738 0.1111 0.0579 1.1745 fold_2 0.0731 0.1101 0.0587 0.9076 fold_3 0.0738 0.1115 0.0567 0.9225 fold_4 0.0718 0.1101 0.0560 0.8007","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-score-stats_5","text":"metric mean max min std mae 0.0731 0.0738 0.0718 0.0007 rmse 0.1103 0.1115 0.1089 0.0009 mape* 0.0574 0.0587 0.0560 0.0009 max_error 0.9413 1.1745 0.8007 0.1243","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-parameters_5","text":"fold params dict fold_0 {'std': [0.06334669888019562, 0.04876908287405968, 0.0713210254907608, 0.06149518862366676, 0.05233978480100632, 0.053833525627851486, 0.045166339725255966, 0.09107258170843124, 0.08312246203422546, 0... fold_1 {'std': [0.04058562591671944, 0.09664303809404373, 0.06196340546011925, 0.07074710726737976, 0.05361659824848175, 0.05300111323595047, 0.04533914476633072, 0.060226064175367355, 0.15155699849128723, 0... fold_2 {'std': [0.04066888242959976, 0.05564378947019577, 0.05513373762369156, 0.03629153221845627, 0.08530019223690033, 0.0363982692360878, 0.07014258950948715, 0.07834821194410324, 0.056601572781801224, 0.... fold_3 {'std': [0.06376504898071289, 0.2838374972343445, 0.025865282863378525, 0.04888685792684555, 0.18576562404632568, 0.045733798295259476, 0.047175027430057526, 0.04196206107735634, 0.06469003856182098, ... fold_4 {'std': [0.06425822526216507, 0.07589271664619446, 0.04857879504561424, 0.07567392289638519, 0.07976284623146057, 0.05443073436617851, 0.0474713109433651, 0.08143744617700577, 0.10169852524995804, 0.0...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#matbench_log_kvrh","text":"","title":"matbench_log_kvrh"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-scores_6","text":"fold mae rmse mape* max_error fold_0 0.0536 0.1013 0.0356 1.5366 fold_1 0.0559 0.1079 0.0366 1.2998 fold_2 0.0510 0.0949 0.0340 1.1808 fold_3 0.0585 0.1126 0.0418 1.1355 fold_4 0.0549 0.1046 0.0370 1.3202","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-score-stats_6","text":"metric mean max min std mae 0.0548 0.0585 0.0510 0.0025 rmse 0.1043 0.1126 0.0949 0.0060 mape* 0.0370 0.0418 0.0340 0.0026 max_error 1.2946 1.5366 1.1355 0.1397","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-parameters_6","text":"fold params dict fold_0 {'std': [0.03473027050495148, 0.05344022810459137, 0.04392522946000099, 0.09693300724029541, 0.0621185339987278, 0.0515923835337162, 0.034392938017845154, 0.0368841215968132, 0.09843463450670242, 0.03... fold_1 {'std': [0.037284620106220245, 0.0660589188337326, 0.05483892932534218, 0.05504067987203598, 0.045397065579891205, 0.053156472742557526, 0.04068203642964363, 0.04492218419909477, 0.1503378003835678, 0... fold_2 {'std': [0.04053986072540283, 0.039209164679050446, 0.04213540256023407, 0.036292385309934616, 0.06385202705860138, 0.032488591969013214, 0.0784469023346901, 0.0694998949766159, 0.050309233367443085, ... fold_3 {'std': [0.04948587343096733, 0.11705353856086731, 0.025648461654782295, 0.03585298731923103, 0.11334579437971115, 0.03046250157058239, 0.040365662425756454, 0.03331249952316284, 0.038164108991622925,... fold_4 {'std': [0.04571979492902756, 0.0366676039993763, 0.036114685237407684, 0.06556463986635208, 0.07480020076036453, 0.03638936206698418, 0.05547630041837692, 0.10959770530462265, 0.16662324965000153, 0....","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#matbench_mp_e_form","text":"","title":"matbench_mp_e_form"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-scores_7","text":"fold mae rmse mape* max_error fold_0 0.0402 0.0817 0.3786 4.0438 fold_1 0.0497 0.1018 0.3121 4.8803 fold_2 0.0475 0.0905 0.2562 1.6230 fold_3 0.0464 0.0889 0.3515 1.5189 fold_4 0.0400 0.0812 0.2882 3.3787","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-score-stats_7","text":"metric mean max min std mae 0.0448 0.0497 0.0400 0.0039 rmse 0.0888 0.1018 0.0812 0.0075 mape* 0.3173 0.3786 0.2562 0.0436 max_error 3.0889 4.8803 1.5189 1.3281","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-parameters_7","text":"fold params dict fold_0 {'std': [[0.0636366754770279], [0.02924380451440811], [0.11916627734899521], [0.11677506566047668], [0.1760452687740326], [0.1811746507883072], [0.08742818236351013], [0.1322329342365265], [0.14015400... fold_1 {'std': [[0.10467445850372314], [0.2568114399909973], [0.08591523766517639], [0.11847899109125137], [1.0217572450637817], [0.2770746350288391], [0.20971281826496124], [0.19037210941314697], [0.0730839... fold_2 {'std': [[0.07034026086330414], [0.07515157759189606], [0.1308293640613556], [0.23308764398097992], [0.2118426114320755], [0.1338074803352356], [0.17896589636802673], [0.09289371967315674], [0.0988285... fold_3 {'std': [[0.17922396957874298], [0.21060164272785187], [0.04639369249343872], [0.0925942063331604], [0.06210273131728172], [0.28422462940216064], [0.2840571105480194], [0.2760363817214966], [0.1231188... fold_4 {'std': [[0.1517249494791031], [0.13391436636447906], [0.40770843625068665], [0.13683228194713593], [0.124815434217453], [0.042988162487745285], [0.13916344940662384], [0.06709353625774384], [0.053676...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#matbench_mp_gap","text":"","title":"matbench_mp_gap"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-scores_8","text":"fold mae rmse mape* max_error fold_0 0.2147 0.4441 2.8966 5.0558 fold_1 0.2161 0.4484 2.6899 6.2874 fold_2 0.2165 0.4433 4.1912 7.5685 fold_3 0.2309 0.4705 4.6749 6.9325 fold_4 0.2211 0.4564 4.9590 4.9406","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-score-stats_8","text":"metric mean max min std mae 0.2199 0.2309 0.2147 0.0059 rmse 0.4525 0.4705 0.4433 0.0101 mape* 3.8823 4.9590 2.6899 0.9248 max_error 6.1570 7.5685 4.9406 1.0299","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-parameters_8","text":"fold params dict fold_0 {'std': [[0.2779600918292999], [0.1588134467601776], [0.0013879217440262437], [0.0013879217440262437], [0.0013879217440262437], [0.15315547585487366], [0.4301016926765442], [0.19215451180934906], [0.0... fold_1 {'std': [[0.0023871688172221184], [0.0023871688172221184], [0.0023871688172221184], [0.0023871688172221184], [0.0023871688172221184], [0.0023871688172221184], [0.35058045387268066], [0.439932823181152... fold_2 {'std': [[0.17874807119369507], [0.0015997957671061158], [0.0015997957671061158], [0.35170578956604004], [0.0015997957671061158], [0.0015997957671061158], [0.0015997957671061158], [0.00156632636208087... fold_3 {'std': [[0.004137647803872824], [0.004137647803872824], [0.004137647803872824], [0.004137647803872824], [0.004137647803872824], [0.004137647803872824], [0.004137647803872824], [0.004137647803872824],... fold_4 {'std': [[0.4364481568336487], [0.0038157568778842688], [0.003807253669947386], [0.003807792905718088], [0.003815052565187216], [0.0038899907376617193], [0.0038147747982293367], [0.19303250312805176],...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#matbench_mp_is_metal","text":"","title":"matbench_mp_is_metal"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-scores_9","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.8515 0.8415 0.8175 0.8415 fold_1 0.8824 0.8709 0.8526 0.8709 fold_2 0.5650 0.5000 0.0000 0.5000 fold_3 0.8575 0.8447 0.8200 0.8447 fold_4 0.8588 0.8453 0.8203 0.8453","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-score-stats_9","text":"metric mean max min std accuracy 0.8031 0.8824 0.5650 0.1195 balanced_accuracy 0.7805 0.8709 0.5000 0.1406 f1 0.6621 0.8526 0.0000 0.3313 rocauc 0.7805 0.8709 0.5000 0.1406","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-parameters_9","text":"fold params dict fold_0 {'std': [[0.0416233129799366, 0.041623327881097794], [0.06803157180547714, 0.06803156435489655], [0.008189204148948193, 0.008189203217625618], [0.0037693644408136606, 0.003769365604966879], [0.0107376... fold_1 {'std': [[0.2837996482849121, 0.2837996482849121], [0.28376519680023193, 0.28376519680023193], [0.2669283449649811, 0.2669283449649811], [0.2666773200035095, 0.2666773200035095], [0.3405170142650604, ... fold_2 {'std': [[0.1777520626783371, 0.1777520775794983], [0.1777520626783371, 0.1777520775794983], [0.1777520626783371, 0.1777520775794983], [0.1777520626783371, 0.1777520775794983], [0.1777520626783371, 0.... fold_3 {'std': [[0.22140955924987793, 0.22140958905220032], [0.2370903342962265, 0.2370903342962265], [0.2370903342962265, 0.2370903342962265], [0.2370903342962265, 0.2370903342962265], [0.2370903342962265, ... fold_4 {'std': [[0.09707242995500565, 0.09707243740558624], [0.259949266910553, 0.2599492371082306], [0.09707242995500565, 0.09707243740558624], [0.23583407700061798, 0.2358340471982956], [0.2358340770006179...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#matbench_perovskites","text":"","title":"matbench_perovskites"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-scores_10","text":"fold mae rmse mape* max_error fold_0 0.0932 0.1304 0.0970 0.8705 fold_1 0.0939 0.1283 0.1058 1.0063 fold_2 0.0861 0.1216 0.0939 0.9432 fold_3 0.0892 0.1274 0.0995 0.8501 fold_4 0.0914 0.1310 0.0894 1.1780","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-score-stats_10","text":"metric mean max min std mae 0.0908 0.0939 0.0861 0.0028 rmse 0.1277 0.1310 0.1216 0.0033 mape* 0.0971 0.1058 0.0894 0.0055 max_error 0.9696 1.1780 0.8501 0.1180","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-parameters_10","text":"fold params dict fold_0 {'std': [[0.10714199393987656], [0.0770525336265564], [0.07103670388460159], [0.047520048916339874], [0.09854762256145477], [0.054435212165117264], [0.06670265644788742], [0.12760771811008453], [0.110... fold_1 {'std': [[0.06410787999629974], [0.09309504926204681], [0.07608579844236374], [0.08194778114557266], [0.11951383203268051], [0.07481898367404938], [0.04808051139116287], [0.08761747926473618], [0.0603... fold_2 {'std': [[0.10084810853004456], [0.10042519122362137], [0.10863561928272247], [0.11654899269342422], [0.08363119512796402], [0.11726558208465576], [0.12616018950939178], [0.104669950902462], [0.083491... fold_3 {'std': [[0.0883394405245781], [0.0751652866601944], [0.07409299165010452], [0.12206761538982391], [0.10416710376739502], [0.11867869645357132], [0.15680250525474548], [0.07212385535240173], [0.066893... fold_4 {'std': [[0.15121375024318695], [0.09383570402860641], [0.10639220476150513], [0.09952569007873535], [0.060146454721689224], [0.0721314549446106], [0.09489388763904572], [0.0831003338098526], [0.09796...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#matbench_phonons","text":"","title":"matbench_phonons"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-scores_11","text":"fold mae rmse mape* max_error fold_0 40.2218 99.9366 0.0661 1031.8168 fold_1 41.1190 83.0600 0.0680 721.2376 fold_2 38.8526 70.0409 0.0705 452.0254 fold_3 37.1039 78.3636 0.0710 662.8152 fold_4 36.4648 59.7092 0.0665 342.3226","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-score-stats_11","text":"metric mean max min std mae 38.7524 41.1190 36.4648 1.7732 rmse 78.2220 99.9366 59.7092 13.4507 mape* 0.0684 0.0710 0.0661 0.0020 max_error 642.0435 1031.8168 342.3226 238.5648","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-parameters_11","text":"fold params dict fold_0 {'std': [[26.26407814025879], [175.91537475585938], [15.736849784851074], [16.808921813964844], [18.036314010620117], [16.40987777709961], [31.393617630004883], [26.229381561279297], [16.6152362823486... fold_1 {'std': [[32.547828674316406], [18.21637535095215], [34.24558639526367], [27.42135238647461], [23.881690979003906], [151.67642211914062], [18.94878578186035], [19.094982147216797], [16.469425201416016... fold_2 {'std': [[21.542694091796875], [21.627588272094727], [15.593945503234863], [67.26934814453125], [11.872613906860352], [11.879145622253418], [12.56555461883545], [24.557519912719727], [22.6404819488525... fold_3 {'std': [[30.841398239135742], [17.576093673706055], [20.379390716552734], [24.910297393798828], [13.184524536132812], [24.256025314331055], [26.51211166381836], [18.35163116455078], [17.3094120025634... fold_4 {'std': [[11.17216968536377], [16.963390350341797], [32.19032287597656], [16.677236557006836], [27.273052215576172], [28.90708351135254], [25.442333221435547], [21.135835647583008], [16.36865615844726...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#matbench_steels","text":"","title":"matbench_steels"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-scores_12","text":"fold mae rmse mape* max_error fold_0 112.2905 189.8130 0.0707 931.3261 fold_1 81.9908 115.9188 0.0604 404.5644 fold_2 99.3739 139.4921 0.0699 411.7195 fold_3 93.2877 152.1443 0.0672 827.5305 fold_4 94.1265 152.3995 0.0709 672.9292","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-score-stats_12","text":"metric mean max min std mae 96.2139 112.2905 81.9908 9.8352 rmse 149.9535 189.8130 115.9188 23.9473 mape* 0.0678 0.0709 0.0604 0.0039 max_error 649.6139 931.3261 404.5644 213.6365","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_modnet_v0.1.10/#fold-parameters_12","text":"fold params dict fold_0 {'std': [[181.11865234375], [192.23825073242188], [42.135902404785156], [54.896053314208984], [164.36167907714844], [53.29085159301758], [41.6890869140625], [68.6667709350586], [96.96005249023438], [3... fold_1 {'std': [[69.18898010253906], [72.29112243652344], [54.083961486816406], [66.05774688720703], [51.890892028808594], [41.604408264160156], [207.0948028564453], [56.53154373168945], [111.53943634033203]... fold_2 {'std': [[179.28602600097656], [267.37554931640625], [125.63412475585938], [117.67745971679688], [43.56736755371094], [56.37009811401367], [37.02374267578125], [116.51256561279297], [162.7328186035156... fold_3 {'std': [[264.0724182128906], [320.3443603515625], [69.12999725341797], [98.54374694824219], [102.41849517822266], [85.94804382324219], [39.834190368652344], [159.82240295410156], [59.77300262451172],... fold_4 {'std': [[172.47418212890625], [83.7674560546875], [355.5929260253906], [119.87616729736328], [59.057350158691406], [119.39688873291016], [50.491127014160156], [79.44507598876953], [93.9663314819336],...","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/","text":"matbench_v0.1: RF-SCM/Magpie Algorithm description: A random forest using features from the Sine Coulomb Matrix and MagPie featurization algorthims. Sine Coulomb Matrix creates structural features based on Coulombic interactions inside a periodic boundary condition (i.e., for crystalline materials with known structure). MagPie features are weighted elemental features based on elemental data such as electronegativity, melting point, and electron affinity. Algorithms were run inside of the Automatminer v1.0.3.20191111 framework for convenience, though no auto-featurization or AutoML were run. Data cleaning dropped features with more than 1% nan samples, imputing missing samples using the mean of the training data. No feature reduction was performed. Both featurization techniques were applied to structure problems, only MagPie features were applied to problems without structure. Random forest uses 500 estimators. Notes: No hyperparameter tuning was performed on the RF, as a large, constant number of trees were used in constructing each fold's model; the entire training+validation set was used as training data for the RF. Raw data download and example notebook available on the matbench repo . References (in bibtex format): ['@article{Dunn2020,\\n' ' doi = {10.1038/s41524-020-00406-3},\\n' ' url = {https://doi.org/10.1038/s41524-020-00406-3},\\n' ' year = {2020},\\n' ' month = sep,\\n' ' publisher = {Springer Science and Business Media {LLC}},\\n' ' volume = {6},\\n' ' number = {1},\\n' ' author = {Alexander Dunn and Qi Wang and Alex Ganose and Daniel Dopp and ' 'Anubhav Jain},\\n' ' title = {Benchmarking materials property prediction methods: the Matbench ' 'test set and Automatminer reference algorithm},\\n' ' journal = {npj Computational Materials}\\n' '}', '@article{Breiman2001,\\n' ' doi = {10.1023/a:1010933404324},\\n' ' url = {https://doi.org/10.1023/a:1010933404324},\\n' ' year = {2001},\\n' ' publisher = {Springer Science and Business Media {LLC}},\\n' ' volume = {45},\\n' ' number = {1},\\n' ' pages = {5--32},\\n' ' author = {Leo Breiman},\\n' ' journal = {Machine Learning}\\n' '}', '@article{Ward2016,\\n' ' doi = {10.1038/npjcompumats.2016.28},\\n' ' url = {https://doi.org/10.1038/npjcompumats.2016.28},\\n' ' year = {2016},\\n' ' month = aug,\\n' ' publisher = {Springer Science and Business Media {LLC}},\\n' ' volume = {2},\\n' ' number = {1},\\n' ' author = {Logan Ward and Ankit Agrawal and Alok Choudhary and Christopher ' 'Wolverton},\\n' ' title = {A general-purpose machine learning framework for predicting ' 'properties of inorganic materials},\\n' ' journal = {npj Computational Materials}\\n' '}', '@article {QUA:QUA24917,author = {Faber, Felix and Lindmaa, Alexander and von ' 'Lilienfeld, O. Anatole and Armiento, Rickard},title = {Crystal structure ' 'representations for machine learning models of formation energies},journal = ' '{International Journal of Quantum Chemistry},volume = {115},number = ' '{16},issn = {1097-461X},url = {http://dx.doi.org/10.1002/qua.24917},doi = ' '{10.1002/qua.24917},pages = {1094--1101},keywords = {machine learning, ' 'formation energies, representations, crystal structure, periodic ' 'systems},year = {2015},}'] User metadata: {'__deepcopy__': {}, '__getstate__': {}, '_ipython_canary_method_should_not_exist_': {'__deepcopy__': {}, '__getstate__': {}}, 'autofeaturizer_kwargs': {'n_jobs': 10, 'preset': 'debug'}, 'best_pipeline': 'RandomForestRegressor(bootstrap=true, criterion=mse, ' 'max_depth=null,\\n' ' max_features=auto, max_leaf_nodes=null,\\n' ' min_impurity_decrease=0.0, ' 'min_impurity_split=null,\\n' ' min_samples_leaf=1, min_samples_split=2,\\n' ' min_weight_fraction_leaf=0.0, n_estimators=500, ' 'n_jobs=null,\\n' ' oob_score=false, random_state=null, verbose=0, ' 'warm_start=false)', 'cleaner_kwargs': {'feature_na_method': 'mean', 'max_na_frac': 0.01, 'na_method_fit': 'drop', 'na_method_transform': 'mean'}, 'features_all': ['MagpieData minimum Number', 'MagpieData maximum Number', 'MagpieData range Number', 'MagpieData mean Number', 'MagpieData avg_dev Number', 'MagpieData mode Number', 'MagpieData minimum MendeleevNumber', 'MagpieData maximum MendeleevNumber', 'MagpieData range MendeleevNumber', 'MagpieData mean MendeleevNumber', 'MagpieData avg_dev MendeleevNumber', 'MagpieData mode MendeleevNumber', 'MagpieData minimum AtomicWeight', 'MagpieData maximum AtomicWeight', 'MagpieData range AtomicWeight', 'MagpieData mean AtomicWeight', 'MagpieData avg_dev AtomicWeight', 'MagpieData mode AtomicWeight', 'MagpieData minimum MeltingT', 'MagpieData maximum MeltingT', 'MagpieData range MeltingT', 'MagpieData mean MeltingT', 'MagpieData avg_dev MeltingT', 'MagpieData mode MeltingT', 'MagpieData minimum Column', 'MagpieData maximum Column', 'MagpieData range Column', 'MagpieData mean Column', 'MagpieData avg_dev Column', 'MagpieData mode Column', 'MagpieData minimum Row', 'MagpieData maximum Row', 'MagpieData range Row', 'MagpieData mean Row', 'MagpieData avg_dev Row', 'MagpieData mode Row', 'MagpieData minimum CovalentRadius', 'MagpieData maximum CovalentRadius', 'MagpieData range CovalentRadius', 'MagpieData mean CovalentRadius', 'MagpieData avg_dev CovalentRadius', 'MagpieData mode CovalentRadius', 'MagpieData minimum Electronegativity', 'MagpieData maximum Electronegativity', 'MagpieData range Electronegativity', 'MagpieData mean Electronegativity', 'MagpieData avg_dev Electronegativity', 'MagpieData mode Electronegativity', 'MagpieData minimum NsValence', 'MagpieData maximum NsValence', 'MagpieData range NsValence', 'MagpieData mean NsValence', 'MagpieData avg_dev NsValence', 'MagpieData mode NsValence', 'MagpieData minimum NpValence', 'MagpieData maximum NpValence', 'MagpieData range NpValence', 'MagpieData mean NpValence', 'MagpieData avg_dev NpValence', 'MagpieData mode NpValence', 'MagpieData minimum NdValence', 'MagpieData maximum NdValence', 'MagpieData range NdValence', 'MagpieData mean NdValence', 'MagpieData avg_dev NdValence', 'MagpieData mode NdValence', 'MagpieData minimum NfValence', 'MagpieData maximum NfValence', 'MagpieData range NfValence', 'MagpieData mean NfValence', 'MagpieData avg_dev NfValence', 'MagpieData mode NfValence', 'MagpieData minimum NValence', 'MagpieData maximum NValence', 'MagpieData range NValence', 'MagpieData mean NValence', 'MagpieData avg_dev NValence', 'MagpieData mode NValence', 'MagpieData minimum NsUnfilled', 'MagpieData maximum NsUnfilled', 'MagpieData range NsUnfilled', 'MagpieData mean NsUnfilled', 'MagpieData avg_dev NsUnfilled', 'MagpieData mode NsUnfilled', 'MagpieData minimum NpUnfilled', 'MagpieData maximum NpUnfilled', 'MagpieData range NpUnfilled', 'MagpieData mean NpUnfilled', 'MagpieData avg_dev NpUnfilled', 'MagpieData mode NpUnfilled', 'MagpieData minimum NdUnfilled', 'MagpieData maximum NdUnfilled', 'MagpieData range NdUnfilled', 'MagpieData mean NdUnfilled', 'MagpieData avg_dev NdUnfilled', 'MagpieData mode NdUnfilled', 'MagpieData minimum NfUnfilled', 'MagpieData maximum NfUnfilled', 'MagpieData range NfUnfilled', 'MagpieData mean NfUnfilled', 'MagpieData avg_dev NfUnfilled', 'MagpieData mode NfUnfilled', 'MagpieData minimum NUnfilled', 'MagpieData maximum NUnfilled', 'MagpieData range NUnfilled', 'MagpieData mean NUnfilled', 'MagpieData avg_dev NUnfilled', 'MagpieData mode NUnfilled', 'MagpieData minimum GSvolume_pa', 'MagpieData maximum GSvolume_pa', 'MagpieData range GSvolume_pa', 'MagpieData mean GSvolume_pa', 'MagpieData avg_dev GSvolume_pa', 'MagpieData mode GSvolume_pa', 'MagpieData minimum GSbandgap', 'MagpieData maximum GSbandgap', 'MagpieData range GSbandgap', 'MagpieData mean GSbandgap', 'MagpieData avg_dev GSbandgap', 'MagpieData mode GSbandgap', 'MagpieData minimum GSmagmom', 'MagpieData maximum GSmagmom', 'MagpieData range GSmagmom', 'MagpieData mean GSmagmom', 'MagpieData avg_dev GSmagmom', 'MagpieData mode GSmagmom', 'MagpieData minimum SpaceGroupNumber', 'MagpieData maximum SpaceGroupNumber', 'MagpieData range SpaceGroupNumber', 'MagpieData mean SpaceGroupNumber', 'MagpieData avg_dev SpaceGroupNumber', 'MagpieData mode SpaceGroupNumber', 'sine coulomb matrix eig 0', 'sine coulomb matrix eig 1', 'sine coulomb matrix eig 2', 'sine coulomb matrix eig 3', 'sine coulomb matrix eig 4', 'sine coulomb matrix eig 5', 'sine coulomb matrix eig 6', 'sine coulomb matrix eig 7', 'sine coulomb matrix eig 8', 'sine coulomb matrix eig 9', 'sine coulomb matrix eig 10', 'sine coulomb matrix eig 11', 'sine coulomb matrix eig 12', 'sine coulomb matrix eig 13', 'sine coulomb matrix eig 14', 'sine coulomb matrix eig 15', 'sine coulomb matrix eig 16', 'sine coulomb matrix eig 17', 'sine coulomb matrix eig 18', 'sine coulomb matrix eig 19', 'sine coulomb matrix eig 20', 'sine coulomb matrix eig 21', 'sine coulomb matrix eig 22', 'sine coulomb matrix eig 23', 'sine coulomb matrix eig 24', 'sine coulomb matrix eig 25', 'sine coulomb matrix eig 26', 'sine coulomb matrix eig 27', 'sine coulomb matrix eig 28', 'sine coulomb matrix eig 29', 'sine coulomb matrix eig 30', 'sine coulomb matrix eig 31', 'sine coulomb matrix eig 32', 'sine coulomb matrix eig 33', 'sine coulomb matrix eig 34', 'sine coulomb matrix eig 35', 'sine coulomb matrix eig 36', 'sine coulomb matrix eig 37', 'sine coulomb matrix eig 38', 'sine coulomb matrix eig 39', 'sine coulomb matrix eig 40', 'sine coulomb matrix eig 41', 'sine coulomb matrix eig 42', 'sine coulomb matrix eig 43', 'sine coulomb matrix eig 44', 'sine coulomb matrix eig 45', 'sine coulomb matrix eig 46', 'sine coulomb matrix eig 47', 'sine coulomb matrix eig 48', 'sine coulomb matrix eig 49', 'sine coulomb matrix eig 50', 'sine coulomb matrix eig 51', 'sine coulomb matrix eig 52', 'sine coulomb matrix eig 53', 'sine coulomb matrix eig 54', 'sine coulomb matrix eig 55', 'sine coulomb matrix eig 56', 'sine coulomb matrix eig 57', 'sine coulomb matrix eig 58', 'sine coulomb matrix eig 59', 'sine coulomb matrix eig 60', 'sine coulomb matrix eig 61', 'sine coulomb matrix eig 62', 'sine coulomb matrix eig 63', 'sine coulomb matrix eig 64', 'sine coulomb matrix eig 65', 'sine coulomb matrix eig 66', 'sine coulomb matrix eig 67', 'sine coulomb matrix eig 68', 'sine coulomb matrix eig 69', 'sine coulomb matrix eig 70', 'sine coulomb matrix eig 71', 'sine coulomb matrix eig 72', 'sine coulomb matrix eig 73', 'sine coulomb matrix eig 74', 'sine coulomb matrix eig 75', 'sine coulomb matrix eig 76', 'sine coulomb matrix eig 77', 'sine coulomb matrix eig 78', 'sine coulomb matrix eig 79', 'sine coulomb matrix eig 80', 'sine coulomb matrix eig 81', 'sine coulomb matrix eig 82', 'sine coulomb matrix eig 83', 'sine coulomb matrix eig 84', 'sine coulomb matrix eig 85', 'sine coulomb matrix eig 86', 'sine coulomb matrix eig 87', 'sine coulomb matrix eig 88', 'sine coulomb matrix eig 89', 'sine coulomb matrix eig 90', 'sine coulomb matrix eig 91', 'sine coulomb matrix eig 92', 'sine coulomb matrix eig 93', 'sine coulomb matrix eig 94', 'sine coulomb matrix eig 95', 'sine coulomb matrix eig 96', 'sine coulomb matrix eig 97', 'sine coulomb matrix eig 98', 'sine coulomb matrix eig 99', 'sine coulomb matrix eig 100', 'sine coulomb matrix eig 101', 'sine coulomb matrix eig 102', 'sine coulomb matrix eig 103', 'sine coulomb matrix eig 104', 'sine coulomb matrix eig 105', 'sine coulomb matrix eig 106', 'sine coulomb matrix eig 107', 'sine coulomb matrix eig 108', 'sine coulomb matrix eig 109', 'sine coulomb matrix eig 110', 'sine coulomb matrix eig 111', 'sine coulomb matrix eig 112', 'sine coulomb matrix eig 113', 'sine coulomb matrix eig 114', 'sine coulomb matrix eig 115', 'sine coulomb matrix eig 116', 'sine coulomb matrix eig 117', 'sine coulomb matrix eig 118', 'sine coulomb matrix eig 119', 'sine coulomb matrix eig 120', 'sine coulomb matrix eig 121', 'sine coulomb matrix eig 122', 'sine coulomb matrix eig 123', 'sine coulomb matrix eig 124', 'sine coulomb matrix eig 125', 'sine coulomb matrix eig 126', 'sine coulomb matrix eig 127', 'sine coulomb matrix eig 128', 'sine coulomb matrix eig 129', 'sine coulomb matrix eig 130', 'sine coulomb matrix eig 131', 'sine coulomb matrix eig 132', 'sine coulomb matrix eig 133', 'sine coulomb matrix eig 134', 'sine coulomb matrix eig 135', 'sine coulomb matrix eig 136', 'sine coulomb matrix eig 137', 'sine coulomb matrix eig 138', 'sine coulomb matrix eig 139', 'sine coulomb matrix eig 140', 'sine coulomb matrix eig 141', 'sine coulomb matrix eig 142', 'sine coulomb matrix eig 143', 'sine coulomb matrix eig 144', 'sine coulomb matrix eig 145', 'sine coulomb matrix eig 146', 'sine coulomb matrix eig 147', 'sine coulomb matrix eig 148', 'sine coulomb matrix eig 149', 'sine coulomb matrix eig 150', 'sine coulomb matrix eig 151', 'sine coulomb matrix eig 152', 'sine coulomb matrix eig 153', 'sine coulomb matrix eig 154', 'sine coulomb matrix eig 155', 'sine coulomb matrix eig 156', 'sine coulomb matrix eig 157', 'sine coulomb matrix eig 158', 'sine coulomb matrix eig 159', 'sine coulomb matrix eig 160', 'sine coulomb matrix eig 161', 'sine coulomb matrix eig 162', 'sine coulomb matrix eig 163', 'sine coulomb matrix eig 164', 'sine coulomb matrix eig 165', 'sine coulomb matrix eig 166', 'sine coulomb matrix eig 167', 'sine coulomb matrix eig 168', 'sine coulomb matrix eig 169', 'sine coulomb matrix eig 170', 'sine coulomb matrix eig 171', 'sine coulomb matrix eig 172', 'sine coulomb matrix eig 173', 'sine coulomb matrix eig 174', 'sine coulomb matrix eig 175', 'sine coulomb matrix eig 176', 'sine coulomb matrix eig 177', 'sine coulomb matrix eig 178', 'sine coulomb matrix eig 179', 'sine coulomb matrix eig 180', 'sine coulomb matrix eig 181', 'sine coulomb matrix eig 182', 'sine coulomb matrix eig 183', 'sine coulomb matrix eig 184', 'sine coulomb matrix eig 185', 'sine coulomb matrix eig 186', 'sine coulomb matrix eig 187', 'sine coulomb matrix eig 188', 'sine coulomb matrix eig 189', 'sine coulomb matrix eig 190', 'sine coulomb matrix eig 191', 'sine coulomb matrix eig 192', 'sine coulomb matrix eig 193', 'sine coulomb matrix eig 194', 'sine coulomb matrix eig 195', 'sine coulomb matrix eig 196', 'sine coulomb matrix eig 197', 'sine coulomb matrix eig 198', 'sine coulomb matrix eig 199', 'sine coulomb matrix eig 200', 'sine coulomb matrix eig 201', 'sine coulomb matrix eig 202', 'sine coulomb matrix eig 203', 'sine coulomb matrix eig 204', 'sine coulomb matrix eig 205', 'sine coulomb matrix eig 206', 'sine coulomb matrix eig 207', 'sine coulomb matrix eig 208', 'sine coulomb matrix eig 209', 'sine coulomb matrix eig 210', 'sine coulomb matrix eig 211', 'sine coulomb matrix eig 212', 'sine coulomb matrix eig 213', 'sine coulomb matrix eig 214', 'sine coulomb matrix eig 215', 'sine coulomb matrix eig 216', 'sine coulomb matrix eig 217', 'sine coulomb matrix eig 218', 'sine coulomb matrix eig 219', 'sine coulomb matrix eig 220', 'sine coulomb matrix eig 221', 'sine coulomb matrix eig 222', 'sine coulomb matrix eig 223', 'sine coulomb matrix eig 224', 'sine coulomb matrix eig 225', 'sine coulomb matrix eig 226', 'sine coulomb matrix eig 227', 'sine coulomb matrix eig 228', 'sine coulomb matrix eig 229', 'sine coulomb matrix eig 230', 'sine coulomb matrix eig 231', 'sine coulomb matrix eig 232', 'sine coulomb matrix eig 233', 'sine coulomb matrix eig 234', 'sine coulomb matrix eig 235', 'sine coulomb matrix eig 236', 'sine coulomb matrix eig 237', 'sine coulomb matrix eig 238', 'sine coulomb matrix eig 239', 'sine coulomb matrix eig 240', 'sine coulomb matrix eig 241', 'sine coulomb matrix eig 242', 'sine coulomb matrix eig 243', 'sine coulomb matrix eig 244', 'sine coulomb matrix eig 245', 'sine coulomb matrix eig 246', 'sine coulomb matrix eig 247', 'sine coulomb matrix eig 248', 'sine coulomb matrix eig 249', 'sine coulomb matrix eig 250', 'sine coulomb matrix eig 251', 'sine coulomb matrix eig 252', 'sine coulomb matrix eig 253', 'sine coulomb matrix eig 254', 'sine coulomb matrix eig 255', 'sine coulomb matrix eig 256', 'sine coulomb matrix eig 257', 'sine coulomb matrix eig 258', 'sine coulomb matrix eig 259', 'sine coulomb matrix eig 260', 'sine coulomb matrix eig 261', 'sine coulomb matrix eig 262', 'sine coulomb matrix eig 263', 'sine coulomb matrix eig 264', 'sine coulomb matrix eig 265', 'sine coulomb matrix eig 266', 'sine coulomb matrix eig 267', 'sine coulomb matrix eig 268', 'sine coulomb matrix eig 269', 'sine coulomb matrix eig 270', 'sine coulomb matrix eig 271', 'sine coulomb matrix eig 272', 'sine coulomb matrix eig 273', 'sine coulomb matrix eig 274', 'sine coulomb matrix eig 275', 'sine coulomb matrix eig 276', 'sine coulomb matrix eig 277', 'sine coulomb matrix eig 278', 'sine coulomb matrix eig 279', 'sine coulomb matrix eig 280', 'sine coulomb matrix eig 281', 'sine coulomb matrix eig 282', 'sine coulomb matrix eig 283', 'sine coulomb matrix eig 284', 'sine coulomb matrix eig 285', 'sine coulomb matrix eig 286', 'sine coulomb matrix eig 287'], 'learner_kwargs': {'n_estimators': 500}, 'learner_name': 'rf', 'reducer_kwargs': {'reducers': []}} Metadata: tasks recorded 13/13 complete? \u2713 composition complete? \u2713 structure complete? \u2713 regression complete? \u2713 classification complete? \u2713 Software Requirements {'python': ['scikit-learn==0.24.1', 'numpy==1.20.1', 'matbench==0.1.0', 'automatminer==v1.0.3.20191111']} Task data: matbench_dielectric Fold scores fold mae rmse mape* max_error fold_0 0.3042 0.7850 0.1176 14.5979 fold_1 0.4079 1.2316 0.1509 20.1279 fold_2 0.5220 2.9832 0.1370 59.1201 fold_3 0.3879 2.1680 0.1057 49.4924 fold_4 0.4760 2.1012 0.1886 31.0645 Fold score stats metric mean max min std mae 0.4196 0.5220 0.3042 0.0750 rmse 1.8538 2.9832 0.7850 0.7700 mape* 0.1400 0.1886 0.1057 0.0289 max_error 34.8806 59.1201 14.5979 16.9980 Fold parameters fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'} matbench_expt_gap Fold scores fold mae rmse mape* max_error fold_0 0.4360 0.7985 0.3380 5.1654 fold_1 0.4387 0.7819 0.3044 4.7122 fold_2 0.4812 0.9435 0.4019 9.5428 fold_3 0.4345 0.8059 0.3647 5.2288 fold_4 0.4400 0.7918 0.4385 5.5833 Fold score stats metric mean max min std mae 0.4461 0.4812 0.4345 0.0177 rmse 0.8243 0.9435 0.7819 0.0601 mape* 0.3695 0.4385 0.3044 0.0470 max_error 6.0465 9.5428 4.7122 1.7700 Fold parameters fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'} matbench_expt_is_metal Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.9249 0.9248 0.9236 0.9248 fold_1 0.9167 0.9166 0.9156 0.9166 fold_2 0.9096 0.9095 0.9076 0.9095 fold_3 0.9228 0.9227 0.9221 0.9227 fold_4 0.9096 0.9096 0.9104 0.9096 Fold score stats metric mean max min std accuracy 0.9167 0.9249 0.9096 0.0064 balanced_accuracy 0.9167 0.9248 0.9095 0.0064 f1 0.9159 0.9236 0.9076 0.0063 rocauc 0.9167 0.9248 0.9095 0.0064 Fold parameters fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'} matbench_glass Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.9199 0.8860 0.9449 0.8860 fold_1 0.8856 0.8402 0.9217 0.8402 fold_2 0.8847 0.8495 0.9200 0.8495 fold_3 0.8891 0.8526 0.9233 0.8526 fold_4 0.8979 0.8651 0.9292 0.8651 Fold score stats metric mean max min std accuracy 0.8954 0.9199 0.8847 0.0131 balanced_accuracy 0.8587 0.8860 0.8402 0.0158 f1 0.9278 0.9449 0.9200 0.0091 rocauc 0.8587 0.8860 0.8402 0.0158 Fold parameters fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'} matbench_jdft2d Fold scores fold mae rmse mape* max_error fold_0 42.7473 72.7391 23.7625 295.7437 fold_1 45.7510 94.3771 0.4382 581.4859 fold_2 66.2421 153.0635 0.8747 836.6225 fold_3 44.0340 81.5112 0.4818 337.7693 fold_4 51.4457 159.6390 0.6384 1538.6073 Fold score stats metric mean max min std mae 50.0440 66.2421 42.7473 8.6271 rmse 112.2660 159.6390 72.7391 36.7066 mape* 5.2391 23.7625 0.4382 9.2629 max_error 718.0457 1538.6073 295.7437 453.6473 Fold parameters fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'} matbench_log_gvrh Fold scores fold mae rmse mape* max_error fold_0 0.1046 0.1515 0.0817 1.1754 fold_1 0.1024 0.1557 0.0815 1.6942 fold_2 0.1025 0.1533 0.0798 1.0668 fold_3 0.1037 0.1495 0.0777 0.9041 fold_4 0.1067 0.1601 0.0832 0.9480 Fold score stats metric mean max min std mae 0.1040 0.1067 0.1024 0.0016 rmse 0.1540 0.1601 0.1495 0.0037 mape* 0.0808 0.0832 0.0777 0.0019 max_error 1.1577 1.6942 0.9041 0.2845 Fold parameters fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'} matbench_log_kvrh Fold scores fold mae rmse mape* max_error fold_0 0.0809 0.1415 0.0522 1.4432 fold_1 0.0808 0.1503 0.0532 1.7642 fold_2 0.0783 0.1383 0.0509 1.1189 fold_3 0.0863 0.1478 0.0608 1.1620 fold_4 0.0836 0.1489 0.0558 1.3775 Fold score stats metric mean max min std mae 0.0820 0.0863 0.0783 0.0027 rmse 0.1454 0.1503 0.1383 0.0046 mape* 0.0546 0.0608 0.0509 0.0035 max_error 1.3732 1.7642 1.1189 0.2311 Fold parameters fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'} matbench_mp_e_form Fold scores fold mae rmse mape* max_error fold_0 0.1158 0.2386 0.9331 4.2469 fold_1 0.1160 0.2459 0.5068 5.4382 fold_2 0.1179 0.2443 0.5549 4.0782 fold_3 0.1159 0.2373 0.7206 2.9374 fold_4 0.1166 0.2435 0.6836 3.8910 Fold score stats metric mean max min std mae 0.1165 0.1179 0.1158 0.0008 rmse 0.2419 0.2459 0.2373 0.0034 mape* 0.6798 0.9331 0.5068 0.1492 max_error 4.1183 5.4382 2.9374 0.8008 Fold parameters fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'} matbench_mp_gap Fold scores fold mae rmse mape* max_error fold_0 0.3456 0.6097 5.6881 6.3322 fold_1 0.3417 0.6104 4.3547 7.0601 fold_2 0.3445 0.6047 6.9303 5.9201 fold_3 0.3427 0.6101 11.9090 6.6456 fold_4 0.3512 0.6276 9.2752 6.0212 Fold score stats metric mean max min std mae 0.3452 0.3512 0.3417 0.0033 rmse 0.6125 0.6276 0.6047 0.0079 mape* 7.6315 11.9090 4.3547 2.6835 max_error 6.3958 7.0601 5.9201 0.4182 Fold parameters fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'} matbench_mp_is_metal Fold scores fold accuracy balanced_accuracy f1 rocauc fold_0 0.9080 0.9025 0.8905 0.9025 fold_1 0.9027 0.8968 0.8839 0.8968 fold_2 0.9049 0.8987 0.8862 0.8987 fold_3 0.9051 0.8994 0.8869 0.8994 fold_4 0.9047 0.8984 0.8858 0.8984 Fold score stats metric mean max min std accuracy 0.9051 0.9080 0.9027 0.0017 balanced_accuracy 0.8992 0.9025 0.8968 0.0019 f1 0.8866 0.8905 0.8839 0.0022 rocauc 0.8992 0.9025 0.8968 0.0019 Fold parameters fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'} matbench_perovskites Fold scores fold mae rmse mape* max_error fold_0 0.2357 0.3292 0.2634 2.8870 fold_1 0.2367 0.3394 0.2888 2.2083 fold_2 0.2365 0.3382 0.2631 2.5900 fold_3 0.2395 0.3369 0.2827 2.6112 fold_4 0.2291 0.3293 0.2411 2.4921 Fold score stats metric mean max min std mae 0.2355 0.2395 0.2291 0.0034 rmse 0.3346 0.3394 0.3292 0.0044 mape* 0.2678 0.2888 0.2411 0.0168 max_error 2.5577 2.8870 2.2083 0.2185 Fold parameters fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'} matbench_phonons Fold scores fold mae rmse mape* max_error fold_0 82.3863 171.1524 0.1348 1004.2770 fold_1 72.8871 172.8015 0.1172 2024.7301 fold_2 59.2712 128.7871 0.1040 1206.8703 fold_3 58.6036 122.1566 0.1167 861.9005 fold_4 64.9149 136.4846 0.1197 1255.6664 Fold score stats metric mean max min std mae 67.6126 82.3863 58.6036 8.9900 rmse 146.2764 172.8015 122.1566 21.4752 mape* 0.1185 0.1348 0.1040 0.0098 max_error 1270.6889 2024.7301 861.9005 402.7307 Fold parameters fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'} matbench_steels Fold scores fold mae rmse mape* max_error fold_0 114.6331 196.3586 0.0731 1121.1276 fold_1 85.6694 113.1549 0.0654 362.6630 fold_2 110.0055 150.1283 0.0807 448.9038 fold_3 111.5273 153.4522 0.0801 633.6092 fold_4 95.7271 133.8257 0.0730 408.6042 Fold score stats metric mean max min std mae 103.5125 114.6331 85.6694 11.0368 rmse 149.3839 196.3586 113.1549 27.4893 mape* 0.0745 0.0807 0.0654 0.0056 max_error 594.9816 1121.1276 362.6630 278.7002 Fold parameters fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'}","title":"matbench_v0.1: RF-SCM/Magpie"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#matbench_v01-rf-scmmagpie","text":"","title":"matbench_v0.1: RF-SCM/Magpie"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#algorithm-description","text":"A random forest using features from the Sine Coulomb Matrix and MagPie featurization algorthims. Sine Coulomb Matrix creates structural features based on Coulombic interactions inside a periodic boundary condition (i.e., for crystalline materials with known structure). MagPie features are weighted elemental features based on elemental data such as electronegativity, melting point, and electron affinity. Algorithms were run inside of the Automatminer v1.0.3.20191111 framework for convenience, though no auto-featurization or AutoML were run. Data cleaning dropped features with more than 1% nan samples, imputing missing samples using the mean of the training data. No feature reduction was performed. Both featurization techniques were applied to structure problems, only MagPie features were applied to problems without structure. Random forest uses 500 estimators.","title":"Algorithm description:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#notes","text":"No hyperparameter tuning was performed on the RF, as a large, constant number of trees were used in constructing each fold's model; the entire training+validation set was used as training data for the RF. Raw data download and example notebook available on the matbench repo .","title":"Notes:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#references-in-bibtex-format","text":"['@article{Dunn2020,\\n' ' doi = {10.1038/s41524-020-00406-3},\\n' ' url = {https://doi.org/10.1038/s41524-020-00406-3},\\n' ' year = {2020},\\n' ' month = sep,\\n' ' publisher = {Springer Science and Business Media {LLC}},\\n' ' volume = {6},\\n' ' number = {1},\\n' ' author = {Alexander Dunn and Qi Wang and Alex Ganose and Daniel Dopp and ' 'Anubhav Jain},\\n' ' title = {Benchmarking materials property prediction methods: the Matbench ' 'test set and Automatminer reference algorithm},\\n' ' journal = {npj Computational Materials}\\n' '}', '@article{Breiman2001,\\n' ' doi = {10.1023/a:1010933404324},\\n' ' url = {https://doi.org/10.1023/a:1010933404324},\\n' ' year = {2001},\\n' ' publisher = {Springer Science and Business Media {LLC}},\\n' ' volume = {45},\\n' ' number = {1},\\n' ' pages = {5--32},\\n' ' author = {Leo Breiman},\\n' ' journal = {Machine Learning}\\n' '}', '@article{Ward2016,\\n' ' doi = {10.1038/npjcompumats.2016.28},\\n' ' url = {https://doi.org/10.1038/npjcompumats.2016.28},\\n' ' year = {2016},\\n' ' month = aug,\\n' ' publisher = {Springer Science and Business Media {LLC}},\\n' ' volume = {2},\\n' ' number = {1},\\n' ' author = {Logan Ward and Ankit Agrawal and Alok Choudhary and Christopher ' 'Wolverton},\\n' ' title = {A general-purpose machine learning framework for predicting ' 'properties of inorganic materials},\\n' ' journal = {npj Computational Materials}\\n' '}', '@article {QUA:QUA24917,author = {Faber, Felix and Lindmaa, Alexander and von ' 'Lilienfeld, O. Anatole and Armiento, Rickard},title = {Crystal structure ' 'representations for machine learning models of formation energies},journal = ' '{International Journal of Quantum Chemistry},volume = {115},number = ' '{16},issn = {1097-461X},url = {http://dx.doi.org/10.1002/qua.24917},doi = ' '{10.1002/qua.24917},pages = {1094--1101},keywords = {machine learning, ' 'formation energies, representations, crystal structure, periodic ' 'systems},year = {2015},}']","title":"References (in bibtex format):"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#user-metadata","text":"{'__deepcopy__': {}, '__getstate__': {}, '_ipython_canary_method_should_not_exist_': {'__deepcopy__': {}, '__getstate__': {}}, 'autofeaturizer_kwargs': {'n_jobs': 10, 'preset': 'debug'}, 'best_pipeline': 'RandomForestRegressor(bootstrap=true, criterion=mse, ' 'max_depth=null,\\n' ' max_features=auto, max_leaf_nodes=null,\\n' ' min_impurity_decrease=0.0, ' 'min_impurity_split=null,\\n' ' min_samples_leaf=1, min_samples_split=2,\\n' ' min_weight_fraction_leaf=0.0, n_estimators=500, ' 'n_jobs=null,\\n' ' oob_score=false, random_state=null, verbose=0, ' 'warm_start=false)', 'cleaner_kwargs': {'feature_na_method': 'mean', 'max_na_frac': 0.01, 'na_method_fit': 'drop', 'na_method_transform': 'mean'}, 'features_all': ['MagpieData minimum Number', 'MagpieData maximum Number', 'MagpieData range Number', 'MagpieData mean Number', 'MagpieData avg_dev Number', 'MagpieData mode Number', 'MagpieData minimum MendeleevNumber', 'MagpieData maximum MendeleevNumber', 'MagpieData range MendeleevNumber', 'MagpieData mean MendeleevNumber', 'MagpieData avg_dev MendeleevNumber', 'MagpieData mode MendeleevNumber', 'MagpieData minimum AtomicWeight', 'MagpieData maximum AtomicWeight', 'MagpieData range AtomicWeight', 'MagpieData mean AtomicWeight', 'MagpieData avg_dev AtomicWeight', 'MagpieData mode AtomicWeight', 'MagpieData minimum MeltingT', 'MagpieData maximum MeltingT', 'MagpieData range MeltingT', 'MagpieData mean MeltingT', 'MagpieData avg_dev MeltingT', 'MagpieData mode MeltingT', 'MagpieData minimum Column', 'MagpieData maximum Column', 'MagpieData range Column', 'MagpieData mean Column', 'MagpieData avg_dev Column', 'MagpieData mode Column', 'MagpieData minimum Row', 'MagpieData maximum Row', 'MagpieData range Row', 'MagpieData mean Row', 'MagpieData avg_dev Row', 'MagpieData mode Row', 'MagpieData minimum CovalentRadius', 'MagpieData maximum CovalentRadius', 'MagpieData range CovalentRadius', 'MagpieData mean CovalentRadius', 'MagpieData avg_dev CovalentRadius', 'MagpieData mode CovalentRadius', 'MagpieData minimum Electronegativity', 'MagpieData maximum Electronegativity', 'MagpieData range Electronegativity', 'MagpieData mean Electronegativity', 'MagpieData avg_dev Electronegativity', 'MagpieData mode Electronegativity', 'MagpieData minimum NsValence', 'MagpieData maximum NsValence', 'MagpieData range NsValence', 'MagpieData mean NsValence', 'MagpieData avg_dev NsValence', 'MagpieData mode NsValence', 'MagpieData minimum NpValence', 'MagpieData maximum NpValence', 'MagpieData range NpValence', 'MagpieData mean NpValence', 'MagpieData avg_dev NpValence', 'MagpieData mode NpValence', 'MagpieData minimum NdValence', 'MagpieData maximum NdValence', 'MagpieData range NdValence', 'MagpieData mean NdValence', 'MagpieData avg_dev NdValence', 'MagpieData mode NdValence', 'MagpieData minimum NfValence', 'MagpieData maximum NfValence', 'MagpieData range NfValence', 'MagpieData mean NfValence', 'MagpieData avg_dev NfValence', 'MagpieData mode NfValence', 'MagpieData minimum NValence', 'MagpieData maximum NValence', 'MagpieData range NValence', 'MagpieData mean NValence', 'MagpieData avg_dev NValence', 'MagpieData mode NValence', 'MagpieData minimum NsUnfilled', 'MagpieData maximum NsUnfilled', 'MagpieData range NsUnfilled', 'MagpieData mean NsUnfilled', 'MagpieData avg_dev NsUnfilled', 'MagpieData mode NsUnfilled', 'MagpieData minimum NpUnfilled', 'MagpieData maximum NpUnfilled', 'MagpieData range NpUnfilled', 'MagpieData mean NpUnfilled', 'MagpieData avg_dev NpUnfilled', 'MagpieData mode NpUnfilled', 'MagpieData minimum NdUnfilled', 'MagpieData maximum NdUnfilled', 'MagpieData range NdUnfilled', 'MagpieData mean NdUnfilled', 'MagpieData avg_dev NdUnfilled', 'MagpieData mode NdUnfilled', 'MagpieData minimum NfUnfilled', 'MagpieData maximum NfUnfilled', 'MagpieData range NfUnfilled', 'MagpieData mean NfUnfilled', 'MagpieData avg_dev NfUnfilled', 'MagpieData mode NfUnfilled', 'MagpieData minimum NUnfilled', 'MagpieData maximum NUnfilled', 'MagpieData range NUnfilled', 'MagpieData mean NUnfilled', 'MagpieData avg_dev NUnfilled', 'MagpieData mode NUnfilled', 'MagpieData minimum GSvolume_pa', 'MagpieData maximum GSvolume_pa', 'MagpieData range GSvolume_pa', 'MagpieData mean GSvolume_pa', 'MagpieData avg_dev GSvolume_pa', 'MagpieData mode GSvolume_pa', 'MagpieData minimum GSbandgap', 'MagpieData maximum GSbandgap', 'MagpieData range GSbandgap', 'MagpieData mean GSbandgap', 'MagpieData avg_dev GSbandgap', 'MagpieData mode GSbandgap', 'MagpieData minimum GSmagmom', 'MagpieData maximum GSmagmom', 'MagpieData range GSmagmom', 'MagpieData mean GSmagmom', 'MagpieData avg_dev GSmagmom', 'MagpieData mode GSmagmom', 'MagpieData minimum SpaceGroupNumber', 'MagpieData maximum SpaceGroupNumber', 'MagpieData range SpaceGroupNumber', 'MagpieData mean SpaceGroupNumber', 'MagpieData avg_dev SpaceGroupNumber', 'MagpieData mode SpaceGroupNumber', 'sine coulomb matrix eig 0', 'sine coulomb matrix eig 1', 'sine coulomb matrix eig 2', 'sine coulomb matrix eig 3', 'sine coulomb matrix eig 4', 'sine coulomb matrix eig 5', 'sine coulomb matrix eig 6', 'sine coulomb matrix eig 7', 'sine coulomb matrix eig 8', 'sine coulomb matrix eig 9', 'sine coulomb matrix eig 10', 'sine coulomb matrix eig 11', 'sine coulomb matrix eig 12', 'sine coulomb matrix eig 13', 'sine coulomb matrix eig 14', 'sine coulomb matrix eig 15', 'sine coulomb matrix eig 16', 'sine coulomb matrix eig 17', 'sine coulomb matrix eig 18', 'sine coulomb matrix eig 19', 'sine coulomb matrix eig 20', 'sine coulomb matrix eig 21', 'sine coulomb matrix eig 22', 'sine coulomb matrix eig 23', 'sine coulomb matrix eig 24', 'sine coulomb matrix eig 25', 'sine coulomb matrix eig 26', 'sine coulomb matrix eig 27', 'sine coulomb matrix eig 28', 'sine coulomb matrix eig 29', 'sine coulomb matrix eig 30', 'sine coulomb matrix eig 31', 'sine coulomb matrix eig 32', 'sine coulomb matrix eig 33', 'sine coulomb matrix eig 34', 'sine coulomb matrix eig 35', 'sine coulomb matrix eig 36', 'sine coulomb matrix eig 37', 'sine coulomb matrix eig 38', 'sine coulomb matrix eig 39', 'sine coulomb matrix eig 40', 'sine coulomb matrix eig 41', 'sine coulomb matrix eig 42', 'sine coulomb matrix eig 43', 'sine coulomb matrix eig 44', 'sine coulomb matrix eig 45', 'sine coulomb matrix eig 46', 'sine coulomb matrix eig 47', 'sine coulomb matrix eig 48', 'sine coulomb matrix eig 49', 'sine coulomb matrix eig 50', 'sine coulomb matrix eig 51', 'sine coulomb matrix eig 52', 'sine coulomb matrix eig 53', 'sine coulomb matrix eig 54', 'sine coulomb matrix eig 55', 'sine coulomb matrix eig 56', 'sine coulomb matrix eig 57', 'sine coulomb matrix eig 58', 'sine coulomb matrix eig 59', 'sine coulomb matrix eig 60', 'sine coulomb matrix eig 61', 'sine coulomb matrix eig 62', 'sine coulomb matrix eig 63', 'sine coulomb matrix eig 64', 'sine coulomb matrix eig 65', 'sine coulomb matrix eig 66', 'sine coulomb matrix eig 67', 'sine coulomb matrix eig 68', 'sine coulomb matrix eig 69', 'sine coulomb matrix eig 70', 'sine coulomb matrix eig 71', 'sine coulomb matrix eig 72', 'sine coulomb matrix eig 73', 'sine coulomb matrix eig 74', 'sine coulomb matrix eig 75', 'sine coulomb matrix eig 76', 'sine coulomb matrix eig 77', 'sine coulomb matrix eig 78', 'sine coulomb matrix eig 79', 'sine coulomb matrix eig 80', 'sine coulomb matrix eig 81', 'sine coulomb matrix eig 82', 'sine coulomb matrix eig 83', 'sine coulomb matrix eig 84', 'sine coulomb matrix eig 85', 'sine coulomb matrix eig 86', 'sine coulomb matrix eig 87', 'sine coulomb matrix eig 88', 'sine coulomb matrix eig 89', 'sine coulomb matrix eig 90', 'sine coulomb matrix eig 91', 'sine coulomb matrix eig 92', 'sine coulomb matrix eig 93', 'sine coulomb matrix eig 94', 'sine coulomb matrix eig 95', 'sine coulomb matrix eig 96', 'sine coulomb matrix eig 97', 'sine coulomb matrix eig 98', 'sine coulomb matrix eig 99', 'sine coulomb matrix eig 100', 'sine coulomb matrix eig 101', 'sine coulomb matrix eig 102', 'sine coulomb matrix eig 103', 'sine coulomb matrix eig 104', 'sine coulomb matrix eig 105', 'sine coulomb matrix eig 106', 'sine coulomb matrix eig 107', 'sine coulomb matrix eig 108', 'sine coulomb matrix eig 109', 'sine coulomb matrix eig 110', 'sine coulomb matrix eig 111', 'sine coulomb matrix eig 112', 'sine coulomb matrix eig 113', 'sine coulomb matrix eig 114', 'sine coulomb matrix eig 115', 'sine coulomb matrix eig 116', 'sine coulomb matrix eig 117', 'sine coulomb matrix eig 118', 'sine coulomb matrix eig 119', 'sine coulomb matrix eig 120', 'sine coulomb matrix eig 121', 'sine coulomb matrix eig 122', 'sine coulomb matrix eig 123', 'sine coulomb matrix eig 124', 'sine coulomb matrix eig 125', 'sine coulomb matrix eig 126', 'sine coulomb matrix eig 127', 'sine coulomb matrix eig 128', 'sine coulomb matrix eig 129', 'sine coulomb matrix eig 130', 'sine coulomb matrix eig 131', 'sine coulomb matrix eig 132', 'sine coulomb matrix eig 133', 'sine coulomb matrix eig 134', 'sine coulomb matrix eig 135', 'sine coulomb matrix eig 136', 'sine coulomb matrix eig 137', 'sine coulomb matrix eig 138', 'sine coulomb matrix eig 139', 'sine coulomb matrix eig 140', 'sine coulomb matrix eig 141', 'sine coulomb matrix eig 142', 'sine coulomb matrix eig 143', 'sine coulomb matrix eig 144', 'sine coulomb matrix eig 145', 'sine coulomb matrix eig 146', 'sine coulomb matrix eig 147', 'sine coulomb matrix eig 148', 'sine coulomb matrix eig 149', 'sine coulomb matrix eig 150', 'sine coulomb matrix eig 151', 'sine coulomb matrix eig 152', 'sine coulomb matrix eig 153', 'sine coulomb matrix eig 154', 'sine coulomb matrix eig 155', 'sine coulomb matrix eig 156', 'sine coulomb matrix eig 157', 'sine coulomb matrix eig 158', 'sine coulomb matrix eig 159', 'sine coulomb matrix eig 160', 'sine coulomb matrix eig 161', 'sine coulomb matrix eig 162', 'sine coulomb matrix eig 163', 'sine coulomb matrix eig 164', 'sine coulomb matrix eig 165', 'sine coulomb matrix eig 166', 'sine coulomb matrix eig 167', 'sine coulomb matrix eig 168', 'sine coulomb matrix eig 169', 'sine coulomb matrix eig 170', 'sine coulomb matrix eig 171', 'sine coulomb matrix eig 172', 'sine coulomb matrix eig 173', 'sine coulomb matrix eig 174', 'sine coulomb matrix eig 175', 'sine coulomb matrix eig 176', 'sine coulomb matrix eig 177', 'sine coulomb matrix eig 178', 'sine coulomb matrix eig 179', 'sine coulomb matrix eig 180', 'sine coulomb matrix eig 181', 'sine coulomb matrix eig 182', 'sine coulomb matrix eig 183', 'sine coulomb matrix eig 184', 'sine coulomb matrix eig 185', 'sine coulomb matrix eig 186', 'sine coulomb matrix eig 187', 'sine coulomb matrix eig 188', 'sine coulomb matrix eig 189', 'sine coulomb matrix eig 190', 'sine coulomb matrix eig 191', 'sine coulomb matrix eig 192', 'sine coulomb matrix eig 193', 'sine coulomb matrix eig 194', 'sine coulomb matrix eig 195', 'sine coulomb matrix eig 196', 'sine coulomb matrix eig 197', 'sine coulomb matrix eig 198', 'sine coulomb matrix eig 199', 'sine coulomb matrix eig 200', 'sine coulomb matrix eig 201', 'sine coulomb matrix eig 202', 'sine coulomb matrix eig 203', 'sine coulomb matrix eig 204', 'sine coulomb matrix eig 205', 'sine coulomb matrix eig 206', 'sine coulomb matrix eig 207', 'sine coulomb matrix eig 208', 'sine coulomb matrix eig 209', 'sine coulomb matrix eig 210', 'sine coulomb matrix eig 211', 'sine coulomb matrix eig 212', 'sine coulomb matrix eig 213', 'sine coulomb matrix eig 214', 'sine coulomb matrix eig 215', 'sine coulomb matrix eig 216', 'sine coulomb matrix eig 217', 'sine coulomb matrix eig 218', 'sine coulomb matrix eig 219', 'sine coulomb matrix eig 220', 'sine coulomb matrix eig 221', 'sine coulomb matrix eig 222', 'sine coulomb matrix eig 223', 'sine coulomb matrix eig 224', 'sine coulomb matrix eig 225', 'sine coulomb matrix eig 226', 'sine coulomb matrix eig 227', 'sine coulomb matrix eig 228', 'sine coulomb matrix eig 229', 'sine coulomb matrix eig 230', 'sine coulomb matrix eig 231', 'sine coulomb matrix eig 232', 'sine coulomb matrix eig 233', 'sine coulomb matrix eig 234', 'sine coulomb matrix eig 235', 'sine coulomb matrix eig 236', 'sine coulomb matrix eig 237', 'sine coulomb matrix eig 238', 'sine coulomb matrix eig 239', 'sine coulomb matrix eig 240', 'sine coulomb matrix eig 241', 'sine coulomb matrix eig 242', 'sine coulomb matrix eig 243', 'sine coulomb matrix eig 244', 'sine coulomb matrix eig 245', 'sine coulomb matrix eig 246', 'sine coulomb matrix eig 247', 'sine coulomb matrix eig 248', 'sine coulomb matrix eig 249', 'sine coulomb matrix eig 250', 'sine coulomb matrix eig 251', 'sine coulomb matrix eig 252', 'sine coulomb matrix eig 253', 'sine coulomb matrix eig 254', 'sine coulomb matrix eig 255', 'sine coulomb matrix eig 256', 'sine coulomb matrix eig 257', 'sine coulomb matrix eig 258', 'sine coulomb matrix eig 259', 'sine coulomb matrix eig 260', 'sine coulomb matrix eig 261', 'sine coulomb matrix eig 262', 'sine coulomb matrix eig 263', 'sine coulomb matrix eig 264', 'sine coulomb matrix eig 265', 'sine coulomb matrix eig 266', 'sine coulomb matrix eig 267', 'sine coulomb matrix eig 268', 'sine coulomb matrix eig 269', 'sine coulomb matrix eig 270', 'sine coulomb matrix eig 271', 'sine coulomb matrix eig 272', 'sine coulomb matrix eig 273', 'sine coulomb matrix eig 274', 'sine coulomb matrix eig 275', 'sine coulomb matrix eig 276', 'sine coulomb matrix eig 277', 'sine coulomb matrix eig 278', 'sine coulomb matrix eig 279', 'sine coulomb matrix eig 280', 'sine coulomb matrix eig 281', 'sine coulomb matrix eig 282', 'sine coulomb matrix eig 283', 'sine coulomb matrix eig 284', 'sine coulomb matrix eig 285', 'sine coulomb matrix eig 286', 'sine coulomb matrix eig 287'], 'learner_kwargs': {'n_estimators': 500}, 'learner_name': 'rf', 'reducer_kwargs': {'reducers': []}}","title":"User metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#metadata","text":"tasks recorded 13/13 complete? \u2713 composition complete? \u2713 structure complete? \u2713 regression complete? \u2713 classification complete? \u2713","title":"Metadata:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#software-requirements","text":"{'python': ['scikit-learn==0.24.1', 'numpy==1.20.1', 'matbench==0.1.0', 'automatminer==v1.0.3.20191111']}","title":"Software Requirements"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#task-data","text":"","title":"Task data:"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#matbench_dielectric","text":"","title":"matbench_dielectric"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-scores","text":"fold mae rmse mape* max_error fold_0 0.3042 0.7850 0.1176 14.5979 fold_1 0.4079 1.2316 0.1509 20.1279 fold_2 0.5220 2.9832 0.1370 59.1201 fold_3 0.3879 2.1680 0.1057 49.4924 fold_4 0.4760 2.1012 0.1886 31.0645","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-score-stats","text":"metric mean max min std mae 0.4196 0.5220 0.3042 0.0750 rmse 1.8538 2.9832 0.7850 0.7700 mape* 0.1400 0.1886 0.1057 0.0289 max_error 34.8806 59.1201 14.5979 16.9980","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-parameters","text":"fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#matbench_expt_gap","text":"","title":"matbench_expt_gap"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-scores_1","text":"fold mae rmse mape* max_error fold_0 0.4360 0.7985 0.3380 5.1654 fold_1 0.4387 0.7819 0.3044 4.7122 fold_2 0.4812 0.9435 0.4019 9.5428 fold_3 0.4345 0.8059 0.3647 5.2288 fold_4 0.4400 0.7918 0.4385 5.5833","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-score-stats_1","text":"metric mean max min std mae 0.4461 0.4812 0.4345 0.0177 rmse 0.8243 0.9435 0.7819 0.0601 mape* 0.3695 0.4385 0.3044 0.0470 max_error 6.0465 9.5428 4.7122 1.7700","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-parameters_1","text":"fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#matbench_expt_is_metal","text":"","title":"matbench_expt_is_metal"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-scores_2","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.9249 0.9248 0.9236 0.9248 fold_1 0.9167 0.9166 0.9156 0.9166 fold_2 0.9096 0.9095 0.9076 0.9095 fold_3 0.9228 0.9227 0.9221 0.9227 fold_4 0.9096 0.9096 0.9104 0.9096","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-score-stats_2","text":"metric mean max min std accuracy 0.9167 0.9249 0.9096 0.0064 balanced_accuracy 0.9167 0.9248 0.9095 0.0064 f1 0.9159 0.9236 0.9076 0.0063 rocauc 0.9167 0.9248 0.9095 0.0064","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-parameters_2","text":"fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#matbench_glass","text":"","title":"matbench_glass"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-scores_3","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.9199 0.8860 0.9449 0.8860 fold_1 0.8856 0.8402 0.9217 0.8402 fold_2 0.8847 0.8495 0.9200 0.8495 fold_3 0.8891 0.8526 0.9233 0.8526 fold_4 0.8979 0.8651 0.9292 0.8651","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-score-stats_3","text":"metric mean max min std accuracy 0.8954 0.9199 0.8847 0.0131 balanced_accuracy 0.8587 0.8860 0.8402 0.0158 f1 0.9278 0.9449 0.9200 0.0091 rocauc 0.8587 0.8860 0.8402 0.0158","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-parameters_3","text":"fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#matbench_jdft2d","text":"","title":"matbench_jdft2d"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-scores_4","text":"fold mae rmse mape* max_error fold_0 42.7473 72.7391 23.7625 295.7437 fold_1 45.7510 94.3771 0.4382 581.4859 fold_2 66.2421 153.0635 0.8747 836.6225 fold_3 44.0340 81.5112 0.4818 337.7693 fold_4 51.4457 159.6390 0.6384 1538.6073","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-score-stats_4","text":"metric mean max min std mae 50.0440 66.2421 42.7473 8.6271 rmse 112.2660 159.6390 72.7391 36.7066 mape* 5.2391 23.7625 0.4382 9.2629 max_error 718.0457 1538.6073 295.7437 453.6473","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-parameters_4","text":"fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#matbench_log_gvrh","text":"","title":"matbench_log_gvrh"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-scores_5","text":"fold mae rmse mape* max_error fold_0 0.1046 0.1515 0.0817 1.1754 fold_1 0.1024 0.1557 0.0815 1.6942 fold_2 0.1025 0.1533 0.0798 1.0668 fold_3 0.1037 0.1495 0.0777 0.9041 fold_4 0.1067 0.1601 0.0832 0.9480","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-score-stats_5","text":"metric mean max min std mae 0.1040 0.1067 0.1024 0.0016 rmse 0.1540 0.1601 0.1495 0.0037 mape* 0.0808 0.0832 0.0777 0.0019 max_error 1.1577 1.6942 0.9041 0.2845","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-parameters_5","text":"fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#matbench_log_kvrh","text":"","title":"matbench_log_kvrh"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-scores_6","text":"fold mae rmse mape* max_error fold_0 0.0809 0.1415 0.0522 1.4432 fold_1 0.0808 0.1503 0.0532 1.7642 fold_2 0.0783 0.1383 0.0509 1.1189 fold_3 0.0863 0.1478 0.0608 1.1620 fold_4 0.0836 0.1489 0.0558 1.3775","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-score-stats_6","text":"metric mean max min std mae 0.0820 0.0863 0.0783 0.0027 rmse 0.1454 0.1503 0.1383 0.0046 mape* 0.0546 0.0608 0.0509 0.0035 max_error 1.3732 1.7642 1.1189 0.2311","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-parameters_6","text":"fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#matbench_mp_e_form","text":"","title":"matbench_mp_e_form"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-scores_7","text":"fold mae rmse mape* max_error fold_0 0.1158 0.2386 0.9331 4.2469 fold_1 0.1160 0.2459 0.5068 5.4382 fold_2 0.1179 0.2443 0.5549 4.0782 fold_3 0.1159 0.2373 0.7206 2.9374 fold_4 0.1166 0.2435 0.6836 3.8910","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-score-stats_7","text":"metric mean max min std mae 0.1165 0.1179 0.1158 0.0008 rmse 0.2419 0.2459 0.2373 0.0034 mape* 0.6798 0.9331 0.5068 0.1492 max_error 4.1183 5.4382 2.9374 0.8008","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-parameters_7","text":"fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#matbench_mp_gap","text":"","title":"matbench_mp_gap"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-scores_8","text":"fold mae rmse mape* max_error fold_0 0.3456 0.6097 5.6881 6.3322 fold_1 0.3417 0.6104 4.3547 7.0601 fold_2 0.3445 0.6047 6.9303 5.9201 fold_3 0.3427 0.6101 11.9090 6.6456 fold_4 0.3512 0.6276 9.2752 6.0212","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-score-stats_8","text":"metric mean max min std mae 0.3452 0.3512 0.3417 0.0033 rmse 0.6125 0.6276 0.6047 0.0079 mape* 7.6315 11.9090 4.3547 2.6835 max_error 6.3958 7.0601 5.9201 0.4182","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-parameters_8","text":"fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#matbench_mp_is_metal","text":"","title":"matbench_mp_is_metal"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-scores_9","text":"fold accuracy balanced_accuracy f1 rocauc fold_0 0.9080 0.9025 0.8905 0.9025 fold_1 0.9027 0.8968 0.8839 0.8968 fold_2 0.9049 0.8987 0.8862 0.8987 fold_3 0.9051 0.8994 0.8869 0.8994 fold_4 0.9047 0.8984 0.8858 0.8984","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-score-stats_9","text":"metric mean max min std accuracy 0.9051 0.9080 0.9027 0.0017 balanced_accuracy 0.8992 0.9025 0.8968 0.0019 f1 0.8866 0.8905 0.8839 0.0022 rocauc 0.8992 0.9025 0.8968 0.0019","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-parameters_9","text":"fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#matbench_perovskites","text":"","title":"matbench_perovskites"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-scores_10","text":"fold mae rmse mape* max_error fold_0 0.2357 0.3292 0.2634 2.8870 fold_1 0.2367 0.3394 0.2888 2.2083 fold_2 0.2365 0.3382 0.2631 2.5900 fold_3 0.2395 0.3369 0.2827 2.6112 fold_4 0.2291 0.3293 0.2411 2.4921","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-score-stats_10","text":"metric mean max min std mae 0.2355 0.2395 0.2291 0.0034 rmse 0.3346 0.3394 0.3292 0.0044 mape* 0.2678 0.2888 0.2411 0.0168 max_error 2.5577 2.8870 2.2083 0.2185","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-parameters_10","text":"fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#matbench_phonons","text":"","title":"matbench_phonons"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-scores_11","text":"fold mae rmse mape* max_error fold_0 82.3863 171.1524 0.1348 1004.2770 fold_1 72.8871 172.8015 0.1172 2024.7301 fold_2 59.2712 128.7871 0.1040 1206.8703 fold_3 58.6036 122.1566 0.1167 861.9005 fold_4 64.9149 136.4846 0.1197 1255.6664","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-score-stats_11","text":"metric mean max min std mae 67.6126 82.3863 58.6036 8.9900 rmse 146.2764 172.8015 122.1566 21.4752 mape* 0.1185 0.1348 0.1040 0.0098 max_error 1270.6889 2024.7301 861.9005 402.7307","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-parameters_11","text":"fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'}","title":"Fold parameters"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#matbench_steels","text":"","title":"matbench_steels"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-scores_12","text":"fold mae rmse mape* max_error fold_0 114.6331 196.3586 0.0731 1121.1276 fold_1 85.6694 113.1549 0.0654 362.6630 fold_2 110.0055 150.1283 0.0807 448.9038 fold_3 111.5273 153.4522 0.0801 633.6092 fold_4 95.7271 133.8257 0.0730 408.6042","title":"Fold scores"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-score-stats_12","text":"metric mean max min std mae 103.5125 114.6331 85.6694 11.0368 rmse 149.3839 196.3586 113.1549 27.4893 mape* 0.0745 0.0807 0.0654 0.0056 max_error 594.9816 1121.1276 362.6630 278.7002","title":"Fold score stats"},{"location":"Full%20Benchmark%20Data/matbench_v0.1_rf/#fold-parameters_12","text":"fold params dict fold_0 {'note': 'single config; see benchmark user metadata'} fold_1 {'note': 'single config; see benchmark user metadata'} fold_2 {'note': 'single config; see benchmark user metadata'} fold_3 {'note': 'single config; see benchmark user metadata'} fold_4 {'note': 'single config; see benchmark user metadata'}","title":"Fold parameters"},{"location":"How%20To%20Use/1install/","text":"1 - Install matbench Via GitHub Clone the repository with: git clone https://github.com/hackingmaterials/matbench To install for the current user: pip install --user ./matbench For development, install with cd matbench pip install -e . -r requirements.txt Via PyPi Install matbench with: pip install matbench Troubleshooting Matbench is supported on all Unix systems. It does not officially support Windows installations, but may work. Please use our friendly matsci.org help forum to ask for help; we don't bite!","title":"1 - Install matbench"},{"location":"How%20To%20Use/1install/#1-install-matbench","text":"","title":"1 - Install matbench"},{"location":"How%20To%20Use/1install/#via-github","text":"Clone the repository with: git clone https://github.com/hackingmaterials/matbench To install for the current user: pip install --user ./matbench For development, install with cd matbench pip install -e . -r requirements.txt","title":"Via GitHub"},{"location":"How%20To%20Use/1install/#via-pypi","text":"Install matbench with: pip install matbench","title":"Via PyPi"},{"location":"How%20To%20Use/1install/#troubleshooting","text":"Matbench is supported on all Unix systems. It does not officially support Windows installations, but may work. Please use our friendly matsci.org help forum to ask for help; we don't bite!","title":"Troubleshooting"},{"location":"How%20To%20Use/2run/","text":"2 - Run benchmark on algorithm Recording your data You can use the matbench python package to retrieve the training and testing splits as well as record new predictions. Recording and saving your data with matbench should take no more than 10 lines of matbench code. The only things you need are: Your algorithm/model - we'll call it my_model in this example The MatbenchBenchmark class. Here's an example of running an entire benchmark (13 tasks) using matbench. from matbench.bench import MatbenchBenchmark mb = MatbenchBenchmark ( autoload = False ) for task in mb . tasks : task . load () for fold in task . folds : # Inputs are either chemical compositions as strings # or crystal structures as pymatgen.Structure objects. # Outputs are either floats (regression tasks) or bools (classification tasks) train_inputs , train_outputs = task . get_train_and_val_data ( fold ) # train and validate your model my_model . train_and_validate ( train_inputs , train_outputs ) # Get testing data test_inputs = task . get_test_data ( fold , include_target = False ) # Predict on the testing data # Your output should be a pandas series, numpy array, or python iterable # where the array elements are floats or bools predictions = my_model . predict ( test_inputs ) # Record your data! task . record ( fold , predictions ) # Save your results mb . to_file ( \"my_models_benchmark.json.gz\" ) And you're done! Your benchmark has been recorded and saved. The output file, in this case my_models_benchmark.json.gz contains everything predicted by your benchmark. Keep this file, as it is the core result that will be submitted to the leaderboard. Please see the docs for Submitting to leaderboard to learn how to upload your data to the automated leaderboard. Note: Benchmark subsets If you want to benchmark on a subset of Matbench tasks, set the subset argument when creating MatbenchBenchmark and use the same code as above. The repo accepts subsets of matbench tasks as well which will appear on a separate \"task-specific\" leaderboard. Recording hyperparameters and user metadata Hyperparameters for each fold Record parameters ( dict type) for each fold using the parameters argument to MatbenchBenchmark.record : from matbench.bench import MatbenchBenchmark mb = MatbenchBenchmark ( autoload = False ) for task in mb . tasks : task . load () for fold in task . folds : train_inputs , train_outputs = task . get_train_and_val_data ( fold ) my_model . train_and_validate ( train_inputs , train_outputs ) test_inputs = task . get_test_data ( fold , include_target = False ) predictions = my_model . predict ( test_inputs ) # Get your model's parameters # Parameters must be a dictionary of python native types, e.g., lists of strings, dicts, etc. params = my_model . get_parameters_as_dictionary () task . record ( fold , predictions , params = params ) We recommend you record the hyperparameters on each fold - but it is optional. Your parameters can be freeform, though we encourage brevity - only recording the most important parameters, not large arrays or weight matrices. User metadata for benchmark Add arbitrary metadata about your algorthm, in dict format, to the benchmark. This will be included as shown on the benchmark leaderboard on the website. my_metadata = { \"algorithm_version\" : \"v1\" , \"tree_type\" : \"entropy\" , \"configuration\" : { \"some_param\" : 4 , \"other_vector\" : [ 1 , 2 , 3 ] } } mb . add_metadata ( my_metadata )","title":"2 - Run benchmark on algorithm"},{"location":"How%20To%20Use/2run/#2-run-benchmark-on-algorithm","text":"","title":"2 - Run benchmark on algorithm"},{"location":"How%20To%20Use/2run/#recording-your-data","text":"You can use the matbench python package to retrieve the training and testing splits as well as record new predictions. Recording and saving your data with matbench should take no more than 10 lines of matbench code. The only things you need are: Your algorithm/model - we'll call it my_model in this example The MatbenchBenchmark class. Here's an example of running an entire benchmark (13 tasks) using matbench. from matbench.bench import MatbenchBenchmark mb = MatbenchBenchmark ( autoload = False ) for task in mb . tasks : task . load () for fold in task . folds : # Inputs are either chemical compositions as strings # or crystal structures as pymatgen.Structure objects. # Outputs are either floats (regression tasks) or bools (classification tasks) train_inputs , train_outputs = task . get_train_and_val_data ( fold ) # train and validate your model my_model . train_and_validate ( train_inputs , train_outputs ) # Get testing data test_inputs = task . get_test_data ( fold , include_target = False ) # Predict on the testing data # Your output should be a pandas series, numpy array, or python iterable # where the array elements are floats or bools predictions = my_model . predict ( test_inputs ) # Record your data! task . record ( fold , predictions ) # Save your results mb . to_file ( \"my_models_benchmark.json.gz\" ) And you're done! Your benchmark has been recorded and saved. The output file, in this case my_models_benchmark.json.gz contains everything predicted by your benchmark. Keep this file, as it is the core result that will be submitted to the leaderboard. Please see the docs for Submitting to leaderboard to learn how to upload your data to the automated leaderboard.","title":"Recording your data"},{"location":"How%20To%20Use/2run/#note-benchmark-subsets","text":"If you want to benchmark on a subset of Matbench tasks, set the subset argument when creating MatbenchBenchmark and use the same code as above. The repo accepts subsets of matbench tasks as well which will appear on a separate \"task-specific\" leaderboard.","title":"Note: Benchmark subsets"},{"location":"How%20To%20Use/2run/#recording-hyperparameters-and-user-metadata","text":"","title":"Recording hyperparameters and user metadata"},{"location":"How%20To%20Use/2run/#hyperparameters-for-each-fold","text":"Record parameters ( dict type) for each fold using the parameters argument to MatbenchBenchmark.record : from matbench.bench import MatbenchBenchmark mb = MatbenchBenchmark ( autoload = False ) for task in mb . tasks : task . load () for fold in task . folds : train_inputs , train_outputs = task . get_train_and_val_data ( fold ) my_model . train_and_validate ( train_inputs , train_outputs ) test_inputs = task . get_test_data ( fold , include_target = False ) predictions = my_model . predict ( test_inputs ) # Get your model's parameters # Parameters must be a dictionary of python native types, e.g., lists of strings, dicts, etc. params = my_model . get_parameters_as_dictionary () task . record ( fold , predictions , params = params ) We recommend you record the hyperparameters on each fold - but it is optional. Your parameters can be freeform, though we encourage brevity - only recording the most important parameters, not large arrays or weight matrices.","title":"Hyperparameters for each fold"},{"location":"How%20To%20Use/2run/#user-metadata-for-benchmark","text":"Add arbitrary metadata about your algorthm, in dict format, to the benchmark. This will be included as shown on the benchmark leaderboard on the website. my_metadata = { \"algorithm_version\" : \"v1\" , \"tree_type\" : \"entropy\" , \"configuration\" : { \"some_param\" : 4 , \"other_vector\" : [ 1 , 2 , 3 ] } } mb . add_metadata ( my_metadata )","title":"User metadata for benchmark"},{"location":"How%20To%20Use/3submit/","text":"3 - Submit to leaderboard Step 1: Create 3 required files To submit to the leaderboard, you need 3 files: results.json.gz : The file you saved when recording your data . Instructions on how to create this file info.json : A short file of some metadata about your algorithm. Instructions on how to create this file, with template Either (a) an .ipynb notebook detailing your algorithm with code for running it, or (b) one or more .py files with source code for running/benchmarking your algorithm. Instructions here Step 2: Put files in appropriate folder a. If you are using matbench through pypi, clone the source repository in order to make a pull request. Find instrucions for cloning the source repository on the Installation page . b. Locate the matbench/benchmarks directory. c. Create a new directory <benchmark name>_<algorithm name> according to your algorithm and the benchmark you ran (e.g., matbench_v0.1_my_algorithm_namev2 ). d. Put the required files from Step 1 into this directory. The files should look like: \u251c\u2500\u2500 benchmarks \u2502 \u2514\u2500\u2500 matbench_v0.1_<your algorithm name> \u2502 \u251c\u2500\u2500 info.json \u2502 \u251c\u2500\u2500 my_python_file.py \u2502 \u2514\u2500\u2500 results.json.gz Warning: the info.json and results.json.gz must have these names exactly for your PR to go through without problems, automatically. You can include any other small files (no naming scheme required) for running your code in this directory. Step 3: Create a PR to the Matbench repository Commit your new changes to the repo with, and create a pull request (PR) to the Matbench repository. Find instructions for creating a PR here. Label your PR with the \"new_benchmark\" label. And you're done! If the tests pass, your submission will be added to the leaderboard! results.json.gz This file is the MatbenchBenchmark you saved during your benchmark. You can find docs about how to record and save a benchmark on the Running a benchmark page . This file is required for a submission. info.json A metadata file about your algorithm, the authors, and any relevant citations. Please ensure the following keys are included, as they are required by our automated leaderboard: \"authors\" : The author names for this PR \"algorithm\" : The short or abbreviated name for your algorithm, e.g., \"MegNET v1.0\" . Should be 5-15 characters. \"algorithm_long\" : A longer description of your algorithm, to be shown as details for your results. Can be up to 1000 words. \"bibtex_refs\" : A comprehensive list of references for your algorithm, including manuscripts and preprints for the algorithm itself, each formatted as bibtex. \"notes\" : Any other freeform notes you'd like to include as details for your algorithm/submission. Can include things like computing resources used to train/run the algorithm, methodology, alternative configurations, links, etc. \"requirements\" : A dictionary of software requirements for running your algorithm. In particular, installing these should ensure your Source files run without issues. Please include the matbench version in these requirments. Here's an template info.json you can copy+paste and edit to get started: { \"authors\": \"My name\", \"algorithm\": \"COOLNet v14\", \"algorithm_long\": \"A longer description of my super cool algorithm, COOLNet v14.\", \"bibtex_refs\": \"@article{Dunn2020,\\n doi = {10.1038/s41524-020-00406-3},\\n url = {https://doi.org/10.1038/s41524-020-00406-3},\\n year = {2020},\\n month = sep,\\n publisher = {Springer Science and Business Media {LLC}},\\n volume = {6},\\n number = {1},\\n author = {Alexander Dunn and Qi Wang and Alex Ganose and Daniel Dopp and Anubhav Jain},\\n title = {Benchmarking materials property prediction methods: the Matbench test set and Automatminer reference algorithm},\\n journal = {npj Computational Materials}\\n}\", \"notes\": \"Some freeform notes users might be interested in, if they were to run your source code files.\", \"requirements\": {\"python\": [\"scikit-learn==0.24.1\", \"numpy==1.20.1\", \"matbench==0.1.0\"]} } Source files At least one source file (one or more .py files or a Jupyter notebook .ipynb ) must be included with submission. This is to help others run and understand your code and results. General guidelines The source file should contain all the code needed for configuring, training, and running your algorithm on all the benchmark tasks you decide to run. The easiest way to create a source file is just use the source file you used while recording your benchmark results . There are no naming requirements for these source files. You can also include other supporting files, like metadata, features, etc. if they are critical for the algorithm to run and they are small (<10MB). Please include the matbench code (e.g., mb.record(...) ) for obtaining benchmarks/recording/examining results in the source files. Jupyter notebooks The preferred format for source files is a jupyter notebook with some code for running your algorithm on Matbench. You can find an example The notebook should generally follow the format of the example notebook /benchmarks/matbench_v0.1_dummy/notebook.ipynb . Try to include a long form, human readable description of how your algorithm works, any package versions needed to have it run correctly, and most importantly, a link to a publication for your algorithm . Aside from that, what goes in your notebook is pretty freeform; put whatever is needed to allow someone else to train and run your algorithm on the benchmark . You can find an example template for a notebook in the matbench repo under /benchmarks/matbench_v0.1_dummy/notebook.ipynb .py files If you use .py files as source in the submission, please comment your code as much as you can to help others run it!","title":"3 - Submit to leaderboard"},{"location":"How%20To%20Use/3submit/#3-submit-to-leaderboard","text":"","title":"3 - Submit to leaderboard"},{"location":"How%20To%20Use/3submit/#step-1-create-3-required-files","text":"To submit to the leaderboard, you need 3 files: results.json.gz : The file you saved when recording your data . Instructions on how to create this file info.json : A short file of some metadata about your algorithm. Instructions on how to create this file, with template Either (a) an .ipynb notebook detailing your algorithm with code for running it, or (b) one or more .py files with source code for running/benchmarking your algorithm. Instructions here","title":"Step 1: Create 3 required files"},{"location":"How%20To%20Use/3submit/#step-2-put-files-in-appropriate-folder","text":"a. If you are using matbench through pypi, clone the source repository in order to make a pull request. Find instrucions for cloning the source repository on the Installation page . b. Locate the matbench/benchmarks directory. c. Create a new directory <benchmark name>_<algorithm name> according to your algorithm and the benchmark you ran (e.g., matbench_v0.1_my_algorithm_namev2 ). d. Put the required files from Step 1 into this directory. The files should look like: \u251c\u2500\u2500 benchmarks \u2502 \u2514\u2500\u2500 matbench_v0.1_<your algorithm name> \u2502 \u251c\u2500\u2500 info.json \u2502 \u251c\u2500\u2500 my_python_file.py \u2502 \u2514\u2500\u2500 results.json.gz Warning: the info.json and results.json.gz must have these names exactly for your PR to go through without problems, automatically. You can include any other small files (no naming scheme required) for running your code in this directory.","title":"Step 2: Put files in appropriate folder"},{"location":"How%20To%20Use/3submit/#step-3-create-a-pr-to-the-matbench-repository","text":"Commit your new changes to the repo with, and create a pull request (PR) to the Matbench repository. Find instructions for creating a PR here. Label your PR with the \"new_benchmark\" label. And you're done! If the tests pass, your submission will be added to the leaderboard!","title":"Step 3: Create a PR to the Matbench repository"},{"location":"How%20To%20Use/3submit/#resultsjsongz","text":"This file is the MatbenchBenchmark you saved during your benchmark. You can find docs about how to record and save a benchmark on the Running a benchmark page . This file is required for a submission.","title":"results.json.gz"},{"location":"How%20To%20Use/3submit/#infojson","text":"A metadata file about your algorithm, the authors, and any relevant citations. Please ensure the following keys are included, as they are required by our automated leaderboard: \"authors\" : The author names for this PR \"algorithm\" : The short or abbreviated name for your algorithm, e.g., \"MegNET v1.0\" . Should be 5-15 characters. \"algorithm_long\" : A longer description of your algorithm, to be shown as details for your results. Can be up to 1000 words. \"bibtex_refs\" : A comprehensive list of references for your algorithm, including manuscripts and preprints for the algorithm itself, each formatted as bibtex. \"notes\" : Any other freeform notes you'd like to include as details for your algorithm/submission. Can include things like computing resources used to train/run the algorithm, methodology, alternative configurations, links, etc. \"requirements\" : A dictionary of software requirements for running your algorithm. In particular, installing these should ensure your Source files run without issues. Please include the matbench version in these requirments. Here's an template info.json you can copy+paste and edit to get started: { \"authors\": \"My name\", \"algorithm\": \"COOLNet v14\", \"algorithm_long\": \"A longer description of my super cool algorithm, COOLNet v14.\", \"bibtex_refs\": \"@article{Dunn2020,\\n doi = {10.1038/s41524-020-00406-3},\\n url = {https://doi.org/10.1038/s41524-020-00406-3},\\n year = {2020},\\n month = sep,\\n publisher = {Springer Science and Business Media {LLC}},\\n volume = {6},\\n number = {1},\\n author = {Alexander Dunn and Qi Wang and Alex Ganose and Daniel Dopp and Anubhav Jain},\\n title = {Benchmarking materials property prediction methods: the Matbench test set and Automatminer reference algorithm},\\n journal = {npj Computational Materials}\\n}\", \"notes\": \"Some freeform notes users might be interested in, if they were to run your source code files.\", \"requirements\": {\"python\": [\"scikit-learn==0.24.1\", \"numpy==1.20.1\", \"matbench==0.1.0\"]} }","title":"info.json"},{"location":"How%20To%20Use/3submit/#source-files","text":"At least one source file (one or more .py files or a Jupyter notebook .ipynb ) must be included with submission. This is to help others run and understand your code and results.","title":"Source files"},{"location":"How%20To%20Use/3submit/#general-guidelines","text":"The source file should contain all the code needed for configuring, training, and running your algorithm on all the benchmark tasks you decide to run. The easiest way to create a source file is just use the source file you used while recording your benchmark results . There are no naming requirements for these source files. You can also include other supporting files, like metadata, features, etc. if they are critical for the algorithm to run and they are small (<10MB). Please include the matbench code (e.g., mb.record(...) ) for obtaining benchmarks/recording/examining results in the source files.","title":"General guidelines"},{"location":"How%20To%20Use/3submit/#jupyter-notebooks","text":"The preferred format for source files is a jupyter notebook with some code for running your algorithm on Matbench. You can find an example The notebook should generally follow the format of the example notebook /benchmarks/matbench_v0.1_dummy/notebook.ipynb . Try to include a long form, human readable description of how your algorithm works, any package versions needed to have it run correctly, and most importantly, a link to a publication for your algorithm . Aside from that, what goes in your notebook is pretty freeform; put whatever is needed to allow someone else to train and run your algorithm on the benchmark . You can find an example template for a notebook in the matbench repo under /benchmarks/matbench_v0.1_dummy/notebook.ipynb","title":"Jupyter notebooks"},{"location":"How%20To%20Use/3submit/#py-files","text":"If you use .py files as source in the submission, please comment your code as much as you can to help others run it!","title":".py files"},{"location":"How%20To%20Use/advanced/","text":"Advanced usage Once you have recorded some data, you can examine it with the MatbenchBenchmark object. If you are looking to record data, see the Recording data page . Pretty much everything in Matbench - including scoring, saving, loading, recording, inspecting, and more - can be done thru MatbenchBenchmark directly. Loading and saving Load a completed, valid benchmark from disk: mb = MatbenchBechmark . from_file ( \"path/to/my_results.json.gz\" ) >>> mb < MatbenchBenchmark > Save a completed, valid benchmark to disk mb . to_file ( \"path/to/my_results.json.gz\" ) Task data Tasks ( MatbenchTask ) are accessible as MatbenchBenchmark attributes through their names. Let's say we are interested in matbench_dielectric . # Access task thru attribute task = mb . matbench_dielectric # This task is a MatbenchTask object >>> print ( task ) < MatbenchTask > See task metadata See metadata for an individual task. metadata = mb . matbench_dielectric . metadata >>> metadata { 'input_type' : 'structure' , 'mad' : 0.808534704217072 , 'n_samples' : 4764 , 'target' : 'n' , 'task_type' : 'regression' , 'unit' : 'unitless' , 'bibtex_refs' : [ \"@Article{Dunn2020, \\n author={Dunn, Alexander \\n and Wang, Qi \\n and Ganose, Alex \\n and Dopp, Daniel \\n and Jain, Anubhav}, \\n title={Benchmarking materials property prediction methods: the Matbench test set and Automatminer reference algorithm}, \\n journal={npj Computational Materials}, \\n year= {2020} , \\n month= {Sep} , \\n day= {15} , \\n volume= {6} , \\n number= {1} , \\n pages= {138} , \\n abstract={We present a benchmark test suite and an automated machine learning procedure for evaluating supervised machine learning (ML) models for predicting properties of inorganic bulk materials. The test suite, Matbench, is a set of 13{ \\\\ thinspace}ML tasks that range in size from 312 to 132k samples and contain data from 10 density functional theory-derived and experimental sources. Tasks include predicting optical, thermal, electronic, thermodynamic, tensile, and elastic properties given a material's composition and/or crystal structure. The reference algorithm, Automatminer, is a highly-extensible, fully automated ML pipeline for predicting materials properties from materials primitives (such as composition and crystal structure) without user intervention or hyperparameter tuning. We test Automatminer on the Matbench test suite and compare its predictive power with state-of-the-art crystal graph neural networks and a traditional descriptor-based Random Forest model. We find Automatminer achieves the best performance on 8 of 13 tasks in the benchmark. We also show our test suite is capable of exposing predictive advantages of each algorithm---namely, that crystal graph methods appear to outperform traditional machine learning methods given { \\\\ textasciitilde}104 or greater data points. We encourage evaluating materials ML algorithms on the Matbench benchmark and comparing them against the latest version of Automatminer.}, \\n issn={2057-3960}, \\n doi={10.1038/s41524-020-00406-3}, \\n url={https://doi.org/10.1038/s41524-020-00406-3} \\n } \\n \" , '@article{Jain2013, \\n author = {Jain, Anubhav and Ong, Shyue Ping and Hautier, Geoffroy and Chen, Wei and Richards, William Davidson and Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and Skinner, David and Ceder, Gerbrand and Persson, Kristin a.}, \\n doi = {10.1063/1.4812323}, \\n issn = {2166532X} , \\n journal = {APL Materials}, \\n number = {1} , \\n pages = {011002} , \\n title = {{The Materials Project: A materials genome approach to accelerating materials innovation}}, \\n url = {http://link.aip.org/link/AMPADS/v1/i1/p011002/s1 \\\\ &Agg=doi}, \\n volume = {1} , \\n year = {2013} \\n }' , '@article{Petousis2017, \\n author={Petousis, Ioannis and Mrdjenovich, David and Ballouz, Eric \\n and Liu, Miao and Winston, Donald and Chen, Wei and Graf, Tanja \\n and Schladt, Thomas D. and Persson, Kristin A. and Prinz, Fritz B.}, \\n title={High-throughput screening of inorganic compounds for the \\n discovery of novel dielectric and optical materials}, \\n journal={Scientific Data}, \\n year= {2017} , \\n month= {Jan} , \\n day= {31} , \\n publisher={The Author(s)}, \\n volume= {4} , \\n pages= {160134} , \\n note={Data Descriptor}, \\n url={http://dx.doi.org/10.1038/sdata.2016.134} \\n }' ], 'columns' : { 'n' : 'Target variable. Refractive index (unitless).' , 'structure' : 'Pymatgen Structure of the material.' }, 'description' : 'Matbench v0.1 test dataset for predicting refractive index from structure. Adapted from Materials Project database. Removed entries having a formation energy (or energy above the convex hull) more than 150meV and those having refractive indices less than 1 and those containing noble gases. Retrieved April 2, 2019. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details.' , 'file_type' : 'json.gz' , 'num_entries' : 4764 , 'url' : 'https://ml.materialsproject.org/projects/matbench_dielectric.json.gz' , 'hash' : '83befa09bc2ec2f4b6143afc413157827a90e5e2e42c1eb507ccfa01bf26a1d6' , 'reference' : 'Petousis, I., Mrdjenovich, D., Ballouz, E., Liu, M., Winston, D., \\n Chen, W., Graf, T., Schladt, T. D., Persson, K. A. & Prinz, F. B. \\n High-throughput screening of inorganic compounds for the discovery \\n of novel dielectric and optical materials. Sci. Data 4, 160134 (2017).' , } # Metadata is also accessible as attributes >>> metadata . unit \"unitless\" See which folds of this task are recorded recorded_folds = mb . matbench_dielectric . is_recorded # In this example, we only have folds 0 and 1 recorded. >>> recorded_folds { 0 : True , 1 : True , 2 : False , 3 : False , 4 : False } See task score stats among folds All folds must be recorded to see score stats. scores = mb . matbench_dielectric . scores # Show score stats taken over all folds >>> scores { 'mae' : { 'mean' : 0.31502894856879793 , 'max' : 0.42569840085084304 , 'min' : 0.21883030230732342 , 'std' : 0.0672172232063864 }, 'rmse' : { 'mean' : 1.7202043807691947 , 'max' : 2.9472145483123082 , 'min' : 0.6855155532720747 , 'std' : 0.8140297551209411 }, 'mape' : { 'mean' : 0.08510552426501797 , 'max' : 0.09872854141937873 , 'min' : 0.07201546203802894 , 'std' : 0.009760258167856002 }, 'max_error' : { 'mean' : 34.996903717427166 , 'max' : 59.01119325894446 , 'min' : 14.665353016975205 , 'std' : 17.978224948280573 } } # scores are also accessible as attrs >>> scores . mae . max 0.42569840085084304 See outputs, parameters, and scores for individual task folds # Get all of our recorded results results = mb . matbench_dielectric . results >>> results { 'fold_0' : { 'data' : { 'mb-dielectric-0008' : 2.1816278769942685 , 'mb-dielectric-0010' : 2.1449892069940995 , 'mb-dielectric-0019' : 3.9022885489716175 , 'mb-dielectric-0025' : 4.105947591302149 , ... }, 'parameters' : { 'best_pipeline' : '[\"(selectfwe, SelectFwe(alpha=0.006, score_func=<function f_regression at 0x2aaaef1a0840>))...\"' ... }, 'scores' : { 'mae' : 0.21883030230732342 , 'mape' : 0.07602888421332273 , 'max_error' : 14.665353016975205 , 'rmse' : 0.6855155532720747 } }, 'fold_1' : { ... }, ... } # Individual fold data are available thru attrs >>> results . fold_4 . data [ 'mb-dielectric-4751' ] 2.5696947646331614 # Including ML parameters for a specific fold, if made available >>> results . fold_4 . parameters { 'best_pipeline' : [ '(selectfwe, SelectFwe(alpha=0.034, score_func=<function f_regression at 0x2aaaf35a08c8>))' , '(zerocount, ZeroCount())' , '(gradientboostingregressor, GradientBoostingRegressor(alpha=0.85, criterion=friedman_mse, init=null, \\n learning_rate=0.1, loss=huber, max_depth=9, \\n max_features=0.7500000000000001, max_leaf_nodes=null, \\n min_impurity_decrease=0.0, min_impurity_split=null, \\n min_samples_leaf=13, min_samples_split=17, \\n min_weight_fraction_leaf=0.0, n_estimators=100, \\n n_iter_no_change=null, presort=auto, \\n random_state=null, subsample=0.7500000000000001, \\n tol=0.0001, validation_fraction=0.1, verbose=0, \\n warm_start=false))' ], 'features_reduced' : [ 'MagpieData maximum Number' , 'MagpieData maximum MendeleevNumber' , 'MagpieData mean MendeleevNumber' , 'MagpieData avg_dev MendeleevNumber' , 'MagpieData range AtomicWeight' , ... } # Get score metrics on fold 4 >>> results . fold_4 . scores { 'mae' : 0.3264316502622554 , 'mape' : 0.09872854141937873 , 'max_error' : 28.160118784575193 , 'rmse' : 1.6137009708660595 } Validate an individual task's results >>> mb . matbench_dielectric . validate () # If does not throw an error, it's valid! Benchmark data A MatbenchBenchmark is a collection of tasks. Once your benchmark is recorded, you can inspect it. Get information about the state of a benchmark >>> mb . get_info () \"\"\" Matbench package 0.1.0 running benchmark 'matbench_v0.1' is complete: True is recorded: True is valid: True Results: - 'matbench_dielectric' MAE mean: 29.09435441521901 - 'matbench_expt_gap' MAE mean: 5.097990146029299 - 'matbench_expt_is_metal' ROCAUC mean: 0.490515739562644 - 'matbench_glass' ROCAUC mean: 0.4915206231191361 - 'matbench_mp_e_form' MAE mean: 1.9798749618345852 - 'matbench_jdft2d' MAE mean: 624.8594821594436 - 'matbench_log_gvrh' MAE mean: 0.7503117195807093 - 'matbench_log_kvrh' MAE mean: 0.8337265925158915 - 'matbench_mp_gap' MAE mean: 3.9947345263133185 - 'matbench_mp_is_metal' ROCAUC mean: 0.4995330363104962 - 'matbench_perovskites' MAE mean: 1.6494389339807394 - 'matbench_phonons' MAE mean: 1442.1910745917485 - 'matbench_steels' MAE mean: 514.6879431114869 \"\"\" Access a summary of score data, across all tasks Access score data for multiple metrics, including fold statistics, programmatically >>> mb . scores { 'matbench_dielectric' : { 'mae' : { 'mean' : 29.09435441521901 , 'max' : 29.790913986352297 , 'min' : 26.50764023789047 , 'std' : 1.2938287761791334 }, 'rmse' : { 'mean' : 33.654269974352744 , 'max' : 34.44945162692406 , 'min' : 30.707221665034698 , 'std' : 1.4740060199828717 }, 'mape' : { 'mean' : 14.169387576348942 , 'max' : 14.56764274096521 , 'min' : 12.928095832225917 , 'std' : 0.6228030143476618 }, 'max_error' : { 'mean' : 58.85621300050616 , 'max' : 60.1966146990726 , 'min' : 53.98208657241693 , 'std' : 2.4395502402545453 }}, 'matbench_expt_gap' : { 'mae' : { 'mean' : 5.097990146029299 , 'max' : 5.290261095781455 , 'min' : 4.6298670001648965 , 'std' : 0.2397514292575463 }, 'rmse' : { 'mean' : 6.006638705150991 , 'max' : 6.226508032402611 , 'min' : 5.47028276176484 , 'std' : 0.27274122238814 }, 'mape' : { 'mean' : 1.38641021305497e+16 , 'max' : 1.5276180519639252e+16 , 'min' : 1.2259552001352658e+16 , 'std' : 986247659935790.8 }, 'max_error' : { 'mean' : 11.407347551284193 , 'max' : 11.688512264782567 , 'min' : 10.489690494035637 , 'std' : 0.45961704429199657 }}, 'matbench_expt_is_metal' : { 'accuracy' : { 'mean' : 0.4903474887540754 , 'max' : 0.5050813008130082 , 'min' : 0.47459349593495936 , 'std' : 0.013195738662206162 }, 'balanced_accuracy' : { 'mean' : 0.490515739562644 , 'max' : 0.5052590266875981 , 'min' : 0.4747707180038007 , 'std' : 0.013195964150335589 }, 'f1' : { 'mean' : 0.5107296153663292 , 'max' : 0.5248780487804879 , 'min' : 0.49560975609756097 , 'std' : 0.012667909247509207 }, 'rocauc' : { 'mean' : 0.490515739562644 , 'max' : 0.5052590266875981 , 'min' : 0.4747707180038007 , 'std' : 0.013195964150335589 }}, 'matbench_glass' : { 'accuracy' : { 'mean' : 0.5059859154929578 , 'max' : 0.528169014084507 , 'min' : 0.477112676056338 , 'std' : 0.018718357549298598 }, 'balanced_accuracy' : { 'mean' : 0.4915206231191361 , 'max' : 0.518476250739163 , 'min' : 0.4564355205025932 , 'std' : 0.022745473256365906 }, 'f1' : { 'mean' : 0.6019858156028368 , 'max' : 0.6198581560283688 , 'min' : 0.5787234042553191 , 'std' : 0.015080889486527119 }, 'rocauc' : { 'mean' : 0.4915206231191361 , 'max' : 0.518476250739163 , 'min' : 0.4564355205025932 , 'std' : 0.022745473256365906 }}, 'matbench_mp_e_form' : { 'mae' : { 'mean' : 1.9798749618345852 , 'max' : 1.9820103943808465 , 'min' : 1.9764313221160588 , 'std' : 0.0018588951040352502 }, 'rmse' : { 'mean' : 2.376419875235826 , 'max' : 2.3794812432136196 , 'min' : 2.3722602233100063 , 'std' : 0.0023430849418330816 }, 'mape' : { 'mean' : 6989111302031.963 , 'max' : 7492035787402.213 , 'min' : 6236081301418.79 , 'std' : 476980899991.28485 }, 'max_error' : { 'mean' : 6.9650087167699155 , 'max' : 7.057955130739103 , 'min' : 6.878168095265195 , 'std' : 0.06657839974500762 }}, 'matbench_jdft2d' : { 'mae' : { 'mean' : 624.8594821594436 , 'max' : 662.8351790033564 , 'min' : 484.0870035426516 , 'std' : 70.41763851884579 }, 'rmse' : { 'mean' : 754.6594168930902 , 'max' : 802.851398577492 , 'min' : 575.4212296101125 , 'std' : 89.65203353138263 }, 'mape' : { 'mean' : 12.691214729498025 , 'max' : 22.18652735053058 , 'min' : 9.642403294653164 , 'std' : 4.833743597331997 }, 'max_error' : { 'mean' : 1455.537803743586 , 'max' : 1532.911339763068 , 'min' : 1229.7021907932801 , 'std' : 113.62938957056699 }}, 'matbench_log_gvrh' : { 'mae' : { 'mean' : 0.7503117195807093 , 'max' : 0.7567499426463542 , 'min' : 0.7458321525860483 , 'std' : 0.004177000349054263 }, 'rmse' : { 'mean' : 0.8922201073043177 , 'max' : 0.8965161788869266 , 'min' : 0.8860255812982848 , 'std' : 0.0034322474625259137 }, 'mape' : { 'mean' : 15059325260426.266 , 'max' : 26158506009539.293 , 'min' : 4541885118479.488 , 'std' : 6978350942510.934 }, 'max_error' : { 'mean' : 2.4294014472589063 , 'max' : 2.7078341735946374 , 'min' : 2.2460171713812693 , 'std' : 0.17276767393879686 }}, 'matbench_log_kvrh' : { 'mae' : { 'mean' : 0.8337265925158915 , 'max' : 0.84093059152486 , 'min' : 0.8252194857104939 , 'std' : 0.005960109281798535 }, 'rmse' : { 'mean' : 1.0122909056359641 , 'max' : 1.0190702248693488 , 'min' : 1.0038726599443502 , 'std' : 0.005051164726815858 }, 'mape' : { 'mean' : 5205086458416.939 , 'max' : 10062434398435.773 , 'min' : 1547244178802.9026 , 'std' : 3081512230166.448 }, 'max_error' : { 'mean' : 2.5243586576102204 , 'max' : 2.7538971602513187 , 'min' : 2.4510034654636557 , 'std' : 0.11592723389441413 }}, 'matbench_mp_gap' : { 'mae' : { 'mean' : 3.9947345263133185 , 'max' : 4.040261917311839 , 'min' : 3.8419572019120563 , 'std' : 0.07657166015944829 }, 'rmse' : { 'mean' : 4.802562096614456 , 'max' : 4.852934760261583 , 'min' : 4.621350867969822 , 'std' : 0.09070340779972745 }, 'mape' : { 'mean' : 9376100521414580.0 , 'max' : 9530024251770120.0 , 'min' : 9017068673849420.0 , 'std' : 191246529837308.34 }, 'max_error' : { 'mean' : 9.641818242181197 , 'max' : 9.721159283071396 , 'min' : 9.326360936678295 , 'std' : 0.15772886643823172 }}, 'matbench_mp_is_metal' : { 'accuracy' : { 'mean' : 0.49927909555806177 , 'max' : 0.5032513429459994 , 'min' : 0.49498185930358574 , 'std' : 0.002961735738880825 }, 'balanced_accuracy' : { 'mean' : 0.4995330363104962 , 'max' : 0.5035756141508568 , 'min' : 0.4951605437227332 , 'std' : 0.003013293507682773 }, 'f1' : { 'mean' : 0.465575654865608 , 'max' : 0.46982498491249247 , 'min' : 0.46097364715349026 , 'std' : 0.0031704147980028555 }, 'rocauc' : { 'mean' : 0.4995330363104962 , 'max' : 0.5035756141508567 , 'min' : 0.4951605437227331 , 'std' : 0.0030132935076827893 }}, 'matbench_perovskites' : { 'mae' : { 'mean' : 1.6494389339807394 , 'max' : 1.6643604327414814 , 'min' : 1.6083671212370563 , 'std' : 0.02130042981456539 }, 'rmse' : { 'mean' : 1.9895605050492304 , 'max' : 2.0097384860674103 , 'min' : 1.9348806762983708 , 'std' : 0.028069501175258544 }, 'mape' : { 'mean' : 8474366075980.172 , 'max' : 17109693350693.695 , 'min' : 170695202621.18396 , 'std' : 5913986606286.262 }, 'max_error' : { 'mean' : 5.122830203832267 , 'max' : 5.401364835279832 , 'min' : 4.933113748862263 , 'std' : 0.15432057817183506 }}, 'matbench_phonons' : { 'mae' : { 'mean' : 1442.1910745917485 , 'max' : 1460.5342302638428 , 'min' : 1404.6727173726108 , 'std' : 19.87835062913105 }, 'rmse' : { 'mean' : 1739.1638204522908 , 'max' : 1748.4453111626615 , 'min' : 1714.4001958506544 , 'std' : 12.506318415067186 }, 'mape' : { 'mean' : 4.535426963268569 , 'max' : 4.692503460859966 , 'min' : 4.356729242935622 , 'std' : 0.11577855407899913 }, 'max_error' : { 'mean' : 3387.1756802926197 , 'max' : 3490.7322416780676 , 'min' : 3312.8239446861567 , 'std' : 60.586867518772216 }}, 'matbench_steels' : { 'mae' : { 'mean' : 514.6879431114869 , 'max' : 548.5353510044772 , 'min' : 488.97286237333986 , 'std' : 24.98451122832146 }, 'rmse' : { 'mean' : 619.9832706475461 , 'max' : 651.1520235084482 , 'min' : 591.9607445092288 , 'std' : 24.183510586935057 }, 'mape' : { 'mean' : 0.39220921441643364 , 'max' : 0.4201053232886023 , 'min' : 0.368378839458224 , 'std' : 0.02076964611162295 }, 'max_error' : { 'mean' : 1331.6729147023618 , 'max' : 1389.1259692340998 , 'min' : 1272.982373277621 , 'std' : 40.71026078669035 }}} Validate an entire benchmark You can validate an entire benchmark with the validate method of MatbenchBenchmark . >>> mb . is_valid True If your results are valid, it ensures the automated leaderboard can understand your data and that all folds for all tasks are recorded. See if a benchmark is complete A benchmark is complete if it contains all the tasks specified in the benchmark specification. In the case of the benchmark Matbench v0.1, this means all 13 tasks are present in your benchmark (though they may not be recorded yet!). >>> mb . is_complete True","title":"Advanced usage"},{"location":"How%20To%20Use/advanced/#advanced-usage","text":"Once you have recorded some data, you can examine it with the MatbenchBenchmark object. If you are looking to record data, see the Recording data page . Pretty much everything in Matbench - including scoring, saving, loading, recording, inspecting, and more - can be done thru MatbenchBenchmark directly.","title":"Advanced usage"},{"location":"How%20To%20Use/advanced/#loading-and-saving","text":"","title":"Loading and saving"},{"location":"How%20To%20Use/advanced/#load-a-completed-valid-benchmark-from-disk","text":"mb = MatbenchBechmark . from_file ( \"path/to/my_results.json.gz\" ) >>> mb < MatbenchBenchmark >","title":"Load a completed, valid benchmark from disk:"},{"location":"How%20To%20Use/advanced/#save-a-completed-valid-benchmark-to-disk","text":"mb . to_file ( \"path/to/my_results.json.gz\" )","title":"Save a completed, valid benchmark to disk"},{"location":"How%20To%20Use/advanced/#task-data","text":"Tasks ( MatbenchTask ) are accessible as MatbenchBenchmark attributes through their names. Let's say we are interested in matbench_dielectric . # Access task thru attribute task = mb . matbench_dielectric # This task is a MatbenchTask object >>> print ( task ) < MatbenchTask >","title":"Task data"},{"location":"How%20To%20Use/advanced/#see-task-metadata","text":"See metadata for an individual task. metadata = mb . matbench_dielectric . metadata >>> metadata { 'input_type' : 'structure' , 'mad' : 0.808534704217072 , 'n_samples' : 4764 , 'target' : 'n' , 'task_type' : 'regression' , 'unit' : 'unitless' , 'bibtex_refs' : [ \"@Article{Dunn2020, \\n author={Dunn, Alexander \\n and Wang, Qi \\n and Ganose, Alex \\n and Dopp, Daniel \\n and Jain, Anubhav}, \\n title={Benchmarking materials property prediction methods: the Matbench test set and Automatminer reference algorithm}, \\n journal={npj Computational Materials}, \\n year= {2020} , \\n month= {Sep} , \\n day= {15} , \\n volume= {6} , \\n number= {1} , \\n pages= {138} , \\n abstract={We present a benchmark test suite and an automated machine learning procedure for evaluating supervised machine learning (ML) models for predicting properties of inorganic bulk materials. The test suite, Matbench, is a set of 13{ \\\\ thinspace}ML tasks that range in size from 312 to 132k samples and contain data from 10 density functional theory-derived and experimental sources. Tasks include predicting optical, thermal, electronic, thermodynamic, tensile, and elastic properties given a material's composition and/or crystal structure. The reference algorithm, Automatminer, is a highly-extensible, fully automated ML pipeline for predicting materials properties from materials primitives (such as composition and crystal structure) without user intervention or hyperparameter tuning. We test Automatminer on the Matbench test suite and compare its predictive power with state-of-the-art crystal graph neural networks and a traditional descriptor-based Random Forest model. We find Automatminer achieves the best performance on 8 of 13 tasks in the benchmark. We also show our test suite is capable of exposing predictive advantages of each algorithm---namely, that crystal graph methods appear to outperform traditional machine learning methods given { \\\\ textasciitilde}104 or greater data points. We encourage evaluating materials ML algorithms on the Matbench benchmark and comparing them against the latest version of Automatminer.}, \\n issn={2057-3960}, \\n doi={10.1038/s41524-020-00406-3}, \\n url={https://doi.org/10.1038/s41524-020-00406-3} \\n } \\n \" , '@article{Jain2013, \\n author = {Jain, Anubhav and Ong, Shyue Ping and Hautier, Geoffroy and Chen, Wei and Richards, William Davidson and Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and Skinner, David and Ceder, Gerbrand and Persson, Kristin a.}, \\n doi = {10.1063/1.4812323}, \\n issn = {2166532X} , \\n journal = {APL Materials}, \\n number = {1} , \\n pages = {011002} , \\n title = {{The Materials Project: A materials genome approach to accelerating materials innovation}}, \\n url = {http://link.aip.org/link/AMPADS/v1/i1/p011002/s1 \\\\ &Agg=doi}, \\n volume = {1} , \\n year = {2013} \\n }' , '@article{Petousis2017, \\n author={Petousis, Ioannis and Mrdjenovich, David and Ballouz, Eric \\n and Liu, Miao and Winston, Donald and Chen, Wei and Graf, Tanja \\n and Schladt, Thomas D. and Persson, Kristin A. and Prinz, Fritz B.}, \\n title={High-throughput screening of inorganic compounds for the \\n discovery of novel dielectric and optical materials}, \\n journal={Scientific Data}, \\n year= {2017} , \\n month= {Jan} , \\n day= {31} , \\n publisher={The Author(s)}, \\n volume= {4} , \\n pages= {160134} , \\n note={Data Descriptor}, \\n url={http://dx.doi.org/10.1038/sdata.2016.134} \\n }' ], 'columns' : { 'n' : 'Target variable. Refractive index (unitless).' , 'structure' : 'Pymatgen Structure of the material.' }, 'description' : 'Matbench v0.1 test dataset for predicting refractive index from structure. Adapted from Materials Project database. Removed entries having a formation energy (or energy above the convex hull) more than 150meV and those having refractive indices less than 1 and those containing noble gases. Retrieved April 2, 2019. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details.' , 'file_type' : 'json.gz' , 'num_entries' : 4764 , 'url' : 'https://ml.materialsproject.org/projects/matbench_dielectric.json.gz' , 'hash' : '83befa09bc2ec2f4b6143afc413157827a90e5e2e42c1eb507ccfa01bf26a1d6' , 'reference' : 'Petousis, I., Mrdjenovich, D., Ballouz, E., Liu, M., Winston, D., \\n Chen, W., Graf, T., Schladt, T. D., Persson, K. A. & Prinz, F. B. \\n High-throughput screening of inorganic compounds for the discovery \\n of novel dielectric and optical materials. Sci. Data 4, 160134 (2017).' , } # Metadata is also accessible as attributes >>> metadata . unit \"unitless\"","title":"See task metadata"},{"location":"How%20To%20Use/advanced/#see-which-folds-of-this-task-are-recorded","text":"recorded_folds = mb . matbench_dielectric . is_recorded # In this example, we only have folds 0 and 1 recorded. >>> recorded_folds { 0 : True , 1 : True , 2 : False , 3 : False , 4 : False }","title":"See which folds of this task are recorded"},{"location":"How%20To%20Use/advanced/#see-task-score-stats-among-folds","text":"All folds must be recorded to see score stats. scores = mb . matbench_dielectric . scores # Show score stats taken over all folds >>> scores { 'mae' : { 'mean' : 0.31502894856879793 , 'max' : 0.42569840085084304 , 'min' : 0.21883030230732342 , 'std' : 0.0672172232063864 }, 'rmse' : { 'mean' : 1.7202043807691947 , 'max' : 2.9472145483123082 , 'min' : 0.6855155532720747 , 'std' : 0.8140297551209411 }, 'mape' : { 'mean' : 0.08510552426501797 , 'max' : 0.09872854141937873 , 'min' : 0.07201546203802894 , 'std' : 0.009760258167856002 }, 'max_error' : { 'mean' : 34.996903717427166 , 'max' : 59.01119325894446 , 'min' : 14.665353016975205 , 'std' : 17.978224948280573 } } # scores are also accessible as attrs >>> scores . mae . max 0.42569840085084304","title":"See task score stats among folds"},{"location":"How%20To%20Use/advanced/#see-outputs-parameters-and-scores-for-individual-task-folds","text":"# Get all of our recorded results results = mb . matbench_dielectric . results >>> results { 'fold_0' : { 'data' : { 'mb-dielectric-0008' : 2.1816278769942685 , 'mb-dielectric-0010' : 2.1449892069940995 , 'mb-dielectric-0019' : 3.9022885489716175 , 'mb-dielectric-0025' : 4.105947591302149 , ... }, 'parameters' : { 'best_pipeline' : '[\"(selectfwe, SelectFwe(alpha=0.006, score_func=<function f_regression at 0x2aaaef1a0840>))...\"' ... }, 'scores' : { 'mae' : 0.21883030230732342 , 'mape' : 0.07602888421332273 , 'max_error' : 14.665353016975205 , 'rmse' : 0.6855155532720747 } }, 'fold_1' : { ... }, ... } # Individual fold data are available thru attrs >>> results . fold_4 . data [ 'mb-dielectric-4751' ] 2.5696947646331614 # Including ML parameters for a specific fold, if made available >>> results . fold_4 . parameters { 'best_pipeline' : [ '(selectfwe, SelectFwe(alpha=0.034, score_func=<function f_regression at 0x2aaaf35a08c8>))' , '(zerocount, ZeroCount())' , '(gradientboostingregressor, GradientBoostingRegressor(alpha=0.85, criterion=friedman_mse, init=null, \\n learning_rate=0.1, loss=huber, max_depth=9, \\n max_features=0.7500000000000001, max_leaf_nodes=null, \\n min_impurity_decrease=0.0, min_impurity_split=null, \\n min_samples_leaf=13, min_samples_split=17, \\n min_weight_fraction_leaf=0.0, n_estimators=100, \\n n_iter_no_change=null, presort=auto, \\n random_state=null, subsample=0.7500000000000001, \\n tol=0.0001, validation_fraction=0.1, verbose=0, \\n warm_start=false))' ], 'features_reduced' : [ 'MagpieData maximum Number' , 'MagpieData maximum MendeleevNumber' , 'MagpieData mean MendeleevNumber' , 'MagpieData avg_dev MendeleevNumber' , 'MagpieData range AtomicWeight' , ... } # Get score metrics on fold 4 >>> results . fold_4 . scores { 'mae' : 0.3264316502622554 , 'mape' : 0.09872854141937873 , 'max_error' : 28.160118784575193 , 'rmse' : 1.6137009708660595 }","title":"See outputs, parameters, and scores for individual task folds"},{"location":"How%20To%20Use/advanced/#validate-an-individual-tasks-results","text":">>> mb . matbench_dielectric . validate () # If does not throw an error, it's valid!","title":"Validate an individual task's results"},{"location":"How%20To%20Use/advanced/#benchmark-data","text":"A MatbenchBenchmark is a collection of tasks. Once your benchmark is recorded, you can inspect it.","title":"Benchmark data"},{"location":"How%20To%20Use/advanced/#get-information-about-the-state-of-a-benchmark","text":">>> mb . get_info () \"\"\" Matbench package 0.1.0 running benchmark 'matbench_v0.1' is complete: True is recorded: True is valid: True Results: - 'matbench_dielectric' MAE mean: 29.09435441521901 - 'matbench_expt_gap' MAE mean: 5.097990146029299 - 'matbench_expt_is_metal' ROCAUC mean: 0.490515739562644 - 'matbench_glass' ROCAUC mean: 0.4915206231191361 - 'matbench_mp_e_form' MAE mean: 1.9798749618345852 - 'matbench_jdft2d' MAE mean: 624.8594821594436 - 'matbench_log_gvrh' MAE mean: 0.7503117195807093 - 'matbench_log_kvrh' MAE mean: 0.8337265925158915 - 'matbench_mp_gap' MAE mean: 3.9947345263133185 - 'matbench_mp_is_metal' ROCAUC mean: 0.4995330363104962 - 'matbench_perovskites' MAE mean: 1.6494389339807394 - 'matbench_phonons' MAE mean: 1442.1910745917485 - 'matbench_steels' MAE mean: 514.6879431114869 \"\"\"","title":"Get information about the state of a benchmark"},{"location":"How%20To%20Use/advanced/#access-a-summary-of-score-data-across-all-tasks","text":"Access score data for multiple metrics, including fold statistics, programmatically >>> mb . scores { 'matbench_dielectric' : { 'mae' : { 'mean' : 29.09435441521901 , 'max' : 29.790913986352297 , 'min' : 26.50764023789047 , 'std' : 1.2938287761791334 }, 'rmse' : { 'mean' : 33.654269974352744 , 'max' : 34.44945162692406 , 'min' : 30.707221665034698 , 'std' : 1.4740060199828717 }, 'mape' : { 'mean' : 14.169387576348942 , 'max' : 14.56764274096521 , 'min' : 12.928095832225917 , 'std' : 0.6228030143476618 }, 'max_error' : { 'mean' : 58.85621300050616 , 'max' : 60.1966146990726 , 'min' : 53.98208657241693 , 'std' : 2.4395502402545453 }}, 'matbench_expt_gap' : { 'mae' : { 'mean' : 5.097990146029299 , 'max' : 5.290261095781455 , 'min' : 4.6298670001648965 , 'std' : 0.2397514292575463 }, 'rmse' : { 'mean' : 6.006638705150991 , 'max' : 6.226508032402611 , 'min' : 5.47028276176484 , 'std' : 0.27274122238814 }, 'mape' : { 'mean' : 1.38641021305497e+16 , 'max' : 1.5276180519639252e+16 , 'min' : 1.2259552001352658e+16 , 'std' : 986247659935790.8 }, 'max_error' : { 'mean' : 11.407347551284193 , 'max' : 11.688512264782567 , 'min' : 10.489690494035637 , 'std' : 0.45961704429199657 }}, 'matbench_expt_is_metal' : { 'accuracy' : { 'mean' : 0.4903474887540754 , 'max' : 0.5050813008130082 , 'min' : 0.47459349593495936 , 'std' : 0.013195738662206162 }, 'balanced_accuracy' : { 'mean' : 0.490515739562644 , 'max' : 0.5052590266875981 , 'min' : 0.4747707180038007 , 'std' : 0.013195964150335589 }, 'f1' : { 'mean' : 0.5107296153663292 , 'max' : 0.5248780487804879 , 'min' : 0.49560975609756097 , 'std' : 0.012667909247509207 }, 'rocauc' : { 'mean' : 0.490515739562644 , 'max' : 0.5052590266875981 , 'min' : 0.4747707180038007 , 'std' : 0.013195964150335589 }}, 'matbench_glass' : { 'accuracy' : { 'mean' : 0.5059859154929578 , 'max' : 0.528169014084507 , 'min' : 0.477112676056338 , 'std' : 0.018718357549298598 }, 'balanced_accuracy' : { 'mean' : 0.4915206231191361 , 'max' : 0.518476250739163 , 'min' : 0.4564355205025932 , 'std' : 0.022745473256365906 }, 'f1' : { 'mean' : 0.6019858156028368 , 'max' : 0.6198581560283688 , 'min' : 0.5787234042553191 , 'std' : 0.015080889486527119 }, 'rocauc' : { 'mean' : 0.4915206231191361 , 'max' : 0.518476250739163 , 'min' : 0.4564355205025932 , 'std' : 0.022745473256365906 }}, 'matbench_mp_e_form' : { 'mae' : { 'mean' : 1.9798749618345852 , 'max' : 1.9820103943808465 , 'min' : 1.9764313221160588 , 'std' : 0.0018588951040352502 }, 'rmse' : { 'mean' : 2.376419875235826 , 'max' : 2.3794812432136196 , 'min' : 2.3722602233100063 , 'std' : 0.0023430849418330816 }, 'mape' : { 'mean' : 6989111302031.963 , 'max' : 7492035787402.213 , 'min' : 6236081301418.79 , 'std' : 476980899991.28485 }, 'max_error' : { 'mean' : 6.9650087167699155 , 'max' : 7.057955130739103 , 'min' : 6.878168095265195 , 'std' : 0.06657839974500762 }}, 'matbench_jdft2d' : { 'mae' : { 'mean' : 624.8594821594436 , 'max' : 662.8351790033564 , 'min' : 484.0870035426516 , 'std' : 70.41763851884579 }, 'rmse' : { 'mean' : 754.6594168930902 , 'max' : 802.851398577492 , 'min' : 575.4212296101125 , 'std' : 89.65203353138263 }, 'mape' : { 'mean' : 12.691214729498025 , 'max' : 22.18652735053058 , 'min' : 9.642403294653164 , 'std' : 4.833743597331997 }, 'max_error' : { 'mean' : 1455.537803743586 , 'max' : 1532.911339763068 , 'min' : 1229.7021907932801 , 'std' : 113.62938957056699 }}, 'matbench_log_gvrh' : { 'mae' : { 'mean' : 0.7503117195807093 , 'max' : 0.7567499426463542 , 'min' : 0.7458321525860483 , 'std' : 0.004177000349054263 }, 'rmse' : { 'mean' : 0.8922201073043177 , 'max' : 0.8965161788869266 , 'min' : 0.8860255812982848 , 'std' : 0.0034322474625259137 }, 'mape' : { 'mean' : 15059325260426.266 , 'max' : 26158506009539.293 , 'min' : 4541885118479.488 , 'std' : 6978350942510.934 }, 'max_error' : { 'mean' : 2.4294014472589063 , 'max' : 2.7078341735946374 , 'min' : 2.2460171713812693 , 'std' : 0.17276767393879686 }}, 'matbench_log_kvrh' : { 'mae' : { 'mean' : 0.8337265925158915 , 'max' : 0.84093059152486 , 'min' : 0.8252194857104939 , 'std' : 0.005960109281798535 }, 'rmse' : { 'mean' : 1.0122909056359641 , 'max' : 1.0190702248693488 , 'min' : 1.0038726599443502 , 'std' : 0.005051164726815858 }, 'mape' : { 'mean' : 5205086458416.939 , 'max' : 10062434398435.773 , 'min' : 1547244178802.9026 , 'std' : 3081512230166.448 }, 'max_error' : { 'mean' : 2.5243586576102204 , 'max' : 2.7538971602513187 , 'min' : 2.4510034654636557 , 'std' : 0.11592723389441413 }}, 'matbench_mp_gap' : { 'mae' : { 'mean' : 3.9947345263133185 , 'max' : 4.040261917311839 , 'min' : 3.8419572019120563 , 'std' : 0.07657166015944829 }, 'rmse' : { 'mean' : 4.802562096614456 , 'max' : 4.852934760261583 , 'min' : 4.621350867969822 , 'std' : 0.09070340779972745 }, 'mape' : { 'mean' : 9376100521414580.0 , 'max' : 9530024251770120.0 , 'min' : 9017068673849420.0 , 'std' : 191246529837308.34 }, 'max_error' : { 'mean' : 9.641818242181197 , 'max' : 9.721159283071396 , 'min' : 9.326360936678295 , 'std' : 0.15772886643823172 }}, 'matbench_mp_is_metal' : { 'accuracy' : { 'mean' : 0.49927909555806177 , 'max' : 0.5032513429459994 , 'min' : 0.49498185930358574 , 'std' : 0.002961735738880825 }, 'balanced_accuracy' : { 'mean' : 0.4995330363104962 , 'max' : 0.5035756141508568 , 'min' : 0.4951605437227332 , 'std' : 0.003013293507682773 }, 'f1' : { 'mean' : 0.465575654865608 , 'max' : 0.46982498491249247 , 'min' : 0.46097364715349026 , 'std' : 0.0031704147980028555 }, 'rocauc' : { 'mean' : 0.4995330363104962 , 'max' : 0.5035756141508567 , 'min' : 0.4951605437227331 , 'std' : 0.0030132935076827893 }}, 'matbench_perovskites' : { 'mae' : { 'mean' : 1.6494389339807394 , 'max' : 1.6643604327414814 , 'min' : 1.6083671212370563 , 'std' : 0.02130042981456539 }, 'rmse' : { 'mean' : 1.9895605050492304 , 'max' : 2.0097384860674103 , 'min' : 1.9348806762983708 , 'std' : 0.028069501175258544 }, 'mape' : { 'mean' : 8474366075980.172 , 'max' : 17109693350693.695 , 'min' : 170695202621.18396 , 'std' : 5913986606286.262 }, 'max_error' : { 'mean' : 5.122830203832267 , 'max' : 5.401364835279832 , 'min' : 4.933113748862263 , 'std' : 0.15432057817183506 }}, 'matbench_phonons' : { 'mae' : { 'mean' : 1442.1910745917485 , 'max' : 1460.5342302638428 , 'min' : 1404.6727173726108 , 'std' : 19.87835062913105 }, 'rmse' : { 'mean' : 1739.1638204522908 , 'max' : 1748.4453111626615 , 'min' : 1714.4001958506544 , 'std' : 12.506318415067186 }, 'mape' : { 'mean' : 4.535426963268569 , 'max' : 4.692503460859966 , 'min' : 4.356729242935622 , 'std' : 0.11577855407899913 }, 'max_error' : { 'mean' : 3387.1756802926197 , 'max' : 3490.7322416780676 , 'min' : 3312.8239446861567 , 'std' : 60.586867518772216 }}, 'matbench_steels' : { 'mae' : { 'mean' : 514.6879431114869 , 'max' : 548.5353510044772 , 'min' : 488.97286237333986 , 'std' : 24.98451122832146 }, 'rmse' : { 'mean' : 619.9832706475461 , 'max' : 651.1520235084482 , 'min' : 591.9607445092288 , 'std' : 24.183510586935057 }, 'mape' : { 'mean' : 0.39220921441643364 , 'max' : 0.4201053232886023 , 'min' : 0.368378839458224 , 'std' : 0.02076964611162295 }, 'max_error' : { 'mean' : 1331.6729147023618 , 'max' : 1389.1259692340998 , 'min' : 1272.982373277621 , 'std' : 40.71026078669035 }}}","title":"Access a summary of score data, across all tasks"},{"location":"How%20To%20Use/advanced/#validate-an-entire-benchmark","text":"You can validate an entire benchmark with the validate method of MatbenchBenchmark . >>> mb . is_valid True If your results are valid, it ensures the automated leaderboard can understand your data and that all folds for all tasks are recorded.","title":"Validate an entire benchmark"},{"location":"How%20To%20Use/advanced/#see-if-a-benchmark-is-complete","text":"A benchmark is complete if it contains all the tasks specified in the benchmark specification. In the case of the benchmark Matbench v0.1, this means all 13 tasks are present in your benchmark (though they may not be recorded yet!). >>> mb . is_complete True","title":"See if a benchmark is complete"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_dielectric/","text":"matbench_v0.1 matbench_dielectric Individual Task Leaderboard for matbench_dielectric Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark. Leaderboard algorithm mean mae std mae mean rmse max max_error fictitious_compound 0.1981 0.0723 1.6165 58.7285 fictitious_model 0.2798 0.0726 1.6898 59.0262 MODNet (v0.1.10) 0.2970 0.0720 1.7185 58.9519 AMMExpress v2020 0.3150 0.0672 1.7202 59.0112 CrabNet 0.3234 0.0714 1.7288 59.1583 ALIGNN 0.3449 0.0871 1.9651 58.7285 RF-SCM/Magpie 0.4196 0.0750 1.8538 59.1201 CGCNN v2019 0.5988 0.0833 1.8976 58.9996 Dummy 0.8088 0.0718 1.9728 59.6653 Dataset info Description Matbench v0.1 test dataset for predicting refractive index from structure. Adapted from Materials Project database. Removed entries having a formation energy (or energy above the convex hull) more than 150meV and those having refractive indices less than 1 and those containing noble gases. Retrieved April 2, 2019. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 4764 Task type: regression Input type: structure Dataset columns n: Target variable. Refractive index (unitless). structure: Pymatgen Structure of the material. Dataset reference Petousis, I., Mrdjenovich, D., Ballouz, E., Liu, M., Winston, D., Chen, W., Graf, T., Schladt, T. D., Persson, K. A. & Prinz, F. B. High-throughput screening of inorganic compounds for the discovery of novel dielectric and optical materials. Sci. Data 4, 160134 (2017). Metadata {'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@article{Jain2013,\\n' 'author = {Jain, Anubhav and Ong, Shyue Ping and Hautier, ' 'Geoffroy and Chen, Wei and Richards, William Davidson and ' 'Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and ' 'Skinner, David and Ceder, Gerbrand and Persson, Kristin ' 'a.},\\n' 'doi = {10.1063/1.4812323},\\n' 'issn = {2166532X},\\n' 'journal = {APL Materials},\\n' 'number = {1},\\n' 'pages = {011002},\\n' 'title = {{The Materials Project: A materials genome approach ' 'to accelerating materials innovation}},\\n' 'url = ' '{http://link.aip.org/link/AMPADS/v1/i1/p011002/s1\\\\&Agg=doi},\\n' 'volume = {1},\\n' 'year = {2013}\\n' '}', '@article{Petousis2017,\\n' 'author={Petousis, Ioannis and Mrdjenovich, David and ' 'Ballouz, Eric\\n' 'and Liu, Miao and Winston, Donald and Chen, Wei and Graf, ' 'Tanja\\n' 'and Schladt, Thomas D. and Persson, Kristin A. and Prinz, ' 'Fritz B.},\\n' 'title={High-throughput screening of inorganic compounds for ' 'the\\n' 'discovery of novel dielectric and optical materials},\\n' 'journal={Scientific Data},\\n' 'year={2017},\\n' 'month={Jan},\\n' 'day={31},\\n' 'publisher={The Author(s)},\\n' 'volume={4},\\n' 'pages={160134},\\n' 'note={Data Descriptor},\\n' 'url={http://dx.doi.org/10.1038/sdata.2016.134}\\n' '}'], 'columns': {'n': 'Target variable. Refractive index (unitless).', 'structure': 'Pymatgen Structure of the material.'}, 'description': 'Matbench v0.1 test dataset for predicting refractive index ' 'from structure. Adapted from Materials Project database. ' 'Removed entries having a formation energy (or energy above ' 'the convex hull) more than 150meV and those having refractive ' 'indices less than 1 and those containing noble gases. ' 'Retrieved April 2, 2019. For benchmarking w/ nested cross ' 'validation, the order of the dataset must be identical to the ' 'retrieved data; refer to the Automatminer/Matbench ' 'publication for more details.', 'file_type': 'json.gz', 'hash': '83befa09bc2ec2f4b6143afc413157827a90e5e2e42c1eb507ccfa01bf26a1d6', 'input_type': 'structure', 'mad': 0.808534704217072, 'n_samples': 4764, 'num_entries': 4764, 'reference': 'Petousis, I., Mrdjenovich, D., Ballouz, E., Liu, M., Winston, ' 'D.,\\n' 'Chen, W., Graf, T., Schladt, T. D., Persson, K. A. & Prinz, F. ' 'B.\\n' 'High-throughput screening of inorganic compounds for the ' 'discovery\\n' 'of novel dielectric and optical materials. Sci. Data 4, 160134 ' '(2017).', 'target': 'n', 'task_type': 'regression', 'unit': 'unitless', 'url': 'https://ml.materialsproject.org/projects/matbench_dielectric.json.gz'}","title":"matbench_v0.1 matbench_dielectric"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_dielectric/#matbench_v01-matbench_dielectric","text":"","title":"matbench_v0.1 matbench_dielectric"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_dielectric/#individual-task-leaderboard-for-matbench_dielectric","text":"Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark.","title":"Individual Task Leaderboard for matbench_dielectric"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_dielectric/#leaderboard","text":"algorithm mean mae std mae mean rmse max max_error fictitious_compound 0.1981 0.0723 1.6165 58.7285 fictitious_model 0.2798 0.0726 1.6898 59.0262 MODNet (v0.1.10) 0.2970 0.0720 1.7185 58.9519 AMMExpress v2020 0.3150 0.0672 1.7202 59.0112 CrabNet 0.3234 0.0714 1.7288 59.1583 ALIGNN 0.3449 0.0871 1.9651 58.7285 RF-SCM/Magpie 0.4196 0.0750 1.8538 59.1201 CGCNN v2019 0.5988 0.0833 1.8976 58.9996 Dummy 0.8088 0.0718 1.9728 59.6653","title":"Leaderboard"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_dielectric/#dataset-info","text":"","title":"Dataset info"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_dielectric/#description","text":"Matbench v0.1 test dataset for predicting refractive index from structure. Adapted from Materials Project database. Removed entries having a formation energy (or energy above the convex hull) more than 150meV and those having refractive indices less than 1 and those containing noble gases. Retrieved April 2, 2019. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 4764 Task type: regression Input type: structure","title":"Description"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_dielectric/#dataset-columns","text":"n: Target variable. Refractive index (unitless). structure: Pymatgen Structure of the material.","title":"Dataset columns"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_dielectric/#dataset-reference","text":"Petousis, I., Mrdjenovich, D., Ballouz, E., Liu, M., Winston, D., Chen, W., Graf, T., Schladt, T. D., Persson, K. A. & Prinz, F. B. High-throughput screening of inorganic compounds for the discovery of novel dielectric and optical materials. Sci. Data 4, 160134 (2017).","title":"Dataset reference"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_dielectric/#metadata","text":"{'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@article{Jain2013,\\n' 'author = {Jain, Anubhav and Ong, Shyue Ping and Hautier, ' 'Geoffroy and Chen, Wei and Richards, William Davidson and ' 'Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and ' 'Skinner, David and Ceder, Gerbrand and Persson, Kristin ' 'a.},\\n' 'doi = {10.1063/1.4812323},\\n' 'issn = {2166532X},\\n' 'journal = {APL Materials},\\n' 'number = {1},\\n' 'pages = {011002},\\n' 'title = {{The Materials Project: A materials genome approach ' 'to accelerating materials innovation}},\\n' 'url = ' '{http://link.aip.org/link/AMPADS/v1/i1/p011002/s1\\\\&Agg=doi},\\n' 'volume = {1},\\n' 'year = {2013}\\n' '}', '@article{Petousis2017,\\n' 'author={Petousis, Ioannis and Mrdjenovich, David and ' 'Ballouz, Eric\\n' 'and Liu, Miao and Winston, Donald and Chen, Wei and Graf, ' 'Tanja\\n' 'and Schladt, Thomas D. and Persson, Kristin A. and Prinz, ' 'Fritz B.},\\n' 'title={High-throughput screening of inorganic compounds for ' 'the\\n' 'discovery of novel dielectric and optical materials},\\n' 'journal={Scientific Data},\\n' 'year={2017},\\n' 'month={Jan},\\n' 'day={31},\\n' 'publisher={The Author(s)},\\n' 'volume={4},\\n' 'pages={160134},\\n' 'note={Data Descriptor},\\n' 'url={http://dx.doi.org/10.1038/sdata.2016.134}\\n' '}'], 'columns': {'n': 'Target variable. Refractive index (unitless).', 'structure': 'Pymatgen Structure of the material.'}, 'description': 'Matbench v0.1 test dataset for predicting refractive index ' 'from structure. Adapted from Materials Project database. ' 'Removed entries having a formation energy (or energy above ' 'the convex hull) more than 150meV and those having refractive ' 'indices less than 1 and those containing noble gases. ' 'Retrieved April 2, 2019. For benchmarking w/ nested cross ' 'validation, the order of the dataset must be identical to the ' 'retrieved data; refer to the Automatminer/Matbench ' 'publication for more details.', 'file_type': 'json.gz', 'hash': '83befa09bc2ec2f4b6143afc413157827a90e5e2e42c1eb507ccfa01bf26a1d6', 'input_type': 'structure', 'mad': 0.808534704217072, 'n_samples': 4764, 'num_entries': 4764, 'reference': 'Petousis, I., Mrdjenovich, D., Ballouz, E., Liu, M., Winston, ' 'D.,\\n' 'Chen, W., Graf, T., Schladt, T. D., Persson, K. A. & Prinz, F. ' 'B.\\n' 'High-throughput screening of inorganic compounds for the ' 'discovery\\n' 'of novel dielectric and optical materials. Sci. Data 4, 160134 ' '(2017).', 'target': 'n', 'task_type': 'regression', 'unit': 'unitless', 'url': 'https://ml.materialsproject.org/projects/matbench_dielectric.json.gz'}","title":"Metadata"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_expt_gap/","text":"matbench_v0.1 matbench_expt_gap Individual Task Leaderboard for matbench_expt_gap Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark. Leaderboard algorithm mean mae std mae mean rmse max max_error fictitious_compound 0.1001 0.0076 0.3245 7.8869 fictitious_model 0.3218 0.0108 0.6774 8.8678 CrabNet 0.3463 0.0088 0.8504 9.8002 MODNet (v0.1.10) 0.3470 0.0222 0.7437 9.8567 Ax+CrabNet v1.2.1 0.3566 0.0248 0.8673 11.0998 CrabNet v1.2.1 0.3757 0.0207 0.8805 10.2572 AMMExpress v2020 0.4161 0.0194 0.9918 12.7533 RF-SCM/Magpie 0.4461 0.0177 0.8243 9.5428 Dummy 1.1435 0.0310 1.4438 10.7354 Dataset info Description Matbench v0.1 test dataset for predicting experimental band gap from composition alone. Retrieved from Zhuo et al. supplementary information. Deduplicated according to composition, removing compositions with reported band gaps spanning more than a 0.1eV range; remaining compositions were assigned values based on the closest experimental value to the mean experimental value for that composition among all reports. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 4604 Task type: regression Input type: composition Dataset columns composition: Chemical formula. gap expt: Target variable. Experimentally measured gap, in eV. Dataset reference Y. Zhuo, A. Masouri Tehrani, J. Brgoch (2018) Predicting the Band Gaps of Inorganic Solids by Machine Learning J. Phys. Chem. Lett. 2018, 9, 7, 1668-1673 https:doi.org/10.1021/acs.jpclett.8b00124. Metadata {'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@article{doi:10.1021/acs.jpclett.8b00124,\\n' 'author = {Zhuo, Ya and Mansouri Tehrani, Aria and Brgoch, ' 'Jakoah},\\n' 'title = {Predicting the Band Gaps of Inorganic Solids by ' 'Machine Learning},\\n' 'journal = {The Journal of Physical Chemistry Letters},\\n' 'volume = {9},\\n' 'number = {7},\\n' 'pages = {1668-1673},\\n' 'year = {2018},\\n' 'doi = {10.1021/acs.jpclett.8b00124},\\n' 'note ={PMID: 29532658},\\n' 'eprint = {\\n' 'https://doi.org/10.1021/acs.jpclett.8b00124\\n' '\\n' '}}'], 'columns': {'composition': 'Chemical formula.', 'gap expt': 'Target variable. Experimentally measured gap, in ' 'eV.'}, 'description': 'Matbench v0.1 test dataset for predicting experimental band ' 'gap from composition alone. Retrieved from Zhuo et al. ' 'supplementary information. Deduplicated according to ' 'composition, removing compositions with reported band gaps ' 'spanning more than a 0.1eV range; remaining compositions were ' 'assigned values based on the closest experimental value to ' 'the mean experimental value for that composition among all ' 'reports. For benchmarking w/ nested cross validation, the ' 'order of the dataset must be identical to the retrieved data; ' 'refer to the Automatminer/Matbench publication for more ' 'details.', 'file_type': 'json.gz', 'hash': '783e7d1461eb83b00b2f2942da4b95fda5e58a0d1ae26b581c24cf8a82ca75b2', 'input_type': 'composition', 'mad': 1.1432002429044061, 'n_samples': 4604, 'num_entries': 4604, 'reference': 'Y. Zhuo, A. Masouri Tehrani, J. Brgoch (2018) Predicting the ' 'Band Gaps of Inorganic Solids by Machine Learning J. Phys. ' 'Chem. Lett. 2018, 9, 7, 1668-1673 ' 'https:doi.org/10.1021/acs.jpclett.8b00124.', 'target': 'gap expt', 'task_type': 'regression', 'unit': 'eV', 'url': 'https://ml.materialsproject.org/projects/matbench_expt_gap.json.gz'}","title":"matbench_v0.1 matbench_expt_gap"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_expt_gap/#matbench_v01-matbench_expt_gap","text":"","title":"matbench_v0.1 matbench_expt_gap"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_expt_gap/#individual-task-leaderboard-for-matbench_expt_gap","text":"Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark.","title":"Individual Task Leaderboard for matbench_expt_gap"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_expt_gap/#leaderboard","text":"algorithm mean mae std mae mean rmse max max_error fictitious_compound 0.1001 0.0076 0.3245 7.8869 fictitious_model 0.3218 0.0108 0.6774 8.8678 CrabNet 0.3463 0.0088 0.8504 9.8002 MODNet (v0.1.10) 0.3470 0.0222 0.7437 9.8567 Ax+CrabNet v1.2.1 0.3566 0.0248 0.8673 11.0998 CrabNet v1.2.1 0.3757 0.0207 0.8805 10.2572 AMMExpress v2020 0.4161 0.0194 0.9918 12.7533 RF-SCM/Magpie 0.4461 0.0177 0.8243 9.5428 Dummy 1.1435 0.0310 1.4438 10.7354","title":"Leaderboard"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_expt_gap/#dataset-info","text":"","title":"Dataset info"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_expt_gap/#description","text":"Matbench v0.1 test dataset for predicting experimental band gap from composition alone. Retrieved from Zhuo et al. supplementary information. Deduplicated according to composition, removing compositions with reported band gaps spanning more than a 0.1eV range; remaining compositions were assigned values based on the closest experimental value to the mean experimental value for that composition among all reports. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 4604 Task type: regression Input type: composition","title":"Description"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_expt_gap/#dataset-columns","text":"composition: Chemical formula. gap expt: Target variable. Experimentally measured gap, in eV.","title":"Dataset columns"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_expt_gap/#dataset-reference","text":"Y. Zhuo, A. Masouri Tehrani, J. Brgoch (2018) Predicting the Band Gaps of Inorganic Solids by Machine Learning J. Phys. Chem. Lett. 2018, 9, 7, 1668-1673 https:doi.org/10.1021/acs.jpclett.8b00124.","title":"Dataset reference"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_expt_gap/#metadata","text":"{'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@article{doi:10.1021/acs.jpclett.8b00124,\\n' 'author = {Zhuo, Ya and Mansouri Tehrani, Aria and Brgoch, ' 'Jakoah},\\n' 'title = {Predicting the Band Gaps of Inorganic Solids by ' 'Machine Learning},\\n' 'journal = {The Journal of Physical Chemistry Letters},\\n' 'volume = {9},\\n' 'number = {7},\\n' 'pages = {1668-1673},\\n' 'year = {2018},\\n' 'doi = {10.1021/acs.jpclett.8b00124},\\n' 'note ={PMID: 29532658},\\n' 'eprint = {\\n' 'https://doi.org/10.1021/acs.jpclett.8b00124\\n' '\\n' '}}'], 'columns': {'composition': 'Chemical formula.', 'gap expt': 'Target variable. Experimentally measured gap, in ' 'eV.'}, 'description': 'Matbench v0.1 test dataset for predicting experimental band ' 'gap from composition alone. Retrieved from Zhuo et al. ' 'supplementary information. Deduplicated according to ' 'composition, removing compositions with reported band gaps ' 'spanning more than a 0.1eV range; remaining compositions were ' 'assigned values based on the closest experimental value to ' 'the mean experimental value for that composition among all ' 'reports. For benchmarking w/ nested cross validation, the ' 'order of the dataset must be identical to the retrieved data; ' 'refer to the Automatminer/Matbench publication for more ' 'details.', 'file_type': 'json.gz', 'hash': '783e7d1461eb83b00b2f2942da4b95fda5e58a0d1ae26b581c24cf8a82ca75b2', 'input_type': 'composition', 'mad': 1.1432002429044061, 'n_samples': 4604, 'num_entries': 4604, 'reference': 'Y. Zhuo, A. Masouri Tehrani, J. Brgoch (2018) Predicting the ' 'Band Gaps of Inorganic Solids by Machine Learning J. Phys. ' 'Chem. Lett. 2018, 9, 7, 1668-1673 ' 'https:doi.org/10.1021/acs.jpclett.8b00124.', 'target': 'gap expt', 'task_type': 'regression', 'unit': 'eV', 'url': 'https://ml.materialsproject.org/projects/matbench_expt_gap.json.gz'}","title":"Metadata"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_expt_is_metal/","text":"matbench_v0.1 matbench_expt_is_metal Individual Task Leaderboard for matbench_expt_is_metal Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark. Leaderboard algorithm mean rocauc std rocauc mean f1 mean balanced_accuracy fictitious_compound 0.9937 0.0017 0.9937 0.9937 AMMExpress v2020 0.9209 0.0028 0.9200 0.9209 fictitious_model 0.9209 0.0028 0.9200 0.9209 RF-SCM/Magpie 0.9167 0.0064 0.9159 0.9167 MODNet (v0.1.10) 0.9161 0.0072 0.9153 0.9161 Dummy 0.4924 0.0128 0.4913 0.4924 Dataset info Description Matbench v0.1 test dataset for classifying metallicity from composition alone. Retrieved from Zhuo et al. supplementary information. Deduplicated according to composition, ensuring no conflicting reports were entered for any compositions (i.e., no reported compositions were both metal and nonmetal). For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 4921 Task type: classification Input type: composition Dataset columns composition: Chemical formula. is_metal: Target variable. 1 if is a metal, 0 if nonmetal. Dataset reference Y. Zhuo, A. Masouri Tehrani, J. Brgoch (2018) Predicting the Band Gaps of Inorganic Solids by Machine Learning J. Phys. Chem. Lett. 2018, 9, 7, 1668-1673 https//:doi.org/10.1021/acs.jpclett.8b00124. Metadata {'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@article{doi:10.1021/acs.jpclett.8b00124,\\n' 'author = {Zhuo, Ya and Mansouri Tehrani, Aria and Brgoch, ' 'Jakoah},\\n' 'title= {Predicting the Band Gaps of Inorganic Solids by ' 'Machine Learning},\\n' 'journal = {The Journal of Physical Chemistry Letters},\\n' 'volume = {9},\\n' 'number = {7},\\n' 'pages = {1668-1673},\\n' 'year = {2018},\\n' 'doi = {10.1021/acs.jpclett.8b00124},\\n' 'note ={PMID: 29532658},\\n' 'eprint = {\\n' 'https://doi.org/10.1021/acs.jpclett.8b00124\\n' '\\n' '}}'], 'columns': {'composition': 'Chemical formula.', 'is_metal': 'Target variable. 1 if is a metal, 0 if nonmetal.'}, 'description': 'Matbench v0.1 test dataset for classifying metallicity from ' 'composition alone. Retrieved from Zhuo et al. supplementary ' 'information. Deduplicated according to composition, ensuring ' 'no conflicting reports were entered for any compositions ' '(i.e., no reported compositions were both metal and ' 'nonmetal). For benchmarking w/ nested cross validation, the ' 'order of the dataset must be identical to the retrieved data; ' 'refer to the Automatminer/Matbench publication for more ' 'details.', 'file_type': 'json.gz', 'frac_true': 0.4980694980694981, 'hash': '8f2a4f9bacdcbc5c2c73615629ee7986f09d39bed40ba7db52b61b2889730887', 'input_type': 'composition', 'n_samples': 4921, 'num_entries': 4921, 'reference': 'Y. Zhuo, A. Masouri Tehrani, J. Brgoch (2018) Predicting the ' 'Band Gaps of Inorganic Solids by Machine Learning J. Phys. ' 'Chem. Lett. 2018, 9, 7, 1668-1673 \\n' ' https//:doi.org/10.1021/acs.jpclett.8b00124.', 'target': 'is_metal', 'task_type': 'classification', 'unit': None, 'url': 'https://ml.materialsproject.org/projects/matbench_expt_is_metal.json.gz'}","title":"matbench_v0.1 matbench_expt_is_metal"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_expt_is_metal/#matbench_v01-matbench_expt_is_metal","text":"","title":"matbench_v0.1 matbench_expt_is_metal"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_expt_is_metal/#individual-task-leaderboard-for-matbench_expt_is_metal","text":"Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark.","title":"Individual Task Leaderboard for matbench_expt_is_metal"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_expt_is_metal/#leaderboard","text":"algorithm mean rocauc std rocauc mean f1 mean balanced_accuracy fictitious_compound 0.9937 0.0017 0.9937 0.9937 AMMExpress v2020 0.9209 0.0028 0.9200 0.9209 fictitious_model 0.9209 0.0028 0.9200 0.9209 RF-SCM/Magpie 0.9167 0.0064 0.9159 0.9167 MODNet (v0.1.10) 0.9161 0.0072 0.9153 0.9161 Dummy 0.4924 0.0128 0.4913 0.4924","title":"Leaderboard"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_expt_is_metal/#dataset-info","text":"","title":"Dataset info"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_expt_is_metal/#description","text":"Matbench v0.1 test dataset for classifying metallicity from composition alone. Retrieved from Zhuo et al. supplementary information. Deduplicated according to composition, ensuring no conflicting reports were entered for any compositions (i.e., no reported compositions were both metal and nonmetal). For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 4921 Task type: classification Input type: composition","title":"Description"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_expt_is_metal/#dataset-columns","text":"composition: Chemical formula. is_metal: Target variable. 1 if is a metal, 0 if nonmetal.","title":"Dataset columns"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_expt_is_metal/#dataset-reference","text":"Y. Zhuo, A. Masouri Tehrani, J. Brgoch (2018) Predicting the Band Gaps of Inorganic Solids by Machine Learning J. Phys. Chem. Lett. 2018, 9, 7, 1668-1673 https//:doi.org/10.1021/acs.jpclett.8b00124.","title":"Dataset reference"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_expt_is_metal/#metadata","text":"{'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@article{doi:10.1021/acs.jpclett.8b00124,\\n' 'author = {Zhuo, Ya and Mansouri Tehrani, Aria and Brgoch, ' 'Jakoah},\\n' 'title= {Predicting the Band Gaps of Inorganic Solids by ' 'Machine Learning},\\n' 'journal = {The Journal of Physical Chemistry Letters},\\n' 'volume = {9},\\n' 'number = {7},\\n' 'pages = {1668-1673},\\n' 'year = {2018},\\n' 'doi = {10.1021/acs.jpclett.8b00124},\\n' 'note ={PMID: 29532658},\\n' 'eprint = {\\n' 'https://doi.org/10.1021/acs.jpclett.8b00124\\n' '\\n' '}}'], 'columns': {'composition': 'Chemical formula.', 'is_metal': 'Target variable. 1 if is a metal, 0 if nonmetal.'}, 'description': 'Matbench v0.1 test dataset for classifying metallicity from ' 'composition alone. Retrieved from Zhuo et al. supplementary ' 'information. Deduplicated according to composition, ensuring ' 'no conflicting reports were entered for any compositions ' '(i.e., no reported compositions were both metal and ' 'nonmetal). For benchmarking w/ nested cross validation, the ' 'order of the dataset must be identical to the retrieved data; ' 'refer to the Automatminer/Matbench publication for more ' 'details.', 'file_type': 'json.gz', 'frac_true': 0.4980694980694981, 'hash': '8f2a4f9bacdcbc5c2c73615629ee7986f09d39bed40ba7db52b61b2889730887', 'input_type': 'composition', 'n_samples': 4921, 'num_entries': 4921, 'reference': 'Y. Zhuo, A. Masouri Tehrani, J. Brgoch (2018) Predicting the ' 'Band Gaps of Inorganic Solids by Machine Learning J. Phys. ' 'Chem. Lett. 2018, 9, 7, 1668-1673 \\n' ' https//:doi.org/10.1021/acs.jpclett.8b00124.', 'target': 'is_metal', 'task_type': 'classification', 'unit': None, 'url': 'https://ml.materialsproject.org/projects/matbench_expt_is_metal.json.gz'}","title":"Metadata"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_glass/","text":"matbench_v0.1 matbench_glass Individual Task Leaderboard for matbench_glass Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark. Leaderboard algorithm mean rocauc std rocauc mean f1 mean balanced_accuracy fictitious_compound 0.9690 0.0079 0.9855 0.9690 AMMExpress v2020 0.8607 0.0199 0.9043 0.8607 fictitious_model 0.8607 0.0199 0.9043 0.8607 RF-SCM/Magpie 0.8587 0.0158 0.9278 0.8587 MODNet (v0.1.10) 0.8107 0.0212 0.9104 0.8107 Dummy 0.5005 0.0178 0.7127 0.5005 Dataset info Description Matbench v0.1 test dataset for predicting full bulk metallic glass formation ability from chemical formula. Retrieved from \"Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys,\u2019 a volume of the Landolt\u2013 B\u00f6rnstein collection. Deduplicated according to composition, ensuring no compositions were reported as both GFA and not GFA (i.e., all reports agreed on the classification designation). For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 5680 Task type: classification Input type: composition Dataset columns composition: Chemical formula. gfa: Target variable. Glass forming ability: 1 means glass forming and corresponds to amorphous, 0 means non full glass forming. Dataset reference Y. Kawazoe, T. Masumoto, A.-P. Tsai, J.-Z. Yu, T. Aihara Jr. (1997) Y. Kawazoe, J.-Z. Yu, A.-P. Tsai, T. Masumoto (ed.) SpringerMaterials Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys \u00b7 1 Introduction Landolt-B\u00f6rnstein - Group III Condensed Matter 37A (Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys) https://www.springer.com/gp/book/9783540605072 (Springer-Verlag Berlin Heidelberg \u00a9 1997) Accessed: 03-09-2019 Metadata {'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@Misc{LandoltBornstein1997:sm_lbs_978-3-540-47679-5_2,\\n' 'author=\"Kawazoe, Y.\\n' 'and Masumoto, T.\\n' 'and Tsai, A.-P.\\n' 'and Yu, J.-Z.\\n' 'and Aihara Jr., T.\",\\n' 'editor=\"Kawazoe, Y.\\n' 'and Yu, J.-Z.\\n' 'and Tsai, A.-P.\\n' 'and Masumoto, T.\",\\n' 'title=\"Nonequilibrium Phase Diagrams of Ternary Amorphous ' 'Alloys {\\\\textperiodcentered} 1 Introduction: Datasheet from ' 'Landolt-B{\\\\\"o}rnstein - Group III Condensed Matter ' '{\\\\textperiodcentered} Volume 37A: ``Nonequilibrium Phase ' \"Diagrams of Ternary Amorphous Alloys'' in SpringerMaterials \" '(https://dx.doi.org/10.1007/10510374{\\\\_}2)\",\\n' 'publisher=\"Springer-Verlag Berlin Heidelberg\",\\n' 'note=\"Copyright 1997 Springer-Verlag Berlin Heidelberg\",\\n' 'note=\"Part of SpringerMaterials\",\\n' 'note=\"accessed 2018-10-23\",\\n' 'doi=\"10.1007/10510374_2\",\\n' 'url=\"https://materials.springer.com/lb/docs/sm_lbs_978-3-540-47679-5_2\"\\n' '}', '@Article{Ward2016,\\n' 'author={Ward, Logan\\n' 'and Agrawal, Ankit\\n' 'and Choudhary, Alok\\n' 'and Wolverton, Christopher},\\n' 'title={A general-purpose machine learning framework for ' 'predicting properties of inorganic materials},\\n' 'journal={Npj Computational Materials},\\n' 'year={2016},\\n' 'month={Aug},\\n' 'day={26},\\n' 'publisher={The Author(s)},\\n' 'volume={2},\\n' 'pages={16028},\\n' 'note={Article},\\n' 'url={http://dx.doi.org/10.1038/npjcompumats.2016.28}\\n' '}'], 'columns': {'composition': 'Chemical formula.', 'gfa': 'Target variable. Glass forming ability: 1 means glass ' 'forming and corresponds to amorphous, 0 means non full ' 'glass forming.'}, 'description': 'Matbench v0.1 test dataset for predicting full bulk metallic ' 'glass formation ability from chemical formula. Retrieved from ' '\"Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys,\u2019 ' 'a volume of the Landolt\u2013 B\u00f6rnstein collection. Deduplicated ' 'according to composition, ensuring no compositions were ' 'reported as both GFA and not GFA (i.e., all reports agreed on ' 'the classification designation). For benchmarking w/ nested ' 'cross validation, the order of the dataset must be identical ' 'to the retrieved data; refer to the Automatminer/Matbench ' 'publication for more details.', 'file_type': 'json.gz', 'frac_true': 0.710387323943662, 'hash': '36beb654e2a463ee2a6572105bea0ca2961eee7c7b26a25377bff2c3b338e53a', 'input_type': 'composition', 'n_samples': 5680, 'num_entries': 5680, 'reference': 'Y. Kawazoe, T. Masumoto, A.-P. Tsai, J.-Z. Yu, T. Aihara Jr. ' '(1997) Y. Kawazoe, J.-Z. Yu, A.-P. Tsai, T. Masumoto (ed.) ' 'SpringerMaterials\\n' 'Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys \u00b7 1 ' 'Introduction Landolt-B\u00f6rnstein - Group III Condensed Matter 37A ' '(Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys) ' 'https://www.springer.com/gp/book/9783540605072 (Springer-Verlag ' 'Berlin Heidelberg \u00a9 1997) Accessed: 03-09-2019', 'target': 'gfa', 'task_type': 'classification', 'unit': None, 'url': 'https://ml.materialsproject.org/projects/matbench_glass.json.gz'}","title":"matbench_v0.1 matbench_glass"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_glass/#matbench_v01-matbench_glass","text":"","title":"matbench_v0.1 matbench_glass"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_glass/#individual-task-leaderboard-for-matbench_glass","text":"Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark.","title":"Individual Task Leaderboard for matbench_glass"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_glass/#leaderboard","text":"algorithm mean rocauc std rocauc mean f1 mean balanced_accuracy fictitious_compound 0.9690 0.0079 0.9855 0.9690 AMMExpress v2020 0.8607 0.0199 0.9043 0.8607 fictitious_model 0.8607 0.0199 0.9043 0.8607 RF-SCM/Magpie 0.8587 0.0158 0.9278 0.8587 MODNet (v0.1.10) 0.8107 0.0212 0.9104 0.8107 Dummy 0.5005 0.0178 0.7127 0.5005","title":"Leaderboard"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_glass/#dataset-info","text":"","title":"Dataset info"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_glass/#description","text":"Matbench v0.1 test dataset for predicting full bulk metallic glass formation ability from chemical formula. Retrieved from \"Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys,\u2019 a volume of the Landolt\u2013 B\u00f6rnstein collection. Deduplicated according to composition, ensuring no compositions were reported as both GFA and not GFA (i.e., all reports agreed on the classification designation). For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 5680 Task type: classification Input type: composition","title":"Description"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_glass/#dataset-columns","text":"composition: Chemical formula. gfa: Target variable. Glass forming ability: 1 means glass forming and corresponds to amorphous, 0 means non full glass forming.","title":"Dataset columns"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_glass/#dataset-reference","text":"Y. Kawazoe, T. Masumoto, A.-P. Tsai, J.-Z. Yu, T. Aihara Jr. (1997) Y. Kawazoe, J.-Z. Yu, A.-P. Tsai, T. Masumoto (ed.) SpringerMaterials Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys \u00b7 1 Introduction Landolt-B\u00f6rnstein - Group III Condensed Matter 37A (Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys) https://www.springer.com/gp/book/9783540605072 (Springer-Verlag Berlin Heidelberg \u00a9 1997) Accessed: 03-09-2019","title":"Dataset reference"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_glass/#metadata","text":"{'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@Misc{LandoltBornstein1997:sm_lbs_978-3-540-47679-5_2,\\n' 'author=\"Kawazoe, Y.\\n' 'and Masumoto, T.\\n' 'and Tsai, A.-P.\\n' 'and Yu, J.-Z.\\n' 'and Aihara Jr., T.\",\\n' 'editor=\"Kawazoe, Y.\\n' 'and Yu, J.-Z.\\n' 'and Tsai, A.-P.\\n' 'and Masumoto, T.\",\\n' 'title=\"Nonequilibrium Phase Diagrams of Ternary Amorphous ' 'Alloys {\\\\textperiodcentered} 1 Introduction: Datasheet from ' 'Landolt-B{\\\\\"o}rnstein - Group III Condensed Matter ' '{\\\\textperiodcentered} Volume 37A: ``Nonequilibrium Phase ' \"Diagrams of Ternary Amorphous Alloys'' in SpringerMaterials \" '(https://dx.doi.org/10.1007/10510374{\\\\_}2)\",\\n' 'publisher=\"Springer-Verlag Berlin Heidelberg\",\\n' 'note=\"Copyright 1997 Springer-Verlag Berlin Heidelberg\",\\n' 'note=\"Part of SpringerMaterials\",\\n' 'note=\"accessed 2018-10-23\",\\n' 'doi=\"10.1007/10510374_2\",\\n' 'url=\"https://materials.springer.com/lb/docs/sm_lbs_978-3-540-47679-5_2\"\\n' '}', '@Article{Ward2016,\\n' 'author={Ward, Logan\\n' 'and Agrawal, Ankit\\n' 'and Choudhary, Alok\\n' 'and Wolverton, Christopher},\\n' 'title={A general-purpose machine learning framework for ' 'predicting properties of inorganic materials},\\n' 'journal={Npj Computational Materials},\\n' 'year={2016},\\n' 'month={Aug},\\n' 'day={26},\\n' 'publisher={The Author(s)},\\n' 'volume={2},\\n' 'pages={16028},\\n' 'note={Article},\\n' 'url={http://dx.doi.org/10.1038/npjcompumats.2016.28}\\n' '}'], 'columns': {'composition': 'Chemical formula.', 'gfa': 'Target variable. Glass forming ability: 1 means glass ' 'forming and corresponds to amorphous, 0 means non full ' 'glass forming.'}, 'description': 'Matbench v0.1 test dataset for predicting full bulk metallic ' 'glass formation ability from chemical formula. Retrieved from ' '\"Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys,\u2019 ' 'a volume of the Landolt\u2013 B\u00f6rnstein collection. Deduplicated ' 'according to composition, ensuring no compositions were ' 'reported as both GFA and not GFA (i.e., all reports agreed on ' 'the classification designation). For benchmarking w/ nested ' 'cross validation, the order of the dataset must be identical ' 'to the retrieved data; refer to the Automatminer/Matbench ' 'publication for more details.', 'file_type': 'json.gz', 'frac_true': 0.710387323943662, 'hash': '36beb654e2a463ee2a6572105bea0ca2961eee7c7b26a25377bff2c3b338e53a', 'input_type': 'composition', 'n_samples': 5680, 'num_entries': 5680, 'reference': 'Y. Kawazoe, T. Masumoto, A.-P. Tsai, J.-Z. Yu, T. Aihara Jr. ' '(1997) Y. Kawazoe, J.-Z. Yu, A.-P. Tsai, T. Masumoto (ed.) ' 'SpringerMaterials\\n' 'Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys \u00b7 1 ' 'Introduction Landolt-B\u00f6rnstein - Group III Condensed Matter 37A ' '(Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys) ' 'https://www.springer.com/gp/book/9783540605072 (Springer-Verlag ' 'Berlin Heidelberg \u00a9 1997) Accessed: 03-09-2019', 'target': 'gfa', 'task_type': 'classification', 'unit': None, 'url': 'https://ml.materialsproject.org/projects/matbench_glass.json.gz'}","title":"Metadata"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_jdft2d/","text":"matbench_v0.1 matbench_jdft2d Individual Task Leaderboard for matbench_jdft2d Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark. Leaderboard algorithm mean mae std mae mean rmse max max_error fictitious_compound 21.9636 9.8048 79.3013 1519.7424 fictitious_model 33.3825 9.8594 91.6341 1533.5975 MODNet (v0.1.10) 34.5368 9.4959 92.2288 1534.9797 AMMExpress v2020 39.8497 9.8835 106.5460 1552.9102 ALIGNN 43.4244 8.9491 117.4213 1519.7424 CrabNet 45.6104 12.2491 120.0088 1532.0118 CGCNN v2019 49.2440 11.5865 112.7689 1516.9120 RF-SCM/Magpie 50.0440 8.6271 112.2660 1538.6073 Dummy 67.2851 10.1832 126.8446 1491.7993 Dataset info Description Matbench v0.1 test dataset for predicting exfoliation energies from crystal structure (computed with the OptB88vdW and TBmBJ functionals). Adapted from the JARVIS DFT database. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 636 Task type: regression Input type: structure Dataset columns exfoliation_en: Target variable. Exfoliation energy (meV/atom). structure: Pymatgen Structure of the material. Dataset reference 2D Dataset discussed in: High-throughput Identification and Characterization of Two dimensional Materials using Density functional theory Kamal Choudhary, Irina Kalish, Ryan Beams & Francesca Tavazza Scientific Reports volume 7, Article number: 5179 (2017) Original 2D Data file sourced from: choudhary, kamal; https://orcid.org/0000-0001-9737-8074 (2018): jdft_2d-7-7-2018.json. figshare. Dataset. Metadata {'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@Article{Choudhary2017,\\n' 'author={Choudhary, Kamal\\n' 'and Kalish, Irina\\n' 'and Beams, Ryan\\n' 'and Tavazza, Francesca},\\n' 'title={High-throughput Identification and Characterization ' 'of Two-dimensional Materials using Density functional ' 'theory},\\n' 'journal={Scientific Reports},\\n' 'year={2017},\\n' 'volume={7},\\n' 'number={1},\\n' 'pages={5179},\\n' 'abstract={We introduce a simple criterion to identify ' 'two-dimensional (2D) materials based on the comparison ' 'between experimental lattice constants and lattice constants ' 'mainly obtained from Materials-Project (MP) density ' 'functional theory (DFT) calculation repository. ' 'Specifically, if the relative difference between the two ' 'lattice constants for a specific material is greater than or ' 'equal to 5%, we predict them to be good candidates for 2D ' 'materials. We have predicted at least 1356 such 2D ' 'materials. For all the systems satisfying our criterion, we ' 'manually create single layer systems and calculate their ' 'energetics, structural, electronic, and elastic properties ' 'for both the bulk and the single layer cases. Currently the ' 'database consists of 1012 bulk and 430 single layer ' 'materials, of which 371 systems are common to bulk and ' 'single layer. The rest of calculations are underway. To ' 'validate our criterion, we calculated the exfoliation energy ' 'of the suggested layered materials, and we found that in ' '88.9% of the cases the currently accepted criterion for ' 'exfoliation was satisfied. Also, using molybdenum telluride ' 'as a test case, we performed X-ray diffraction and Raman ' 'scattering experiments to benchmark our calculations and ' 'understand their applicability and limitations. The data is ' 'publicly available at the website ' 'http://www.ctcms.nist.gov/{\\t' 'extasciitilde}knc6/JVASP.html.},\\n' 'issn={2045-2322},\\n' 'doi={10.1038/s41598-017-05402-0},\\n' 'url={https://doi.org/10.1038/s41598-017-05402-0}\\n' '}', '@misc{choudhary__2018, title={jdft_2d-7-7-2018.json}, ' 'url={https://figshare.com/articles/jdft_2d-7-7-2018_json/6815705/1}, ' 'DOI={10.6084/m9.figshare.6815705.v1}, abstractNote={2D ' 'materials}, publisher={figshare}, author={choudhary, kamal ' 'and https://orcid.org/0000-0001-9737-8074}, year={2018}, ' 'month={Jul}}'], 'columns': {'exfoliation_en': 'Target variable. Exfoliation energy ' '(meV/atom).', 'structure': 'Pymatgen Structure of the material.'}, 'description': 'Matbench v0.1 test dataset for predicting exfoliation ' 'energies from crystal structure (computed with the OptB88vdW ' 'and TBmBJ functionals). Adapted from the JARVIS DFT database. ' 'For benchmarking w/ nested cross validation, the order of the ' 'dataset must be identical to the retrieved data; refer to the ' 'Automatminer/Matbench publication for more details.', 'file_type': 'json.gz', 'hash': '26057dc4524e193e32abffb296ce819b58b6e11d1278cae329a2f97817a4eddf', 'input_type': 'structure', 'mad': 67.20200406491116, 'n_samples': 636, 'num_entries': 636, 'reference': '2D Dataset discussed in:\\n' 'High-throughput Identification and Characterization of Two ' 'dimensional Materials using Density functional theory Kamal ' 'Choudhary, Irina Kalish, Ryan Beams & Francesca Tavazza ' 'Scientific Reports volume 7, Article number: 5179 (2017)\\n' 'Original 2D Data file sourced from:\\n' 'choudhary, kamal; https://orcid.org/0000-0001-9737-8074 (2018): ' 'jdft_2d-7-7-2018.json. figshare. Dataset.', 'target': 'exfoliation_en', 'task_type': 'regression', 'unit': 'meV/atom', 'url': 'https://ml.materialsproject.org/projects/matbench_jdft2d.json.gz'}","title":"matbench_v0.1 matbench_jdft2d"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_jdft2d/#matbench_v01-matbench_jdft2d","text":"","title":"matbench_v0.1 matbench_jdft2d"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_jdft2d/#individual-task-leaderboard-for-matbench_jdft2d","text":"Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark.","title":"Individual Task Leaderboard for matbench_jdft2d"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_jdft2d/#leaderboard","text":"algorithm mean mae std mae mean rmse max max_error fictitious_compound 21.9636 9.8048 79.3013 1519.7424 fictitious_model 33.3825 9.8594 91.6341 1533.5975 MODNet (v0.1.10) 34.5368 9.4959 92.2288 1534.9797 AMMExpress v2020 39.8497 9.8835 106.5460 1552.9102 ALIGNN 43.4244 8.9491 117.4213 1519.7424 CrabNet 45.6104 12.2491 120.0088 1532.0118 CGCNN v2019 49.2440 11.5865 112.7689 1516.9120 RF-SCM/Magpie 50.0440 8.6271 112.2660 1538.6073 Dummy 67.2851 10.1832 126.8446 1491.7993","title":"Leaderboard"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_jdft2d/#dataset-info","text":"","title":"Dataset info"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_jdft2d/#description","text":"Matbench v0.1 test dataset for predicting exfoliation energies from crystal structure (computed with the OptB88vdW and TBmBJ functionals). Adapted from the JARVIS DFT database. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 636 Task type: regression Input type: structure","title":"Description"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_jdft2d/#dataset-columns","text":"exfoliation_en: Target variable. Exfoliation energy (meV/atom). structure: Pymatgen Structure of the material.","title":"Dataset columns"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_jdft2d/#dataset-reference","text":"2D Dataset discussed in: High-throughput Identification and Characterization of Two dimensional Materials using Density functional theory Kamal Choudhary, Irina Kalish, Ryan Beams & Francesca Tavazza Scientific Reports volume 7, Article number: 5179 (2017) Original 2D Data file sourced from: choudhary, kamal; https://orcid.org/0000-0001-9737-8074 (2018): jdft_2d-7-7-2018.json. figshare. Dataset.","title":"Dataset reference"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_jdft2d/#metadata","text":"{'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@Article{Choudhary2017,\\n' 'author={Choudhary, Kamal\\n' 'and Kalish, Irina\\n' 'and Beams, Ryan\\n' 'and Tavazza, Francesca},\\n' 'title={High-throughput Identification and Characterization ' 'of Two-dimensional Materials using Density functional ' 'theory},\\n' 'journal={Scientific Reports},\\n' 'year={2017},\\n' 'volume={7},\\n' 'number={1},\\n' 'pages={5179},\\n' 'abstract={We introduce a simple criterion to identify ' 'two-dimensional (2D) materials based on the comparison ' 'between experimental lattice constants and lattice constants ' 'mainly obtained from Materials-Project (MP) density ' 'functional theory (DFT) calculation repository. ' 'Specifically, if the relative difference between the two ' 'lattice constants for a specific material is greater than or ' 'equal to 5%, we predict them to be good candidates for 2D ' 'materials. We have predicted at least 1356 such 2D ' 'materials. For all the systems satisfying our criterion, we ' 'manually create single layer systems and calculate their ' 'energetics, structural, electronic, and elastic properties ' 'for both the bulk and the single layer cases. Currently the ' 'database consists of 1012 bulk and 430 single layer ' 'materials, of which 371 systems are common to bulk and ' 'single layer. The rest of calculations are underway. To ' 'validate our criterion, we calculated the exfoliation energy ' 'of the suggested layered materials, and we found that in ' '88.9% of the cases the currently accepted criterion for ' 'exfoliation was satisfied. Also, using molybdenum telluride ' 'as a test case, we performed X-ray diffraction and Raman ' 'scattering experiments to benchmark our calculations and ' 'understand their applicability and limitations. The data is ' 'publicly available at the website ' 'http://www.ctcms.nist.gov/{\\t' 'extasciitilde}knc6/JVASP.html.},\\n' 'issn={2045-2322},\\n' 'doi={10.1038/s41598-017-05402-0},\\n' 'url={https://doi.org/10.1038/s41598-017-05402-0}\\n' '}', '@misc{choudhary__2018, title={jdft_2d-7-7-2018.json}, ' 'url={https://figshare.com/articles/jdft_2d-7-7-2018_json/6815705/1}, ' 'DOI={10.6084/m9.figshare.6815705.v1}, abstractNote={2D ' 'materials}, publisher={figshare}, author={choudhary, kamal ' 'and https://orcid.org/0000-0001-9737-8074}, year={2018}, ' 'month={Jul}}'], 'columns': {'exfoliation_en': 'Target variable. Exfoliation energy ' '(meV/atom).', 'structure': 'Pymatgen Structure of the material.'}, 'description': 'Matbench v0.1 test dataset for predicting exfoliation ' 'energies from crystal structure (computed with the OptB88vdW ' 'and TBmBJ functionals). Adapted from the JARVIS DFT database. ' 'For benchmarking w/ nested cross validation, the order of the ' 'dataset must be identical to the retrieved data; refer to the ' 'Automatminer/Matbench publication for more details.', 'file_type': 'json.gz', 'hash': '26057dc4524e193e32abffb296ce819b58b6e11d1278cae329a2f97817a4eddf', 'input_type': 'structure', 'mad': 67.20200406491116, 'n_samples': 636, 'num_entries': 636, 'reference': '2D Dataset discussed in:\\n' 'High-throughput Identification and Characterization of Two ' 'dimensional Materials using Density functional theory Kamal ' 'Choudhary, Irina Kalish, Ryan Beams & Francesca Tavazza ' 'Scientific Reports volume 7, Article number: 5179 (2017)\\n' 'Original 2D Data file sourced from:\\n' 'choudhary, kamal; https://orcid.org/0000-0001-9737-8074 (2018): ' 'jdft_2d-7-7-2018.json. figshare. Dataset.', 'target': 'exfoliation_en', 'task_type': 'regression', 'unit': 'meV/atom', 'url': 'https://ml.materialsproject.org/projects/matbench_jdft2d.json.gz'}","title":"Metadata"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_log_gvrh/","text":"matbench_v0.1 matbench_log_gvrh Individual Task Leaderboard for matbench_log_gvrh Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark. Leaderboard algorithm mean mae std mae mean rmse max max_error fictitious_compound 0.0393 0.0008 0.0757 1.0887 fictitious_model 0.0675 0.0006 0.1053 1.1515 ALIGNN 0.0715 0.0006 0.1123 1.1324 MODNet (v0.1.10) 0.0731 0.0007 0.1103 1.1745 AMMExpress v2020 0.0874 0.0020 0.1277 1.1580 CGCNN v2019 0.0895 0.0016 0.1337 1.4520 CrabNet 0.1014 0.0017 0.1604 2.4220 RF-SCM/Magpie 0.1040 0.0016 0.1540 1.6942 Dummy 0.2931 0.0031 0.3716 1.5552 Dataset info Description Matbench v0.1 test dataset for predicting DFT log10 VRH-average shear modulus from structure. Adapted from Materials Project database. Removed entries having a formation energy (or energy above the convex hull) more than 150meV and those having negative G_Voigt, G_Reuss, G_VRH, K_Voigt, K_Reuss, or K_VRH and those failing G_Reuss <= G_VRH <= G_Voigt or K_Reuss <= K_VRH <= K_Voigt and those containing noble gases. Retrieved April 2, 2019. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 10987 Task type: regression Input type: structure Dataset columns log10(G_VRH): Target variable. Base 10 logarithm of the DFT Voigt-Reuss-Hill average shear moduli in GPa structure: Pymatgen Structure of the material. Dataset reference Jong, M. De, Chen, W., Angsten, T., Jain, A., Notestine, R., Gamst, A., Sluiter, M., Ande, C. K., Zwaag, S. Van Der, Plata, J. J., Toher, C., Curtarolo, S., Ceder, G., Persson, K. and Asta, M., \"Charting the complete elastic properties of inorganic crystalline compounds\", Scientific Data volume 2, Article number: 150009 (2015) Metadata {'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@Article{deJong2015,\\n' 'author={de Jong, Maarten and Chen, Wei and Angsten, Thomas\\n' 'and Jain, Anubhav and Notestine, Randy and Gamst, Anthony\\n' 'and Sluiter, Marcel and Krishna Ande, Chaitanya\\n' 'and van der Zwaag, Sybrand and Plata, Jose J. and Toher, ' 'Cormac\\n' 'and Curtarolo, Stefano and Ceder, Gerbrand and Persson, ' 'Kristin A.\\n' 'and Asta, Mark},\\n' 'title={Charting the complete elastic properties\\n' 'of inorganic crystalline compounds},\\n' 'journal={Scientific Data},\\n' 'year={2015},\\n' 'month={Mar},\\n' 'day={17},\\n' 'publisher={The Author(s)},\\n' 'volume={2},\\n' 'pages={150009},\\n' 'note={Data Descriptor},\\n' 'url={http://dx.doi.org/10.1038/sdata.2015.9}\\n' '}'], 'columns': {'log10(G_VRH)': 'Target variable. Base 10 logarithm of the DFT ' 'Voigt-Reuss-Hill average shear moduli in GPa', 'structure': 'Pymatgen Structure of the material.'}, 'description': 'Matbench v0.1 test dataset for predicting DFT log10 ' 'VRH-average shear modulus from structure. Adapted from ' 'Materials Project database. Removed entries having a ' 'formation energy (or energy above the convex hull) more than ' '150meV and those having negative G_Voigt, G_Reuss, G_VRH, ' 'K_Voigt, K_Reuss, or K_VRH and those failing G_Reuss <= G_VRH ' '<= G_Voigt or K_Reuss <= K_VRH <= K_Voigt and those ' 'containing noble gases. Retrieved April 2, 2019. For ' 'benchmarking w/ nested cross validation, the order of the ' 'dataset must be identical to the retrieved data; refer to the ' 'Automatminer/Matbench publication for more details.', 'file_type': 'json.gz', 'hash': '098af941f4c663270f1fe21abf20ffad6fb85ecbfcba5786ceac03983ac29da7', 'input_type': 'structure', 'mad': 0.29313828328604646, 'n_samples': 10987, 'num_entries': 10987, 'reference': 'Jong, M. De, Chen, W., Angsten, T., Jain, A., Notestine, R., ' 'Gamst,\\n' 'A., Sluiter, M., Ande, C. K., Zwaag, S. Van Der, Plata, J. J., ' 'Toher,\\n' 'C., Curtarolo, S., Ceder, G., Persson, K. and Asta, M., ' '\"Charting\\n' 'the complete elastic properties of inorganic crystalline ' 'compounds\",\\n' 'Scientific Data volume 2, Article number: 150009 (2015)', 'target': 'log10(G_VRH)', 'task_type': 'regression', 'unit': 'log10(GPa)', 'url': 'https://ml.materialsproject.org/projects/matbench_log_gvrh.json.gz'}","title":"matbench_v0.1 matbench_log_gvrh"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_log_gvrh/#matbench_v01-matbench_log_gvrh","text":"","title":"matbench_v0.1 matbench_log_gvrh"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_log_gvrh/#individual-task-leaderboard-for-matbench_log_gvrh","text":"Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark.","title":"Individual Task Leaderboard for matbench_log_gvrh"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_log_gvrh/#leaderboard","text":"algorithm mean mae std mae mean rmse max max_error fictitious_compound 0.0393 0.0008 0.0757 1.0887 fictitious_model 0.0675 0.0006 0.1053 1.1515 ALIGNN 0.0715 0.0006 0.1123 1.1324 MODNet (v0.1.10) 0.0731 0.0007 0.1103 1.1745 AMMExpress v2020 0.0874 0.0020 0.1277 1.1580 CGCNN v2019 0.0895 0.0016 0.1337 1.4520 CrabNet 0.1014 0.0017 0.1604 2.4220 RF-SCM/Magpie 0.1040 0.0016 0.1540 1.6942 Dummy 0.2931 0.0031 0.3716 1.5552","title":"Leaderboard"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_log_gvrh/#dataset-info","text":"","title":"Dataset info"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_log_gvrh/#description","text":"Matbench v0.1 test dataset for predicting DFT log10 VRH-average shear modulus from structure. Adapted from Materials Project database. Removed entries having a formation energy (or energy above the convex hull) more than 150meV and those having negative G_Voigt, G_Reuss, G_VRH, K_Voigt, K_Reuss, or K_VRH and those failing G_Reuss <= G_VRH <= G_Voigt or K_Reuss <= K_VRH <= K_Voigt and those containing noble gases. Retrieved April 2, 2019. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 10987 Task type: regression Input type: structure","title":"Description"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_log_gvrh/#dataset-columns","text":"log10(G_VRH): Target variable. Base 10 logarithm of the DFT Voigt-Reuss-Hill average shear moduli in GPa structure: Pymatgen Structure of the material.","title":"Dataset columns"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_log_gvrh/#dataset-reference","text":"Jong, M. De, Chen, W., Angsten, T., Jain, A., Notestine, R., Gamst, A., Sluiter, M., Ande, C. K., Zwaag, S. Van Der, Plata, J. J., Toher, C., Curtarolo, S., Ceder, G., Persson, K. and Asta, M., \"Charting the complete elastic properties of inorganic crystalline compounds\", Scientific Data volume 2, Article number: 150009 (2015)","title":"Dataset reference"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_log_gvrh/#metadata","text":"{'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@Article{deJong2015,\\n' 'author={de Jong, Maarten and Chen, Wei and Angsten, Thomas\\n' 'and Jain, Anubhav and Notestine, Randy and Gamst, Anthony\\n' 'and Sluiter, Marcel and Krishna Ande, Chaitanya\\n' 'and van der Zwaag, Sybrand and Plata, Jose J. and Toher, ' 'Cormac\\n' 'and Curtarolo, Stefano and Ceder, Gerbrand and Persson, ' 'Kristin A.\\n' 'and Asta, Mark},\\n' 'title={Charting the complete elastic properties\\n' 'of inorganic crystalline compounds},\\n' 'journal={Scientific Data},\\n' 'year={2015},\\n' 'month={Mar},\\n' 'day={17},\\n' 'publisher={The Author(s)},\\n' 'volume={2},\\n' 'pages={150009},\\n' 'note={Data Descriptor},\\n' 'url={http://dx.doi.org/10.1038/sdata.2015.9}\\n' '}'], 'columns': {'log10(G_VRH)': 'Target variable. Base 10 logarithm of the DFT ' 'Voigt-Reuss-Hill average shear moduli in GPa', 'structure': 'Pymatgen Structure of the material.'}, 'description': 'Matbench v0.1 test dataset for predicting DFT log10 ' 'VRH-average shear modulus from structure. Adapted from ' 'Materials Project database. Removed entries having a ' 'formation energy (or energy above the convex hull) more than ' '150meV and those having negative G_Voigt, G_Reuss, G_VRH, ' 'K_Voigt, K_Reuss, or K_VRH and those failing G_Reuss <= G_VRH ' '<= G_Voigt or K_Reuss <= K_VRH <= K_Voigt and those ' 'containing noble gases. Retrieved April 2, 2019. For ' 'benchmarking w/ nested cross validation, the order of the ' 'dataset must be identical to the retrieved data; refer to the ' 'Automatminer/Matbench publication for more details.', 'file_type': 'json.gz', 'hash': '098af941f4c663270f1fe21abf20ffad6fb85ecbfcba5786ceac03983ac29da7', 'input_type': 'structure', 'mad': 0.29313828328604646, 'n_samples': 10987, 'num_entries': 10987, 'reference': 'Jong, M. De, Chen, W., Angsten, T., Jain, A., Notestine, R., ' 'Gamst,\\n' 'A., Sluiter, M., Ande, C. K., Zwaag, S. Van Der, Plata, J. J., ' 'Toher,\\n' 'C., Curtarolo, S., Ceder, G., Persson, K. and Asta, M., ' '\"Charting\\n' 'the complete elastic properties of inorganic crystalline ' 'compounds\",\\n' 'Scientific Data volume 2, Article number: 150009 (2015)', 'target': 'log10(G_VRH)', 'task_type': 'regression', 'unit': 'log10(GPa)', 'url': 'https://ml.materialsproject.org/projects/matbench_log_gvrh.json.gz'}","title":"Metadata"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_log_kvrh/","text":"matbench_v0.1 matbench_log_kvrh Individual Task Leaderboard for matbench_log_kvrh Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark. Leaderboard algorithm mean mae std mae mean rmse max max_error fictitious_compound 0.0302 0.0019 0.0773 1.4823 fictitious_model 0.0514 0.0025 0.1012 1.5820 MODNet (v0.1.10) 0.0548 0.0025 0.1043 1.5366 ALIGNN 0.0568 0.0028 0.1106 1.6438 AMMExpress v2020 0.0647 0.0015 0.1183 1.4823 CGCNN v2019 0.0712 0.0028 0.1301 1.7725 CrabNet 0.0758 0.0034 0.1471 1.8430 RF-SCM/Magpie 0.0820 0.0027 0.1454 1.7642 Dummy 0.2897 0.0043 0.3693 1.8822 Dataset info Description Matbench v0.1 test dataset for predicting DFT log10 VRH-average bulk modulus from structure. Adapted from Materials Project database. Removed entries having a formation energy (or energy above the convex hull) more than 150meV and those having negative G_Voigt, G_Reuss, G_VRH, K_Voigt, K_Reuss, or K_VRH and those failing G_Reuss <= G_VRH <= G_Voigt or K_Reuss <= K_VRH <= K_Voigt and those containing noble gases. Retrieved April 2, 2019. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 10987 Task type: regression Input type: structure Dataset columns log10(K_VRH): Target variable. Base 10 logarithm of the DFT Voigt-Reuss-Hill average bulk moduli in GPa. structure: Pymatgen Structure of the material. Dataset reference Jong, M. De, Chen, W., Angsten, T., Jain, A., Notestine, R., Gamst, A., Sluiter, M., Ande, C. K., Zwaag, S. Van Der, Plata, J. J., Toher, C., Curtarolo, S., Ceder, G., Persson, K. and Asta, M., \"Charting the complete elastic properties of inorganic crystalline compounds\", Scientific Data volume 2, Article number: 150009 (2015) Metadata {'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@Article{deJong2015,\\n' 'author={de Jong, Maarten and Chen, Wei and Angsten, Thomas\\n' 'and Jain, Anubhav and Notestine, Randy and Gamst, Anthony\\n' 'and Sluiter, Marcel and Krishna Ande, Chaitanya\\n' 'and van der Zwaag, Sybrand and Plata, Jose J. and Toher, ' 'Cormac\\n' 'and Curtarolo, Stefano and Ceder, Gerbrand and Persson, ' 'Kristin A.\\n' 'and Asta, Mark},\\n' 'title={Charting the complete elastic properties\\n' 'of inorganic crystalline compounds},\\n' 'journal={Scientific Data},\\n' 'year={2015},\\n' 'month={Mar},\\n' 'day={17},\\n' 'publisher={The Author(s)},\\n' 'volume={2},\\n' 'pages={150009},\\n' 'note={Data Descriptor},\\n' 'url={http://dx.doi.org/10.1038/sdata.2015.9}\\n' '}'], 'columns': {'log10(K_VRH)': 'Target variable. Base 10 logarithm of the DFT ' 'Voigt-Reuss-Hill average bulk moduli in GPa.', 'structure': 'Pymatgen Structure of the material.'}, 'description': 'Matbench v0.1 test dataset for predicting DFT log10 ' 'VRH-average bulk modulus from structure. Adapted from ' 'Materials Project database. Removed entries having a ' 'formation energy (or energy above the convex hull) more than ' '150meV and those having negative G_Voigt, G_Reuss, G_VRH, ' 'K_Voigt, K_Reuss, or K_VRH and those failing G_Reuss <= G_VRH ' '<= G_Voigt or K_Reuss <= K_VRH <= K_Voigt and those ' 'containing noble gases. Retrieved April 2, 2019. For ' 'benchmarking w/ nested cross validation, the order of the ' 'dataset must be identical to the retrieved data; refer to the ' 'Automatminer/Matbench publication for more details.', 'file_type': 'json.gz', 'hash': '44b113ddb7e23aa18731a62c74afa7e5aa654199e0db5f951c8248a00955c9cd', 'input_type': 'structure', 'mad': 0.2896736342937069, 'n_samples': 10987, 'num_entries': 10987, 'reference': 'Jong, M. De, Chen, W., Angsten, T., Jain, A., Notestine, R., ' 'Gamst,\\n' 'A., Sluiter, M., Ande, C. K., Zwaag, S. Van Der, Plata, J. J., ' 'Toher,\\n' 'C., Curtarolo, S., Ceder, G., Persson, K. and Asta, M., ' '\"Charting\\n' 'the complete elastic properties of inorganic crystalline ' 'compounds\",\\n' 'Scientific Data volume 2, Article number: 150009 (2015)', 'target': 'log10(K_VRH)', 'task_type': 'regression', 'unit': 'log10(GPa)', 'url': 'https://ml.materialsproject.org/projects/matbench_log_kvrh.json.gz'}","title":"matbench_v0.1 matbench_log_kvrh"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_log_kvrh/#matbench_v01-matbench_log_kvrh","text":"","title":"matbench_v0.1 matbench_log_kvrh"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_log_kvrh/#individual-task-leaderboard-for-matbench_log_kvrh","text":"Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark.","title":"Individual Task Leaderboard for matbench_log_kvrh"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_log_kvrh/#leaderboard","text":"algorithm mean mae std mae mean rmse max max_error fictitious_compound 0.0302 0.0019 0.0773 1.4823 fictitious_model 0.0514 0.0025 0.1012 1.5820 MODNet (v0.1.10) 0.0548 0.0025 0.1043 1.5366 ALIGNN 0.0568 0.0028 0.1106 1.6438 AMMExpress v2020 0.0647 0.0015 0.1183 1.4823 CGCNN v2019 0.0712 0.0028 0.1301 1.7725 CrabNet 0.0758 0.0034 0.1471 1.8430 RF-SCM/Magpie 0.0820 0.0027 0.1454 1.7642 Dummy 0.2897 0.0043 0.3693 1.8822","title":"Leaderboard"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_log_kvrh/#dataset-info","text":"","title":"Dataset info"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_log_kvrh/#description","text":"Matbench v0.1 test dataset for predicting DFT log10 VRH-average bulk modulus from structure. Adapted from Materials Project database. Removed entries having a formation energy (or energy above the convex hull) more than 150meV and those having negative G_Voigt, G_Reuss, G_VRH, K_Voigt, K_Reuss, or K_VRH and those failing G_Reuss <= G_VRH <= G_Voigt or K_Reuss <= K_VRH <= K_Voigt and those containing noble gases. Retrieved April 2, 2019. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 10987 Task type: regression Input type: structure","title":"Description"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_log_kvrh/#dataset-columns","text":"log10(K_VRH): Target variable. Base 10 logarithm of the DFT Voigt-Reuss-Hill average bulk moduli in GPa. structure: Pymatgen Structure of the material.","title":"Dataset columns"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_log_kvrh/#dataset-reference","text":"Jong, M. De, Chen, W., Angsten, T., Jain, A., Notestine, R., Gamst, A., Sluiter, M., Ande, C. K., Zwaag, S. Van Der, Plata, J. J., Toher, C., Curtarolo, S., Ceder, G., Persson, K. and Asta, M., \"Charting the complete elastic properties of inorganic crystalline compounds\", Scientific Data volume 2, Article number: 150009 (2015)","title":"Dataset reference"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_log_kvrh/#metadata","text":"{'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@Article{deJong2015,\\n' 'author={de Jong, Maarten and Chen, Wei and Angsten, Thomas\\n' 'and Jain, Anubhav and Notestine, Randy and Gamst, Anthony\\n' 'and Sluiter, Marcel and Krishna Ande, Chaitanya\\n' 'and van der Zwaag, Sybrand and Plata, Jose J. and Toher, ' 'Cormac\\n' 'and Curtarolo, Stefano and Ceder, Gerbrand and Persson, ' 'Kristin A.\\n' 'and Asta, Mark},\\n' 'title={Charting the complete elastic properties\\n' 'of inorganic crystalline compounds},\\n' 'journal={Scientific Data},\\n' 'year={2015},\\n' 'month={Mar},\\n' 'day={17},\\n' 'publisher={The Author(s)},\\n' 'volume={2},\\n' 'pages={150009},\\n' 'note={Data Descriptor},\\n' 'url={http://dx.doi.org/10.1038/sdata.2015.9}\\n' '}'], 'columns': {'log10(K_VRH)': 'Target variable. Base 10 logarithm of the DFT ' 'Voigt-Reuss-Hill average bulk moduli in GPa.', 'structure': 'Pymatgen Structure of the material.'}, 'description': 'Matbench v0.1 test dataset for predicting DFT log10 ' 'VRH-average bulk modulus from structure. Adapted from ' 'Materials Project database. Removed entries having a ' 'formation energy (or energy above the convex hull) more than ' '150meV and those having negative G_Voigt, G_Reuss, G_VRH, ' 'K_Voigt, K_Reuss, or K_VRH and those failing G_Reuss <= G_VRH ' '<= G_Voigt or K_Reuss <= K_VRH <= K_Voigt and those ' 'containing noble gases. Retrieved April 2, 2019. For ' 'benchmarking w/ nested cross validation, the order of the ' 'dataset must be identical to the retrieved data; refer to the ' 'Automatminer/Matbench publication for more details.', 'file_type': 'json.gz', 'hash': '44b113ddb7e23aa18731a62c74afa7e5aa654199e0db5f951c8248a00955c9cd', 'input_type': 'structure', 'mad': 0.2896736342937069, 'n_samples': 10987, 'num_entries': 10987, 'reference': 'Jong, M. De, Chen, W., Angsten, T., Jain, A., Notestine, R., ' 'Gamst,\\n' 'A., Sluiter, M., Ande, C. K., Zwaag, S. Van Der, Plata, J. J., ' 'Toher,\\n' 'C., Curtarolo, S., Ceder, G., Persson, K. and Asta, M., ' '\"Charting\\n' 'the complete elastic properties of inorganic crystalline ' 'compounds\",\\n' 'Scientific Data volume 2, Article number: 150009 (2015)', 'target': 'log10(K_VRH)', 'task_type': 'regression', 'unit': 'log10(GPa)', 'url': 'https://ml.materialsproject.org/projects/matbench_log_kvrh.json.gz'}","title":"Metadata"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_e_form/","text":"matbench_v0.1 matbench_mp_e_form Individual Task Leaderboard for matbench_mp_e_form Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark. Leaderboard algorithm mean mae std mae mean rmse max max_error fictitious_compound 0.0135 0.0003 0.0378 3.5487 ALIGNN 0.0215 0.0005 0.0544 3.5487 fictitious_model 0.0215 0.0004 0.0541 3.5983 CGCNN v2019 0.0337 0.0006 0.0682 7.7205 MODNet (v0.1.10) 0.0448 0.0039 0.0888 4.8803 CrabNet 0.0862 0.0010 0.2544 6.3774 RF-SCM/Magpie 0.1165 0.0008 0.2419 5.4382 AMMExpress v2020 0.1726 0.0270 0.2602 5.8108 Dummy 1.0059 0.0030 1.1631 3.9096 Dataset info Description Matbench v0.1 test dataset for predicting DFT formation energy from structure. Adapted from Materials Project database. Removed entries having formation energy more than 2.5eV and those containing noble gases. Retrieved April 2, 2019. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 132752 Task type: regression Input type: structure Dataset columns e_form: Target variable. Formation energy in eV as calculated by the Materials Project. structure: Pymatgen Structure of the material. Dataset reference A. Jain*, S.P. Ong*, G. Hautier, W. Chen, W.D. Richards, S. Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, K.A. Persson (*=equal contributions) The Materials Project: A materials genome approach to accelerating materials innovation APL Materials, 2013, 1(1), 011002. doi:10.1063/1.4812323 Metadata {'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@article{Jain2013,\\n' 'author = {Jain, Anubhav and Ong, Shyue Ping and Hautier, ' 'Geoffroy and Chen, Wei and Richards, William Davidson and ' 'Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and ' 'Skinner, David and Ceder, Gerbrand and Persson, Kristin ' 'a.},\\n' 'doi = {10.1063/1.4812323},\\n' 'issn = {2166532X},\\n' 'journal = {APL Materials},\\n' 'number = {1},\\n' 'pages = {011002},\\n' 'title = {{The Materials Project: A materials genome approach ' 'to accelerating materials innovation}},\\n' 'url = ' '{http://link.aip.org/link/AMPADS/v1/i1/p011002/s1\\\\&Agg=doi},\\n' 'volume = {1},\\n' 'year = {2013}\\n' '}'], 'columns': {'e_form': 'Target variable. Formation energy in eV as calculated ' 'by the Materials Project.', 'structure': 'Pymatgen Structure of the material.'}, 'description': 'Matbench v0.1 test dataset for predicting DFT formation ' 'energy from structure. Adapted from Materials Project ' 'database. Removed entries having formation energy more than ' '2.5eV and those containing noble gases. Retrieved April 2, ' '2019. For benchmarking w/ nested cross validation, the order ' 'of the dataset must be identical to the retrieved data; refer ' 'to the Automatminer/Matbench publication for more details.', 'file_type': 'json.gz', 'hash': 'dedcb1d4ba2e3e50dbdd45ba5bc647a00e9c2bcf8f8bf556dc8e92caa39eb21f', 'input_type': 'structure', 'mad': 1.0059220443295362, 'n_samples': 132752, 'num_entries': 132752, 'reference': 'A. Jain*, S.P. Ong*, G. Hautier, W. Chen, W.D. Richards, S. ' 'Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, K.A. Persson ' '(*=equal contributions)\\n' 'The Materials Project: A materials genome approach to ' 'accelerating materials innovation\\n' 'APL Materials, 2013, 1(1), 011002.\\n' 'doi:10.1063/1.4812323', 'target': 'e_form', 'task_type': 'regression', 'unit': 'eV/atom', 'url': 'https://ml.materialsproject.org/projects/matbench_mp_e_form.json.gz'}","title":"matbench_v0.1 matbench_mp_e_form"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_e_form/#matbench_v01-matbench_mp_e_form","text":"","title":"matbench_v0.1 matbench_mp_e_form"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_e_form/#individual-task-leaderboard-for-matbench_mp_e_form","text":"Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark.","title":"Individual Task Leaderboard for matbench_mp_e_form"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_e_form/#leaderboard","text":"algorithm mean mae std mae mean rmse max max_error fictitious_compound 0.0135 0.0003 0.0378 3.5487 ALIGNN 0.0215 0.0005 0.0544 3.5487 fictitious_model 0.0215 0.0004 0.0541 3.5983 CGCNN v2019 0.0337 0.0006 0.0682 7.7205 MODNet (v0.1.10) 0.0448 0.0039 0.0888 4.8803 CrabNet 0.0862 0.0010 0.2544 6.3774 RF-SCM/Magpie 0.1165 0.0008 0.2419 5.4382 AMMExpress v2020 0.1726 0.0270 0.2602 5.8108 Dummy 1.0059 0.0030 1.1631 3.9096","title":"Leaderboard"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_e_form/#dataset-info","text":"","title":"Dataset info"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_e_form/#description","text":"Matbench v0.1 test dataset for predicting DFT formation energy from structure. Adapted from Materials Project database. Removed entries having formation energy more than 2.5eV and those containing noble gases. Retrieved April 2, 2019. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 132752 Task type: regression Input type: structure","title":"Description"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_e_form/#dataset-columns","text":"e_form: Target variable. Formation energy in eV as calculated by the Materials Project. structure: Pymatgen Structure of the material.","title":"Dataset columns"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_e_form/#dataset-reference","text":"A. Jain*, S.P. Ong*, G. Hautier, W. Chen, W.D. Richards, S. Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, K.A. Persson (*=equal contributions) The Materials Project: A materials genome approach to accelerating materials innovation APL Materials, 2013, 1(1), 011002. doi:10.1063/1.4812323","title":"Dataset reference"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_e_form/#metadata","text":"{'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@article{Jain2013,\\n' 'author = {Jain, Anubhav and Ong, Shyue Ping and Hautier, ' 'Geoffroy and Chen, Wei and Richards, William Davidson and ' 'Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and ' 'Skinner, David and Ceder, Gerbrand and Persson, Kristin ' 'a.},\\n' 'doi = {10.1063/1.4812323},\\n' 'issn = {2166532X},\\n' 'journal = {APL Materials},\\n' 'number = {1},\\n' 'pages = {011002},\\n' 'title = {{The Materials Project: A materials genome approach ' 'to accelerating materials innovation}},\\n' 'url = ' '{http://link.aip.org/link/AMPADS/v1/i1/p011002/s1\\\\&Agg=doi},\\n' 'volume = {1},\\n' 'year = {2013}\\n' '}'], 'columns': {'e_form': 'Target variable. Formation energy in eV as calculated ' 'by the Materials Project.', 'structure': 'Pymatgen Structure of the material.'}, 'description': 'Matbench v0.1 test dataset for predicting DFT formation ' 'energy from structure. Adapted from Materials Project ' 'database. Removed entries having formation energy more than ' '2.5eV and those containing noble gases. Retrieved April 2, ' '2019. For benchmarking w/ nested cross validation, the order ' 'of the dataset must be identical to the retrieved data; refer ' 'to the Automatminer/Matbench publication for more details.', 'file_type': 'json.gz', 'hash': 'dedcb1d4ba2e3e50dbdd45ba5bc647a00e9c2bcf8f8bf556dc8e92caa39eb21f', 'input_type': 'structure', 'mad': 1.0059220443295362, 'n_samples': 132752, 'num_entries': 132752, 'reference': 'A. Jain*, S.P. Ong*, G. Hautier, W. Chen, W.D. Richards, S. ' 'Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, K.A. Persson ' '(*=equal contributions)\\n' 'The Materials Project: A materials genome approach to ' 'accelerating materials innovation\\n' 'APL Materials, 2013, 1(1), 011002.\\n' 'doi:10.1063/1.4812323', 'target': 'e_form', 'task_type': 'regression', 'unit': 'eV/atom', 'url': 'https://ml.materialsproject.org/projects/matbench_mp_e_form.json.gz'}","title":"Metadata"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_gap/","text":"matbench_v0.1 matbench_mp_gap Individual Task Leaderboard for matbench_mp_gap Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark. Leaderboard algorithm mean mae std mae mean rmse max max_error fictitious_compound 0.0787 0.0014 0.2118 4.9652 fictitious_model 0.1781 0.0018 0.4076 6.3716 ALIGNN 0.1861 0.0030 0.4635 7.4756 MODNet (v0.1.10) 0.2199 0.0059 0.4525 7.5685 CrabNet 0.2655 0.0029 0.5898 7.9829 AMMExpress v2020 0.2824 0.0061 0.5611 6.9105 CGCNN v2019 0.2972 0.0035 0.6771 13.6569 RF-SCM/Magpie 0.3452 0.0033 0.6125 7.0601 Dummy 1.3272 0.0060 1.5989 8.5092 Dataset info Description Matbench v0.1 test dataset for predicting DFT PBE band gap from structure. Adapted from Materials Project database. Removed entries having a formation energy (or energy above the convex hull) more than 150meV and those containing noble gases. Retrieved April 2, 2019. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 106113 Task type: regression Input type: structure Dataset columns gap pbe: Target variable. The band gap as calculated by PBE DFT from the Materials Project, in eV. structure: Pymatgen Structure of the material. Dataset reference A. Jain*, S.P. Ong*, G. Hautier, W. Chen, W.D. Richards, S. Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, K.A. Persson (*=equal contributions) The Materials Project: A materials genome approach to accelerating materials innovation APL Materials, 2013, 1(1), 011002. doi:10.1063/1.4812323 Metadata {'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@article{Jain2013,\\n' 'author = {Jain, Anubhav and Ong, Shyue Ping and Hautier, ' 'Geoffroy and Chen, Wei and Richards, William Davidson and ' 'Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and ' 'Skinner, David and Ceder, Gerbrand and Persson, Kristin ' 'a.},\\n' 'doi = {10.1063/1.4812323},\\n' 'issn = {2166532X},\\n' 'journal = {APL Materials},\\n' 'number = {1},\\n' 'pages = {011002},\\n' 'title = {{The Materials Project: A materials genome approach ' 'to accelerating materials innovation}},\\n' 'url = ' '{http://link.aip.org/link/AMPADS/v1/i1/p011002/s1\\\\&Agg=doi},\\n' 'volume = {1},\\n' 'year = {2013}\\n' '}'], 'columns': {'gap pbe': 'Target variable. The band gap as calculated by PBE ' 'DFT from the Materials Project, in eV.', 'structure': 'Pymatgen Structure of the material.'}, 'description': 'Matbench v0.1 test dataset for predicting DFT PBE band gap ' 'from structure. Adapted from Materials Project database. ' 'Removed entries having a formation energy (or energy above ' 'the convex hull) more than 150meV and those containing noble ' 'gases. Retrieved April 2, 2019. For benchmarking w/ nested ' 'cross validation, the order of the dataset must be identical ' 'to the retrieved data; refer to the Automatminer/Matbench ' 'publication for more details.', 'file_type': 'json.gz', 'hash': '58b65746bd88329986ed66031a2ac1369c7c522f7bc9f9081528e07097c2c057', 'input_type': 'structure', 'mad': 1.3271449960162496, 'n_samples': 106113, 'num_entries': 106113, 'reference': 'A. Jain*, S.P. Ong*, G. Hautier, W. Chen, W.D. Richards, S. ' 'Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, K.A. Persson ' '(*=equal contributions)\\n' 'The Materials Project: A materials genome approach to ' 'accelerating materials innovation\\n' 'APL Materials, 2013, 1(1), 011002.\\n' 'doi:10.1063/1.4812323', 'target': 'gap pbe', 'task_type': 'regression', 'unit': 'eV', 'url': 'https://ml.materialsproject.org/projects/matbench_mp_gap.json.gz'}","title":"matbench_v0.1 matbench_mp_gap"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_gap/#matbench_v01-matbench_mp_gap","text":"","title":"matbench_v0.1 matbench_mp_gap"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_gap/#individual-task-leaderboard-for-matbench_mp_gap","text":"Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark.","title":"Individual Task Leaderboard for matbench_mp_gap"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_gap/#leaderboard","text":"algorithm mean mae std mae mean rmse max max_error fictitious_compound 0.0787 0.0014 0.2118 4.9652 fictitious_model 0.1781 0.0018 0.4076 6.3716 ALIGNN 0.1861 0.0030 0.4635 7.4756 MODNet (v0.1.10) 0.2199 0.0059 0.4525 7.5685 CrabNet 0.2655 0.0029 0.5898 7.9829 AMMExpress v2020 0.2824 0.0061 0.5611 6.9105 CGCNN v2019 0.2972 0.0035 0.6771 13.6569 RF-SCM/Magpie 0.3452 0.0033 0.6125 7.0601 Dummy 1.3272 0.0060 1.5989 8.5092","title":"Leaderboard"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_gap/#dataset-info","text":"","title":"Dataset info"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_gap/#description","text":"Matbench v0.1 test dataset for predicting DFT PBE band gap from structure. Adapted from Materials Project database. Removed entries having a formation energy (or energy above the convex hull) more than 150meV and those containing noble gases. Retrieved April 2, 2019. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 106113 Task type: regression Input type: structure","title":"Description"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_gap/#dataset-columns","text":"gap pbe: Target variable. The band gap as calculated by PBE DFT from the Materials Project, in eV. structure: Pymatgen Structure of the material.","title":"Dataset columns"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_gap/#dataset-reference","text":"A. Jain*, S.P. Ong*, G. Hautier, W. Chen, W.D. Richards, S. Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, K.A. Persson (*=equal contributions) The Materials Project: A materials genome approach to accelerating materials innovation APL Materials, 2013, 1(1), 011002. doi:10.1063/1.4812323","title":"Dataset reference"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_gap/#metadata","text":"{'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@article{Jain2013,\\n' 'author = {Jain, Anubhav and Ong, Shyue Ping and Hautier, ' 'Geoffroy and Chen, Wei and Richards, William Davidson and ' 'Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and ' 'Skinner, David and Ceder, Gerbrand and Persson, Kristin ' 'a.},\\n' 'doi = {10.1063/1.4812323},\\n' 'issn = {2166532X},\\n' 'journal = {APL Materials},\\n' 'number = {1},\\n' 'pages = {011002},\\n' 'title = {{The Materials Project: A materials genome approach ' 'to accelerating materials innovation}},\\n' 'url = ' '{http://link.aip.org/link/AMPADS/v1/i1/p011002/s1\\\\&Agg=doi},\\n' 'volume = {1},\\n' 'year = {2013}\\n' '}'], 'columns': {'gap pbe': 'Target variable. The band gap as calculated by PBE ' 'DFT from the Materials Project, in eV.', 'structure': 'Pymatgen Structure of the material.'}, 'description': 'Matbench v0.1 test dataset for predicting DFT PBE band gap ' 'from structure. Adapted from Materials Project database. ' 'Removed entries having a formation energy (or energy above ' 'the convex hull) more than 150meV and those containing noble ' 'gases. Retrieved April 2, 2019. For benchmarking w/ nested ' 'cross validation, the order of the dataset must be identical ' 'to the retrieved data; refer to the Automatminer/Matbench ' 'publication for more details.', 'file_type': 'json.gz', 'hash': '58b65746bd88329986ed66031a2ac1369c7c522f7bc9f9081528e07097c2c057', 'input_type': 'structure', 'mad': 1.3271449960162496, 'n_samples': 106113, 'num_entries': 106113, 'reference': 'A. Jain*, S.P. Ong*, G. Hautier, W. Chen, W.D. Richards, S. ' 'Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, K.A. Persson ' '(*=equal contributions)\\n' 'The Materials Project: A materials genome approach to ' 'accelerating materials innovation\\n' 'APL Materials, 2013, 1(1), 011002.\\n' 'doi:10.1063/1.4812323', 'target': 'gap pbe', 'task_type': 'regression', 'unit': 'eV', 'url': 'https://ml.materialsproject.org/projects/matbench_mp_gap.json.gz'}","title":"Metadata"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_is_metal/","text":"matbench_v0.1 matbench_mp_is_metal Individual Task Leaderboard for matbench_mp_is_metal Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark. Leaderboard algorithm mean rocauc std rocauc mean f1 mean balanced_accuracy fictitious_compound 0.9627 0.0031 0.9597 0.9627 CGCNN v2019 0.9520 0.0074 0.9462 0.9520 ALIGNN 0.9128 0.0015 0.9015 0.9128 fictitious_model 0.9126 0.0016 0.9015 0.9126 AMMExpress v2020 0.9093 0.0008 0.8981 0.9093 RF-SCM/Magpie 0.8992 0.0019 0.8866 0.8992 MODNet (v0.1.10) 0.7805 0.1406 0.6621 0.7805 Dummy 0.5012 0.0043 0.4353 0.5012 Dataset info Description Matbench v0.1 test dataset for predicting DFT metallicity from structure. Adapted from Materials Project database. Removed entries having a formation energy (or energy above the convex hull) more than 150meV and those containing noble gases. Retrieved April 2, 2019. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 106113 Task type: classification Input type: structure Dataset columns is_metal: Target variable. 1 if the compound is a metal, 0 if the compound is not a metal. Metallicity determined with pymatgen structure: Pymatgen Structure of the material. Dataset reference A. Jain*, S.P. Ong*, G. Hautier, W. Chen, W.D. Richards, S. Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, K.A. Persson (*=equal contributions) The Materials Project: A materials genome approach to accelerating materials innovation APL Materials, 2013, 1(1), 011002. doi:10.1063/1.4812323 Metadata {'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@article{Jain2013,\\n' 'author = {Jain, Anubhav and Ong, Shyue Ping and Hautier, ' 'Geoffroy and Chen, Wei and Richards, William Davidson and ' 'Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and ' 'Skinner, David and Ceder, Gerbrand and Persson, Kristin ' 'a.},\\n' 'doi = {10.1063/1.4812323},\\n' 'issn = {2166532X},\\n' 'journal = {APL Materials},\\n' 'number = {1},\\n' 'pages = {011002},\\n' 'title = {{The Materials Project: A materials genome approach ' 'to accelerating materials innovation}},\\n' 'url = ' '{http://link.aip.org/link/AMPADS/v1/i1/p011002/s1\\\\&Agg=doi},\\n' 'volume = {1},\\n' 'year = {2013}\\n' '}'], 'columns': {'is_metal': 'Target variable. 1 if the compound is a metal, 0 if ' 'the compound is not a metal. Metallicity determined ' 'with pymatgen', 'structure': 'Pymatgen Structure of the material.'}, 'description': 'Matbench v0.1 test dataset for predicting DFT metallicity ' 'from structure. Adapted from Materials Project database. ' 'Removed entries having a formation energy (or energy above ' 'the convex hull) more than 150meV and those containing noble ' 'gases. Retrieved April 2, 2019. For benchmarking w/ nested ' 'cross validation, the order of the dataset must be identical ' 'to the retrieved data; refer to the Automatminer/Matbench ' 'publication for more details.', 'file_type': 'json.gz', 'frac_true': 0.43492314796490533, 'hash': '9a028ed5750a4c76ca36e9f3c8d48fe0bf3fb21b76ec2289e58ae7048d527919', 'input_type': 'structure', 'n_samples': 106113, 'num_entries': 106113, 'reference': 'A. Jain*, S.P. Ong*, G. Hautier, W. Chen, W.D. Richards, S. ' 'Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, K.A. Persson ' '(*=equal contributions)\\n' 'The Materials Project: A materials genome approach to ' 'accelerating materials innovation\\n' 'APL Materials, 2013, 1(1), 011002.\\n' 'doi:10.1063/1.4812323', 'target': 'is_metal', 'task_type': 'classification', 'unit': None, 'url': 'https://ml.materialsproject.org/projects/matbench_mp_is_metal.json.gz'}","title":"matbench_v0.1 matbench_mp_is_metal"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_is_metal/#matbench_v01-matbench_mp_is_metal","text":"","title":"matbench_v0.1 matbench_mp_is_metal"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_is_metal/#individual-task-leaderboard-for-matbench_mp_is_metal","text":"Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark.","title":"Individual Task Leaderboard for matbench_mp_is_metal"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_is_metal/#leaderboard","text":"algorithm mean rocauc std rocauc mean f1 mean balanced_accuracy fictitious_compound 0.9627 0.0031 0.9597 0.9627 CGCNN v2019 0.9520 0.0074 0.9462 0.9520 ALIGNN 0.9128 0.0015 0.9015 0.9128 fictitious_model 0.9126 0.0016 0.9015 0.9126 AMMExpress v2020 0.9093 0.0008 0.8981 0.9093 RF-SCM/Magpie 0.8992 0.0019 0.8866 0.8992 MODNet (v0.1.10) 0.7805 0.1406 0.6621 0.7805 Dummy 0.5012 0.0043 0.4353 0.5012","title":"Leaderboard"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_is_metal/#dataset-info","text":"","title":"Dataset info"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_is_metal/#description","text":"Matbench v0.1 test dataset for predicting DFT metallicity from structure. Adapted from Materials Project database. Removed entries having a formation energy (or energy above the convex hull) more than 150meV and those containing noble gases. Retrieved April 2, 2019. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 106113 Task type: classification Input type: structure","title":"Description"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_is_metal/#dataset-columns","text":"is_metal: Target variable. 1 if the compound is a metal, 0 if the compound is not a metal. Metallicity determined with pymatgen structure: Pymatgen Structure of the material.","title":"Dataset columns"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_is_metal/#dataset-reference","text":"A. Jain*, S.P. Ong*, G. Hautier, W. Chen, W.D. Richards, S. Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, K.A. Persson (*=equal contributions) The Materials Project: A materials genome approach to accelerating materials innovation APL Materials, 2013, 1(1), 011002. doi:10.1063/1.4812323","title":"Dataset reference"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_mp_is_metal/#metadata","text":"{'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@article{Jain2013,\\n' 'author = {Jain, Anubhav and Ong, Shyue Ping and Hautier, ' 'Geoffroy and Chen, Wei and Richards, William Davidson and ' 'Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and ' 'Skinner, David and Ceder, Gerbrand and Persson, Kristin ' 'a.},\\n' 'doi = {10.1063/1.4812323},\\n' 'issn = {2166532X},\\n' 'journal = {APL Materials},\\n' 'number = {1},\\n' 'pages = {011002},\\n' 'title = {{The Materials Project: A materials genome approach ' 'to accelerating materials innovation}},\\n' 'url = ' '{http://link.aip.org/link/AMPADS/v1/i1/p011002/s1\\\\&Agg=doi},\\n' 'volume = {1},\\n' 'year = {2013}\\n' '}'], 'columns': {'is_metal': 'Target variable. 1 if the compound is a metal, 0 if ' 'the compound is not a metal. Metallicity determined ' 'with pymatgen', 'structure': 'Pymatgen Structure of the material.'}, 'description': 'Matbench v0.1 test dataset for predicting DFT metallicity ' 'from structure. Adapted from Materials Project database. ' 'Removed entries having a formation energy (or energy above ' 'the convex hull) more than 150meV and those containing noble ' 'gases. Retrieved April 2, 2019. For benchmarking w/ nested ' 'cross validation, the order of the dataset must be identical ' 'to the retrieved data; refer to the Automatminer/Matbench ' 'publication for more details.', 'file_type': 'json.gz', 'frac_true': 0.43492314796490533, 'hash': '9a028ed5750a4c76ca36e9f3c8d48fe0bf3fb21b76ec2289e58ae7048d527919', 'input_type': 'structure', 'n_samples': 106113, 'num_entries': 106113, 'reference': 'A. Jain*, S.P. Ong*, G. Hautier, W. Chen, W.D. Richards, S. ' 'Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, K.A. Persson ' '(*=equal contributions)\\n' 'The Materials Project: A materials genome approach to ' 'accelerating materials innovation\\n' 'APL Materials, 2013, 1(1), 011002.\\n' 'doi:10.1063/1.4812323', 'target': 'is_metal', 'task_type': 'classification', 'unit': None, 'url': 'https://ml.materialsproject.org/projects/matbench_mp_is_metal.json.gz'}","title":"Metadata"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_perovskites/","text":"matbench_v0.1 matbench_perovskites Individual Task Leaderboard for matbench_perovskites Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark. Leaderboard algorithm mean mae std mae mean rmse max max_error fictitious_compound 0.0218 0.0010 0.0416 0.7967 ALIGNN 0.0288 0.0009 0.0559 0.9028 fictitious_model 0.0307 0.0013 0.0572 0.9146 CGCNN v2019 0.0452 0.0007 0.0722 0.9923 MODNet (v0.1.10) 0.0908 0.0028 0.1277 1.1780 AMMExpress v2020 0.2005 0.0085 0.2954 3.3116 RF-SCM/Magpie 0.2355 0.0034 0.3346 2.8870 CrabNet 0.4065 0.0069 0.5412 2.3726 Dummy 0.5660 0.0048 0.7424 3.6873 Dataset info Description Matbench v0.1 test dataset for predicting formation energy from crystal structure. Adapted from an original dataset generated by Castelli et al. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 18928 Task type: regression Input type: structure Dataset columns e_form: Target variable. Heat of formation of the entire 5-atom perovskite cell, in eV as calculated by RPBE GGA-DFT. Note the reference state for oxygen was computed from oxygen's chemical potential in water vapor, not as oxygen molecules, to reflect the application which these perovskites were studied for. structure: Pymatgen Structure of the material. Dataset reference Ivano E. Castelli, David D. Landis, Kristian S. Thygesen, S\u00f8ren Dahl, Ib Chorkendorff, Thomas F. Jaramillo and Karsten W. Jacobsen (2012) New cubic perovskites for one- and two-photon water splitting using the computational materials repository. Energy Environ. Sci., 2012,5, 9034-9043 https://doi.org/10.1039/C2EE22341D Metadata {'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@Article{C2EE22341D,\\n' 'author =\"Castelli, Ivano E. and Landis, David D. and ' 'Thygesen, Kristian S. and Dahl, S\u00f8ren and Chorkendorff, Ib ' 'and Jaramillo, Thomas F. and Jacobsen, Karsten W.\",\\n' 'title =\"New cubic perovskites for one- and two-photon water ' 'splitting using the computational materials repository\",\\n' 'journal =\"Energy Environ. Sci.\",\\n' 'year =\"2012\",\\n' 'volume =\"5\",\\n' 'issue =\"10\",\\n' 'pages =\"9034-9043\",\\n' 'publisher =\"The Royal Society of Chemistry\",\\n' 'doi =\"10.1039/C2EE22341D\",\\n' 'url =\"http://dx.doi.org/10.1039/C2EE22341D\",\\n' 'abstract =\"A new efficient photoelectrochemical cell (PEC) ' 'is one of the possible solutions to the energy and climate ' 'problems of our time. Such a device requires development of ' 'new semiconducting materials with tailored properties with ' 'respect to stability and light absorption. Here we perform ' 'computational screening of around 19\\u2009000 oxides{,} ' 'oxynitrides{,} oxysulfides{,} oxyfluorides{,} and ' 'oxyfluoronitrides in the cubic perovskite structure with PEC ' 'applications in mind. We address three main applications: ' 'light absorbers for one- and two-photon water splitting and ' 'high-stability transparent shields to protect against ' 'corrosion. We end up with 20{,} 12{,} and 15 different ' 'combinations of oxides{,} oxynitrides and oxyfluorides{,} ' 'respectively{,} inviting further experimental ' 'investigation.\"}'], 'columns': {'e_form': 'Target variable. Heat of formation of the entire ' '5-atom perovskite cell, in eV as calculated by RPBE ' 'GGA-DFT. Note the reference state for oxygen was ' \"computed from oxygen's chemical potential in water \" 'vapor, not as oxygen molecules, to reflect the ' 'application which these perovskites were studied for.', 'structure': 'Pymatgen Structure of the material.'}, 'description': 'Matbench v0.1 test dataset for predicting formation energy ' 'from crystal structure. Adapted from an original dataset ' 'generated by Castelli et al. For benchmarking w/ nested cross ' 'validation, the order of the dataset must be identical to the ' 'retrieved data; refer to the Automatminer/Matbench ' 'publication for more details.', 'file_type': 'json.gz', 'hash': '4641e2417f8ec8b50096d2230864468dfa08278dc9d257c327f65d0305278483', 'input_type': 'structure', 'mad': 0.5659924184827462, 'n_samples': 18928, 'num_entries': 18928, 'reference': 'Ivano E. Castelli, David D. Landis, Kristian S. Thygesen, S\u00f8ren ' 'Dahl, Ib Chorkendorff, Thomas F. Jaramillo and Karsten W. ' 'Jacobsen (2012) New cubic perovskites for one- and two-photon ' 'water splitting using the computational materials repository. ' 'Energy Environ. Sci., 2012,5, 9034-9043 ' 'https://doi.org/10.1039/C2EE22341D', 'target': 'e_form', 'task_type': 'regression', 'unit': 'eV/unit cell', 'url': 'https://ml.materialsproject.org/projects/matbench_perovskites.json.gz'}","title":"matbench_v0.1 matbench_perovskites"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_perovskites/#matbench_v01-matbench_perovskites","text":"","title":"matbench_v0.1 matbench_perovskites"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_perovskites/#individual-task-leaderboard-for-matbench_perovskites","text":"Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark.","title":"Individual Task Leaderboard for matbench_perovskites"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_perovskites/#leaderboard","text":"algorithm mean mae std mae mean rmse max max_error fictitious_compound 0.0218 0.0010 0.0416 0.7967 ALIGNN 0.0288 0.0009 0.0559 0.9028 fictitious_model 0.0307 0.0013 0.0572 0.9146 CGCNN v2019 0.0452 0.0007 0.0722 0.9923 MODNet (v0.1.10) 0.0908 0.0028 0.1277 1.1780 AMMExpress v2020 0.2005 0.0085 0.2954 3.3116 RF-SCM/Magpie 0.2355 0.0034 0.3346 2.8870 CrabNet 0.4065 0.0069 0.5412 2.3726 Dummy 0.5660 0.0048 0.7424 3.6873","title":"Leaderboard"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_perovskites/#dataset-info","text":"","title":"Dataset info"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_perovskites/#description","text":"Matbench v0.1 test dataset for predicting formation energy from crystal structure. Adapted from an original dataset generated by Castelli et al. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 18928 Task type: regression Input type: structure","title":"Description"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_perovskites/#dataset-columns","text":"e_form: Target variable. Heat of formation of the entire 5-atom perovskite cell, in eV as calculated by RPBE GGA-DFT. Note the reference state for oxygen was computed from oxygen's chemical potential in water vapor, not as oxygen molecules, to reflect the application which these perovskites were studied for. structure: Pymatgen Structure of the material.","title":"Dataset columns"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_perovskites/#dataset-reference","text":"Ivano E. Castelli, David D. Landis, Kristian S. Thygesen, S\u00f8ren Dahl, Ib Chorkendorff, Thomas F. Jaramillo and Karsten W. Jacobsen (2012) New cubic perovskites for one- and two-photon water splitting using the computational materials repository. Energy Environ. Sci., 2012,5, 9034-9043 https://doi.org/10.1039/C2EE22341D","title":"Dataset reference"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_perovskites/#metadata","text":"{'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@Article{C2EE22341D,\\n' 'author =\"Castelli, Ivano E. and Landis, David D. and ' 'Thygesen, Kristian S. and Dahl, S\u00f8ren and Chorkendorff, Ib ' 'and Jaramillo, Thomas F. and Jacobsen, Karsten W.\",\\n' 'title =\"New cubic perovskites for one- and two-photon water ' 'splitting using the computational materials repository\",\\n' 'journal =\"Energy Environ. Sci.\",\\n' 'year =\"2012\",\\n' 'volume =\"5\",\\n' 'issue =\"10\",\\n' 'pages =\"9034-9043\",\\n' 'publisher =\"The Royal Society of Chemistry\",\\n' 'doi =\"10.1039/C2EE22341D\",\\n' 'url =\"http://dx.doi.org/10.1039/C2EE22341D\",\\n' 'abstract =\"A new efficient photoelectrochemical cell (PEC) ' 'is one of the possible solutions to the energy and climate ' 'problems of our time. Such a device requires development of ' 'new semiconducting materials with tailored properties with ' 'respect to stability and light absorption. Here we perform ' 'computational screening of around 19\\u2009000 oxides{,} ' 'oxynitrides{,} oxysulfides{,} oxyfluorides{,} and ' 'oxyfluoronitrides in the cubic perovskite structure with PEC ' 'applications in mind. We address three main applications: ' 'light absorbers for one- and two-photon water splitting and ' 'high-stability transparent shields to protect against ' 'corrosion. We end up with 20{,} 12{,} and 15 different ' 'combinations of oxides{,} oxynitrides and oxyfluorides{,} ' 'respectively{,} inviting further experimental ' 'investigation.\"}'], 'columns': {'e_form': 'Target variable. Heat of formation of the entire ' '5-atom perovskite cell, in eV as calculated by RPBE ' 'GGA-DFT. Note the reference state for oxygen was ' \"computed from oxygen's chemical potential in water \" 'vapor, not as oxygen molecules, to reflect the ' 'application which these perovskites were studied for.', 'structure': 'Pymatgen Structure of the material.'}, 'description': 'Matbench v0.1 test dataset for predicting formation energy ' 'from crystal structure. Adapted from an original dataset ' 'generated by Castelli et al. For benchmarking w/ nested cross ' 'validation, the order of the dataset must be identical to the ' 'retrieved data; refer to the Automatminer/Matbench ' 'publication for more details.', 'file_type': 'json.gz', 'hash': '4641e2417f8ec8b50096d2230864468dfa08278dc9d257c327f65d0305278483', 'input_type': 'structure', 'mad': 0.5659924184827462, 'n_samples': 18928, 'num_entries': 18928, 'reference': 'Ivano E. Castelli, David D. Landis, Kristian S. Thygesen, S\u00f8ren ' 'Dahl, Ib Chorkendorff, Thomas F. Jaramillo and Karsten W. ' 'Jacobsen (2012) New cubic perovskites for one- and two-photon ' 'water splitting using the computational materials repository. ' 'Energy Environ. Sci., 2012,5, 9034-9043 ' 'https://doi.org/10.1039/C2EE22341D', 'target': 'e_form', 'task_type': 'regression', 'unit': 'eV/unit cell', 'url': 'https://ml.materialsproject.org/projects/matbench_perovskites.json.gz'}","title":"Metadata"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_phonons/","text":"matbench_v0.1 matbench_phonons Individual Task Leaderboard for matbench_phonons Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark. Leaderboard algorithm mean mae std mae mean rmse max max_error fictitious_compound 13.6984 0.8867 29.4214 394.6285 fictitious_model 27.8584 2.5164 53.6576 629.1442 ALIGNN 29.5385 2.1148 53.5010 615.3466 MODNet (v0.1.10) 38.7524 1.7732 78.2220 1031.8168 CrabNet 55.1114 5.7317 138.3775 1452.7562 AMMExpress v2020 56.1706 6.7981 109.7048 1151.5570 CGCNN v2019 57.7635 12.3109 141.7018 2504.8743 RF-SCM/Magpie 67.6126 8.9900 146.2764 2024.7301 Dummy 323.9822 17.7269 492.1533 3062.3450 Dataset info Description Matbench v0.1 test dataset for predicting vibration properties from crystal structure. Original data retrieved from Petretto et al. Original calculations done via ABINIT in the harmonic approximation based on density functional perturbation theory. Removed entries having a formation energy (or energy above the convex hull) more than 150meV. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 1265 Task type: regression Input type: structure Dataset columns last phdos peak: Target variable. Frequency of the highest frequency optical phonon mode peak, in units of 1/cm; ; may be used as an estimation of dominant longitudinal optical phonon frequency. structure: Pymatgen Structure of the material. Dataset reference Petretto, G. et al. High-throughput density functional perturbation theory phonons for inorganic materials. Sci. Data 5:180065 doi: 10.1038/sdata.2018.65 (2018). Petretto, G. et al. High-throughput density functional perturbation theory phonons for inorganic materials. (2018). figshare. Collection. Metadata {'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@Article{Petretto2018,\\n' 'author={Petretto, Guido\\n' 'and Dwaraknath, Shyam\\n' 'and P.C. Miranda, Henrique\\n' 'and Winston, Donald\\n' 'and Giantomassi, Matteo\\n' 'and van Setten, Michiel J.\\n' 'and Gonze, Xavier\\n' 'and Persson, Kristin A.\\n' 'and Hautier, Geoffroy\\n' 'and Rignanese, Gian-Marco},\\n' 'title={High-throughput density-functional perturbation ' 'theory phonons for inorganic materials},\\n' 'journal={Scientific Data},\\n' 'year={2018},\\n' 'month={May},\\n' 'day={01},\\n' 'publisher={The Author(s)},\\n' 'volume={5},\\n' 'pages={180065},\\n' 'note={Data Descriptor},\\n' 'url={http://dx.doi.org/10.1038/sdata.2018.65}\\n' '}', '@misc{petretto_dwaraknath_miranda_winston_giantomassi_rignanese_van ' 'setten_gonze_persson_hautier_2018, title={High-throughput ' 'Density-Functional Perturbation Theory phonons for inorganic ' 'materials}, ' 'url={https://figshare.com/collections/High-throughput_Density-Functional_Perturbation_Theory_phonons_for_inorganic_materials/3938023/1}, ' 'DOI={10.6084/m9.figshare.c.3938023.v1}, abstractNote={The ' 'knowledge of the vibrational properties of a material is of ' 'key importance to understand physical phenomena such as ' 'thermal conductivity, superconductivity, and ' 'ferroelectricity among others. However, detailed ' 'experimental phonon spectra are available only for a limited ' 'number of materials which hinders the large-scale analysis ' 'of vibrational properties and their derived quantities. In ' 'this work, we perform ab initio calculations of the full ' 'phonon dispersion and vibrational density of states for 1521 ' 'semiconductor compounds in the harmonic approximation based ' 'on density functional perturbation theory. The data is ' 'collected along with derived dielectric and thermodynamic ' 'properties. We present the procedure used to obtain the ' 'results, the details of the provided database and a ' 'validation based on the comparison with experimental data.}, ' 'publisher={figshare}, author={Petretto, Guido and ' 'Dwaraknath, Shyam and Miranda, Henrique P. C. and Winston, ' 'Donald and Giantomassi, Matteo and Rignanese, Gian-Marco and ' 'Van Setten, Michiel J. and Gonze, Xavier and Persson, ' 'Kristin A and Hautier, Geoffroy}, year={2018}, month={Apr}}'], 'columns': {'last phdos peak': 'Target variable. Frequency of the highest ' 'frequency optical phonon mode peak, in units ' 'of 1/cm; ; may be used as an estimation of ' 'dominant longitudinal optical phonon ' 'frequency.', 'structure': 'Pymatgen Structure of the material.'}, 'description': 'Matbench v0.1 test dataset for predicting vibration ' 'properties from crystal structure. Original data retrieved ' 'from Petretto et al. Original calculations done via ABINIT in ' 'the harmonic approximation based on density functional ' 'perturbation theory. Removed entries having a formation ' 'energy (or energy above the convex hull) more than 150meV. ' 'For benchmarking w/ nested cross validation, the order of the ' 'dataset must be identical to the retrieved data; refer to the ' 'Automatminer/Matbench publication for more details.', 'file_type': 'json.gz', 'hash': '4db551f21ec5f577e6202725f10e34dfc509aa7df3a6bdaac497da7f6dbbb9b3', 'input_type': 'structure', 'mad': 323.78696979348734, 'n_samples': 1265, 'num_entries': 1265, 'reference': 'Petretto, G. et al. High-throughput density functional ' 'perturbation theory phonons for inorganic materials. Sci. Data ' '5:180065 doi: 10.1038/sdata.2018.65 (2018).\\n' 'Petretto, G. et al. High-throughput density functional ' 'perturbation theory phonons for inorganic materials. (2018). ' 'figshare. Collection.', 'target': 'last phdos peak', 'task_type': 'regression', 'unit': 'cm^-1', 'url': 'https://ml.materialsproject.org/projects/matbench_phonons.json.gz'}","title":"matbench_v0.1 matbench_phonons"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_phonons/#matbench_v01-matbench_phonons","text":"","title":"matbench_v0.1 matbench_phonons"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_phonons/#individual-task-leaderboard-for-matbench_phonons","text":"Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark.","title":"Individual Task Leaderboard for matbench_phonons"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_phonons/#leaderboard","text":"algorithm mean mae std mae mean rmse max max_error fictitious_compound 13.6984 0.8867 29.4214 394.6285 fictitious_model 27.8584 2.5164 53.6576 629.1442 ALIGNN 29.5385 2.1148 53.5010 615.3466 MODNet (v0.1.10) 38.7524 1.7732 78.2220 1031.8168 CrabNet 55.1114 5.7317 138.3775 1452.7562 AMMExpress v2020 56.1706 6.7981 109.7048 1151.5570 CGCNN v2019 57.7635 12.3109 141.7018 2504.8743 RF-SCM/Magpie 67.6126 8.9900 146.2764 2024.7301 Dummy 323.9822 17.7269 492.1533 3062.3450","title":"Leaderboard"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_phonons/#dataset-info","text":"","title":"Dataset info"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_phonons/#description","text":"Matbench v0.1 test dataset for predicting vibration properties from crystal structure. Original data retrieved from Petretto et al. Original calculations done via ABINIT in the harmonic approximation based on density functional perturbation theory. Removed entries having a formation energy (or energy above the convex hull) more than 150meV. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 1265 Task type: regression Input type: structure","title":"Description"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_phonons/#dataset-columns","text":"last phdos peak: Target variable. Frequency of the highest frequency optical phonon mode peak, in units of 1/cm; ; may be used as an estimation of dominant longitudinal optical phonon frequency. structure: Pymatgen Structure of the material.","title":"Dataset columns"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_phonons/#dataset-reference","text":"Petretto, G. et al. High-throughput density functional perturbation theory phonons for inorganic materials. Sci. Data 5:180065 doi: 10.1038/sdata.2018.65 (2018). Petretto, G. et al. High-throughput density functional perturbation theory phonons for inorganic materials. (2018). figshare. Collection.","title":"Dataset reference"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_phonons/#metadata","text":"{'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@Article{Petretto2018,\\n' 'author={Petretto, Guido\\n' 'and Dwaraknath, Shyam\\n' 'and P.C. Miranda, Henrique\\n' 'and Winston, Donald\\n' 'and Giantomassi, Matteo\\n' 'and van Setten, Michiel J.\\n' 'and Gonze, Xavier\\n' 'and Persson, Kristin A.\\n' 'and Hautier, Geoffroy\\n' 'and Rignanese, Gian-Marco},\\n' 'title={High-throughput density-functional perturbation ' 'theory phonons for inorganic materials},\\n' 'journal={Scientific Data},\\n' 'year={2018},\\n' 'month={May},\\n' 'day={01},\\n' 'publisher={The Author(s)},\\n' 'volume={5},\\n' 'pages={180065},\\n' 'note={Data Descriptor},\\n' 'url={http://dx.doi.org/10.1038/sdata.2018.65}\\n' '}', '@misc{petretto_dwaraknath_miranda_winston_giantomassi_rignanese_van ' 'setten_gonze_persson_hautier_2018, title={High-throughput ' 'Density-Functional Perturbation Theory phonons for inorganic ' 'materials}, ' 'url={https://figshare.com/collections/High-throughput_Density-Functional_Perturbation_Theory_phonons_for_inorganic_materials/3938023/1}, ' 'DOI={10.6084/m9.figshare.c.3938023.v1}, abstractNote={The ' 'knowledge of the vibrational properties of a material is of ' 'key importance to understand physical phenomena such as ' 'thermal conductivity, superconductivity, and ' 'ferroelectricity among others. However, detailed ' 'experimental phonon spectra are available only for a limited ' 'number of materials which hinders the large-scale analysis ' 'of vibrational properties and their derived quantities. In ' 'this work, we perform ab initio calculations of the full ' 'phonon dispersion and vibrational density of states for 1521 ' 'semiconductor compounds in the harmonic approximation based ' 'on density functional perturbation theory. The data is ' 'collected along with derived dielectric and thermodynamic ' 'properties. We present the procedure used to obtain the ' 'results, the details of the provided database and a ' 'validation based on the comparison with experimental data.}, ' 'publisher={figshare}, author={Petretto, Guido and ' 'Dwaraknath, Shyam and Miranda, Henrique P. C. and Winston, ' 'Donald and Giantomassi, Matteo and Rignanese, Gian-Marco and ' 'Van Setten, Michiel J. and Gonze, Xavier and Persson, ' 'Kristin A and Hautier, Geoffroy}, year={2018}, month={Apr}}'], 'columns': {'last phdos peak': 'Target variable. Frequency of the highest ' 'frequency optical phonon mode peak, in units ' 'of 1/cm; ; may be used as an estimation of ' 'dominant longitudinal optical phonon ' 'frequency.', 'structure': 'Pymatgen Structure of the material.'}, 'description': 'Matbench v0.1 test dataset for predicting vibration ' 'properties from crystal structure. Original data retrieved ' 'from Petretto et al. Original calculations done via ABINIT in ' 'the harmonic approximation based on density functional ' 'perturbation theory. Removed entries having a formation ' 'energy (or energy above the convex hull) more than 150meV. ' 'For benchmarking w/ nested cross validation, the order of the ' 'dataset must be identical to the retrieved data; refer to the ' 'Automatminer/Matbench publication for more details.', 'file_type': 'json.gz', 'hash': '4db551f21ec5f577e6202725f10e34dfc509aa7df3a6bdaac497da7f6dbbb9b3', 'input_type': 'structure', 'mad': 323.78696979348734, 'n_samples': 1265, 'num_entries': 1265, 'reference': 'Petretto, G. et al. High-throughput density functional ' 'perturbation theory phonons for inorganic materials. Sci. Data ' '5:180065 doi: 10.1038/sdata.2018.65 (2018).\\n' 'Petretto, G. et al. High-throughput density functional ' 'perturbation theory phonons for inorganic materials. (2018). ' 'figshare. Collection.', 'target': 'last phdos peak', 'task_type': 'regression', 'unit': 'cm^-1', 'url': 'https://ml.materialsproject.org/projects/matbench_phonons.json.gz'}","title":"Metadata"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_steels/","text":"matbench_v0.1 matbench_steels Individual Task Leaderboard for matbench_steels Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark. Leaderboard algorithm mean mae std mae mean rmse max max_error fictitious_compound 53.3729 6.3355 88.7840 553.2987 fictitious_model 83.5116 9.7961 128.0854 846.0036 MODNet (v0.1.10) 96.2139 9.8352 149.9535 931.3261 AMMExpress v2020 97.4929 13.7919 154.0161 1142.9223 RF-SCM/Magpie 103.5125 11.0368 149.3839 1121.1276 CrabNet 107.3160 18.9057 153.0041 576.3912 Dummy 229.7445 9.6958 301.2211 1088.0568 Dataset info Description Matbench v0.1 test dataset for predicting steel yield strengths from chemical composition alone. Retrieved from Citrine informatics. Deduplicated. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 312 Task type: regression Input type: composition Dataset columns composition: Chemical formula. yield strength: Target variable. Experimentally measured steel yield strengths, in MPa. Dataset reference https://citrination.com/datasets/153092/ Metadata {'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@misc{Citrine Informatics,\\n' 'title = {Mechanical properties of some steels},\\n' 'howpublished = ' '{\\\\url{https://citrination.com/datasets/153092/},\\n' '}'], 'columns': {'composition': 'Chemical formula.', 'yield strength': 'Target variable. Experimentally measured steel ' 'yield strengths, in MPa.'}, 'description': 'Matbench v0.1 test dataset for predicting steel yield ' 'strengths from chemical composition alone. Retrieved from ' 'Citrine informatics. Deduplicated. For benchmarking w/ nested ' 'cross validation, the order of the dataset must be identical ' 'to the retrieved data; refer to the Automatminer/Matbench ' 'publication for more details.', 'file_type': 'json.gz', 'hash': '473bc4957b2ea5e6465aef84bc29bb48ac34db27d69ea4ec5f508745c6fae252', 'input_type': 'composition', 'mad': 229.37426857330706, 'n_samples': 312, 'num_entries': 312, 'reference': 'https://citrination.com/datasets/153092/', 'target': 'yield strength', 'task_type': 'regression', 'unit': 'MPa', 'url': 'https://ml.materialsproject.org/projects/matbench_steels.json.gz'}","title":"matbench_v0.1 matbench_steels"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_steels/#matbench_v01-matbench_steels","text":"","title":"matbench_v0.1 matbench_steels"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_steels/#individual-task-leaderboard-for-matbench_steels","text":"Leaderboard for an individual task. Algorithms shown here may include both general purpose and specialized algorithms (i.e., algorithms which are only valid for a subset of tasks in the benchmark.","title":"Individual Task Leaderboard for matbench_steels"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_steels/#leaderboard","text":"algorithm mean mae std mae mean rmse max max_error fictitious_compound 53.3729 6.3355 88.7840 553.2987 fictitious_model 83.5116 9.7961 128.0854 846.0036 MODNet (v0.1.10) 96.2139 9.8352 149.9535 931.3261 AMMExpress v2020 97.4929 13.7919 154.0161 1142.9223 RF-SCM/Magpie 103.5125 11.0368 149.3839 1121.1276 CrabNet 107.3160 18.9057 153.0041 576.3912 Dummy 229.7445 9.6958 301.2211 1088.0568","title":"Leaderboard"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_steels/#dataset-info","text":"","title":"Dataset info"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_steels/#description","text":"Matbench v0.1 test dataset for predicting steel yield strengths from chemical composition alone. Retrieved from Citrine informatics. Deduplicated. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details. Number of samples: 312 Task type: regression Input type: composition","title":"Description"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_steels/#dataset-columns","text":"composition: Chemical formula. yield strength: Target variable. Experimentally measured steel yield strengths, in MPa.","title":"Dataset columns"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_steels/#dataset-reference","text":"https://citrination.com/datasets/153092/","title":"Dataset reference"},{"location":"Leaderboards%20Per-Task/matbench_v0.1_matbench_steels/#metadata","text":"{'bibtex_refs': ['@Article{Dunn2020,\\n' 'author={Dunn, Alexander\\n' 'and Wang, Qi\\n' 'and Ganose, Alex\\n' 'and Dopp, Daniel\\n' 'and Jain, Anubhav},\\n' 'title={Benchmarking materials property prediction methods: ' 'the Matbench test set and Automatminer reference ' 'algorithm},\\n' 'journal={npj Computational Materials},\\n' 'year={2020},\\n' 'month={Sep},\\n' 'day={15},\\n' 'volume={6},\\n' 'number={1},\\n' 'pages={138},\\n' 'abstract={We present a benchmark test suite and an automated ' 'machine learning procedure for evaluating supervised machine ' 'learning (ML) models for predicting properties of inorganic ' 'bulk materials. The test suite, Matbench, is a set of ' '13{\\\\thinspace}ML tasks that range in size from 312 to 132k ' 'samples and contain data from 10 density functional ' 'theory-derived and experimental sources. Tasks include ' 'predicting optical, thermal, electronic, thermodynamic, ' \"tensile, and elastic properties given a material's \" 'composition and/or crystal structure. The reference ' 'algorithm, Automatminer, is a highly-extensible, fully ' 'automated ML pipeline for predicting materials properties ' 'from materials primitives (such as composition and crystal ' 'structure) without user intervention or hyperparameter ' 'tuning. We test Automatminer on the Matbench test suite and ' 'compare its predictive power with state-of-the-art crystal ' 'graph neural networks and a traditional descriptor-based ' 'Random Forest model. We find Automatminer achieves the best ' 'performance on 8 of 13 tasks in the benchmark. We also show ' 'our test suite is capable of exposing predictive advantages ' 'of each algorithm---namely, that crystal graph methods ' 'appear to outperform traditional machine learning methods ' 'given {\\\\textasciitilde}104 or greater data points. We ' 'encourage evaluating materials ML algorithms on the Matbench ' 'benchmark and comparing them against the latest version of ' 'Automatminer.},\\n' 'issn={2057-3960},\\n' 'doi={10.1038/s41524-020-00406-3},\\n' 'url={https://doi.org/10.1038/s41524-020-00406-3}\\n' '}\\n', '@misc{Citrine Informatics,\\n' 'title = {Mechanical properties of some steels},\\n' 'howpublished = ' '{\\\\url{https://citrination.com/datasets/153092/},\\n' '}'], 'columns': {'composition': 'Chemical formula.', 'yield strength': 'Target variable. Experimentally measured steel ' 'yield strengths, in MPa.'}, 'description': 'Matbench v0.1 test dataset for predicting steel yield ' 'strengths from chemical composition alone. Retrieved from ' 'Citrine informatics. Deduplicated. For benchmarking w/ nested ' 'cross validation, the order of the dataset must be identical ' 'to the retrieved data; refer to the Automatminer/Matbench ' 'publication for more details.', 'file_type': 'json.gz', 'hash': '473bc4957b2ea5e6465aef84bc29bb48ac34db27d69ea4ec5f508745c6fae252', 'input_type': 'composition', 'mad': 229.37426857330706, 'n_samples': 312, 'num_entries': 312, 'reference': 'https://citrination.com/datasets/153092/', 'target': 'yield strength', 'task_type': 'regression', 'unit': 'MPa', 'url': 'https://ml.materialsproject.org/projects/matbench_steels.json.gz'}","title":"Metadata"},{"location":"Reference/MatbenchBenchmark/","text":"MatbenchBenchmark The core class for benchmarking with Matbench. MatbenchBenchmark is capable of benchmarking and validating arbitrary materials science benchmarks. It is a container class for sets of MatbenchTasks, objects which provide predetermined sets of training/validation and testing data for any algorithm to benchmark with. MatbenchBenchmark can also give summaries of entire complex benchmarks, including access to individual score statistics for each metric. MatbenchBenchmark can run any benchmark as long as it has a corresponding benchmark name key. Matbench v0.1 (\"matbench_v0.1\") is the only benchmark currently configured for use with MatbenchBenchmark. MatbenchBenchmark is capable of running benchmark subsets; for example, only 3 of the 13 available Matbench v0.1 problems. See the documentation for more details. Attributes: Name Type Description benchmark_name str The benchmark name, defaults to the original Matbench v0.1 \"matbench_v0.1\". Should have an associated validation file in order for the MatbenchTasks to work correctly. metadata dict The corresponding metadata file for this benchmark, which defines the basic configuration for each task. See matbench_v0.1_validation for an example. Each dataset has the same required keys in order to work correctly. user_metadata dict Any metadata about the algorithm or benchmark that the user wants to keep as part of the benchmark file. tasks_map {str MatbenchTask}): A mapping of task name to the corresponding MatbenchTask object. <<task_names>> MatbenchTask Access any task obj via MatbenchTask.< >. For example: mb = MatbenchBenchmark() mb.matbench_dielectric < > Source code in matbench/bench.py class MatbenchBenchmark ( MSONable , MSONable2File ): \"\"\"The core class for benchmarking with Matbench. MatbenchBenchmark is capable of benchmarking and validating arbitrary materials science benchmarks. It is a container class for sets of MatbenchTasks, objects which provide predetermined sets of training/validation and testing data for any algorithm to benchmark with. MatbenchBenchmark can also give summaries of entire complex benchmarks, including access to individual score statistics for each metric. MatbenchBenchmark can run any benchmark as long as it has a corresponding benchmark name key. Matbench v0.1 (\"matbench_v0.1\") is the only benchmark currently configured for use with MatbenchBenchmark. MatbenchBenchmark is capable of running benchmark subsets; for example, only 3 of the 13 available Matbench v0.1 problems. See the documentation for more details. Attributes: benchmark_name (str): The benchmark name, defaults to the original Matbench v0.1 \"matbench_v0.1\". Should have an associated validation file in order for the MatbenchTasks to work correctly. metadata (dict): The corresponding metadata file for this benchmark, which defines the basic configuration for each task. See matbench_v0.1_validation for an example. Each dataset has the same required keys in order to work correctly. user_metadata (dict): Any metadata about the algorithm or benchmark that the user wants to keep as part of the benchmark file. tasks_map ({str: MatbenchTask}): A mapping of task name to the corresponding MatbenchTask object. <<task_names>> (MatbenchTask): Access any task obj via MatbenchTask.<<task_name>>. For example: mb = MatbenchBenchmark() mb.matbench_dielectric <<MatbenchTask object>> \"\"\" # For serialization _VERSION_KEY = \"version\" _BENCHMARK_KEY = \"benchmark_name\" _USER_METADATA_KEY = \"user_metadata\" _TASKS_KEY = \"tasks\" _DATESTAMP_KEY = \"datestamp\" _DATESTAMP_FMT = \"%Y.%m. %d %H:%M.%S\" _HASH_KEY = \"hash\" # For class usage ALL_KEY = \"all\" def __init__ ( self , benchmark = MBV01_KEY , autoload = False , subset = None ): \"\"\" Args: benchmark (str): The name of the benchmark. Only supported benchmark currently is \"matbench_v0.1\", though more will be added in the future. autoload (bool): If True, automatically load the dataset into memory For a full benchmark, this can take some time. If False, you'll need to load each task with .load before you can access the raw data. subset ([str]): A list of task names to use as a subset of a full benchmark. Only the named tasks will be contained in the class. Must correspond to the metadata file defined by the benchmark name. \"\"\" if benchmark == MBV01_KEY : self . benchmark_name = MBV01_KEY self . metadata = mbv01_metadata else : raise ValueError ( f \"Only ' { MBV01_KEY } ' available. No other benchmarks defined!\" ) if subset : not_datasets = [ k for k in subset if k not in self . metadata ] if not_datasets : raise KeyError ( f \"Some tasks in { subset } are not benchmark=\" f \"' { self . benchmark_name } ' datasets! Remove { not_datasets } .\" ) else : available_tasks = subset else : available_tasks = self . metadata . keys () self . user_metadata = {} self . tasks_map = RecursiveDotDict () for ds in available_tasks : self . tasks_map [ ds ] = MatbenchTask ( ds , autoload = autoload , benchmark = self . benchmark_name ) logger . info ( f \"Initialized benchmark ' { benchmark } ' \" f \"with { len ( available_tasks ) } tasks: \\n \" f \" { pprint . pformat ( list ( available_tasks )) } \" ) def __getattr__ ( self , item ): \"\"\" Enable MatbenchBenchmark.task_name behavior. Args: item (str): The name of the attr. Returns: The attr, if not in the metadata defined by the benchmark If the attr is a task name, returns that MatBenchTask object. \"\"\" if item in self . metadata : return self . tasks_map [ item ] else : return self . __getattribute__ ( item ) @classmethod def from_preset ( cls , benchmark , preset_name , autoload = False ): \"\"\" The following presets are defined for each benchmark: benchmark: 'matbench_v0.1': - preset: 'structure' - Only structure problems - preset: 'composition' - Only composition problems - preset: 'regression' - Only regression problems - preset: 'classification' - Only classification problems - preset: 'all' - All problems in matbench v0.1 Args: benchmark (str): Name of the benchmark set you'd like to use. The only supported benchmark set currently is \"matbench_v0.1\" preset_name (str): The name of the preset autoload (bool): If true, automatically loads all the datasets upon instantiation. Be warned; this can take a while. Returns: (MatbenchBenchmark object) \"\"\" if benchmark == MBV01_KEY : if preset_name == STRUCTURE_KEY : available_tasks = [ k for k , v in mbv01_metadata . items () if v . input_type == STRUCTURE_KEY ] elif preset_name == COMPOSITION_KEY : available_tasks = [ k for k , v in mbv01_metadata . items () if v . input_type == COMPOSITION_KEY ] elif preset_name == REG_KEY : available_tasks = [ k for k , v in mbv01_metadata . items () if v . task_type == REG_KEY ] elif preset_name == CLF_KEY : available_tasks = [ k for k , v in mbv01_metadata . items () if v . task_type == CLF_KEY ] elif preset_name == cls . ALL_KEY : available_tasks = [ k for k , v in mbv01_metadata . items ()] else : valid_keys = [ STRUCTURE_KEY , COMPOSITION_KEY , CLF_KEY , REG_KEY , cls . ALL_KEY , ] raise ValueError ( f \"Preset name ' { preset_name } ' not recognized for \" f \"benchmark ' { MBV01_KEY } '! Select from \" f \" { valid_keys } \" ) else : raise ValueError ( f \"Only ' { MBV01_KEY } ' available. No other benchmarks defined!\" ) return cls ( benchmark = benchmark , autoload = autoload , subset = available_tasks ) @classmethod def from_dict ( cls , d ): \"\"\"Create a MatbenchBenchmark object from a dictionary. Args: d (dict): Returns: (MatbenchBenchmark) \"\"\" required_keys = [ \"@module\" , \"@class\" , cls . _VERSION_KEY , cls . _BENCHMARK_KEY , cls . _TASKS_KEY , cls . _USER_METADATA_KEY , cls . _DATESTAMP_KEY , cls . _HASH_KEY , ] missing_keys = [] for k in required_keys : if k not in d : missing_keys . append ( k ) extra_keys = [] for k in d : if k not in required_keys : extra_keys . append ( k ) if missing_keys and not extra_keys : raise ValueError ( f \"Required keys { missing_keys } for { cls . __class__ . __name__ } \" f \"not found!\" ) elif not missing_keys and extra_keys : raise ValueError ( f \"Extra keys { extra_keys } for { cls . __class__ . __name__ } \" f \"present!\" ) elif missing_keys and extra_keys : raise ValueError ( f \"Missing required keys { missing_keys } and extra keys \" f \" { extra_keys } present!\" ) # Check all tasks to make sure their benchmark name is matching in the # benchmark and in the tasks not_matching_bench = [] for t_dict in d [ cls . _TASKS_KEY ] . values (): if t_dict [ MatbenchTask . _BENCHMARK_KEY ] != d [ cls . _BENCHMARK_KEY ]: not_matching_bench . append ( t_dict [ MatbenchTask . _DATASET_KEY ]) if not_matching_bench : raise ValueError ( f \"Tasks { not_matching_bench } do not have a benchmark name \" f \"matching the benchmark ( { d [ cls . _BENCHMARK_KEY ] } )!\" ) # Ensure the hash is matching, i.e., the data was not modified after # matbench got done with it m_from_dict = d . pop ( cls . _HASH_KEY ) m = hash_dictionary ( d ) if m != m_from_dict : raise ValueError ( f \"Hash of dictionary does not match it's reported value! { m } \" f \"!= { m_from_dict } . Was the data modified after saving?)\" ) # Check to see if any tasks have task names not matching their key # names in the benchmark not_matching_tasks = [] for task_name , task_info in d [ cls . _TASKS_KEY ] . items (): key_as_per_task = task_info [ MatbenchTask . _DATASET_KEY ] if task_name != key_as_per_task : not_matching_tasks . append (( task_name , key_as_per_task )) if not_matching_tasks : raise ValueError ( f \"Task names in benchmark and task names in tasks not \" f \"matching: { not_matching_tasks } \" ) # Warn if versions are not matching if d [ cls . _VERSION_KEY ] != VERSION : logger . warning ( f \"Warning! Versions not matching: \" f \"(data file has version { d [ cls . _VERSION_KEY ] } , \" f \"this package is { VERSION } ).\" ) return cls . _from_args ( benchmark_name = d [ cls . _BENCHMARK_KEY ], tasks_dict = d [ cls . _TASKS_KEY ], user_metadata = d [ cls . _USER_METADATA_KEY ], ) @classmethod def _from_args ( cls , benchmark_name , tasks_dict , user_metadata ): \"\"\"Create a MatbenchBenchmark object from arguments Args: benchmark_name (str): name of the benchmark tasks_dict (dict): formatted dict of task data user_metadata (dict): freeform user metadata Returns: (MatbenchBenchmark) \"\"\" subset = list ( tasks_dict . keys ()) obj = cls ( benchmark = benchmark_name , autoload = False , subset = subset ) obj . tasks_map = RecursiveDotDict ( { t_name : MatbenchTask . from_dict ( t_dict ) for t_name , t_dict in tasks_dict . items () } ) logger . warning ( \"To add new data to this benchmark, the \" \"benchmark must be loaded with .load(). Alternatively, \" \"load individual tasks with MatbenchTask.load().\" ) # MatbenchTask automatically validates files during its from_dict obj . user_metadata = user_metadata logger . debug ( f \"Successfully converted dict/args to ' { cls . __name__ } '.\" ) return obj def _determine_completeness ( self , completeness_type ): \"\"\"Determine the completeness of this benchmark. Completeness means the tasks are included (but not necessarily recorded yet) in the benchmark. Supported completeness types are: - \"all\": All tasks are included - \"composition\": All composition tasks are included - \"structure\": All structure tasks are included - \"regression\": All regression problems - \"classification\": All classification problems Args: completeness_type (str): One of the above completeness types. Returns: (bool) True if this benchmark object is complete with respect to the completeness type. \"\"\" if completeness_type == self . ALL_KEY : required_tasks = list ( self . metadata . keys ()) elif completeness_type in ( COMPOSITION_KEY , STRUCTURE_KEY ): required_tasks = [ k for k , v in self . metadata . items () if v . input_type == completeness_type ] elif completeness_type in ( REG_KEY , CLF_KEY ): required_tasks = [ k for k , v in self . metadata . items () if v . task_type == completeness_type ] else : allowed_completeness_types = [ self . ALL_KEY , COMPOSITION_KEY , STRUCTURE_KEY , REG_KEY , CLF_KEY , ] raise ValueError ( \"Only supported completeness types are \" f \" { allowed_completeness_types } \" ) for task in required_tasks : if task not in self . tasks_map : return False else : return True def as_dict ( self ): \"\"\"Overridden from MSONable.as_dict, get dict repr of this obj Returns: (dict) \"\"\" tasksd = { mbt . dataset_name : mbt . as_dict () for mbt in self . tasks } tasksd_jsonable = immutify_dictionary ( tasksd ) d = { \"@module\" : self . __class__ . __module__ , \"@class\" : self . __class__ . __name__ , self . _VERSION_KEY : VERSION , self . _TASKS_KEY : tasksd_jsonable , self . _USER_METADATA_KEY : self . user_metadata , self . _BENCHMARK_KEY : self . benchmark_name , self . _DATESTAMP_KEY : datetime . datetime . utcnow () . strftime ( self . _DATESTAMP_FMT ), } # to obtain a hash for this benchmark, immutify the dictionary # and then stringify it d [ self . _HASH_KEY ] = hash_dictionary ( d ) logger . debug ( f \"Successfully converted { self . __class__ . __name__ } to dictionary.\" ) return d def get_info ( self ): \"\"\"Log info about the benchmark to the respective logging handlers. Returns: None \"\"\" logger . info ( self . info ) def add_metadata ( self , metadata ): \"\"\"Add freeform information about this run to the object (and subsequent json), accessible thru the 'user_metadata' attr. All keys must be strings. All values must be either: a. a numpy ndarray b. python native types, such as bools, floats, ints, strs c. a pandas series d. a list/tuple of python native types (bools, floats, ints) OR e. A dictionary where all keys are strs and all values are one of a, b, c, d, or e (recursive). Args: metadata (dict) Returns: None \"\"\" # Use logging here so bad metadata addition does not # ruin an entire run... if not isinstance ( metadata , dict ): logger . critical ( f \"User metadata must be reducible to dict format, \" f \"not type( { type ( metadata ) } )\" ) logger . info ( \"User metadata not added.\" ) else : if self . user_metadata : logger . warning ( \"User metadata already exists! Overwriting...\" ) self . user_metadata = immutify_dictionary ( metadata ) logger . info ( \"User metadata added successfully!\" ) def load ( self ): \"\"\"Load all tasks in this benchmark. Returns: None \"\"\" for t in self . tasks : t . load () def validate ( self ): \"\"\"Run validation on each task in this benchmark. Returns: ({str: str}): dict of errors, if they exist \"\"\" errors = {} for t , t_obj in self . tasks_map . items (): try : t_obj . validate () except BaseException : errors [ t ] = traceback . format_exc () return errors @property def tasks ( self ): \"\"\"Return the tasks as a list. Returns: ([MatbenchTask]): A list of matbench tasks in this benchmark \"\"\" return self . tasks_map . values () @property def scores ( self ): \"\"\"Get all score metrics for all tasks as a dictionary. Returns: (RecursiveDotDict): A nested dictionary-like object of scores for each task. \"\"\" return RecursiveDotDict ({ t . dataset_name : t . scores for t in self . tasks }) @property def info ( self ): \"\"\"Get a formatted string of info about this benchmark and its current state. Returns: (str) \"\"\" complete = self . is_complete recorded = self . is_recorded valid = self . is_valid s = \"\" s += ( f \" \\n Matbench package { VERSION } running benchmark \" f \"' { self . benchmark_name } '\" ) s += f \" \\n\\t is complete: { complete } \" s += f \" \\n\\t is recorded: { recorded } \" s += f \" \\n\\t is valid: { valid } \" if not recorded : s += ( \" \\n\\n Benchmark is not fully recorded; limited information \" \"shown.\" ) if not valid : s += \" \\n\\n Benchmark is not valid; limited information shown.\" if not valid or not recorded : s += \" \\n\\n Tasks:\" for t in self . tasks_map . values (): s += f \" \\n\\t - ' { t . dataset_name } : recorded= { t . all_folds_recorded } \" if valid and recorded : s += \" \\n\\n Results:\" for t in self . tasks : if t . metadata . task_type == REG_KEY : score_text = ( f \"MAE mean: \" f \" { self . scores [ t . dataset_name ] . mae . mean } \" ) else : score_text = ( f \"ROCAUC mean: \" f \" { self . scores [ t . dataset_name ] . rocauc . mean } \" ) s += f \" \\n\\t - ' { t . dataset_name } ' { score_text } \" return s @property def is_complete ( self ): \"\"\"Determine if all available tasks are included in this benchmark. For matbench v0.1, this means all 13 tasks are in the benchmark. Returns: (bool) \"\"\" return self . _determine_completeness ( completeness_type = self . ALL_KEY ) @property def is_composition_complete ( self ): \"\"\"Determine if all composition tasks for this benchmark are included Returns: (bool) \"\"\" return self . _determine_completeness ( completeness_type = COMPOSITION_KEY ) @property def is_structure_complete ( self ): \"\"\"Determine if all structure tasks for this benchmark are included Returns: (bool) \"\"\" return self . _determine_completeness ( completeness_type = STRUCTURE_KEY ) @property def is_regression_complete ( self ): \"\"\"Determine if all regression tasks for this benchmark are included Returns: (bool) \"\"\" return self . _determine_completeness ( completeness_type = REG_KEY ) @property def is_classification_complete ( self ): \"\"\"Determine if all classification tasks for this benchmark are included Returns: (bool) \"\"\" return self . _determine_completeness ( completeness_type = CLF_KEY ) @property def is_recorded ( self ): \"\"\"All tasks in this benchmark (whether or not it includes all tasks in the benchmark set) are recorded. Returns: (bool): True if all tasks (even if only a subset of all matbench) for this benchmark are recorded. \"\"\" return all ([ t . all_folds_recorded for t in self . tasks_map . values ()]) @property def is_valid ( self ): \"\"\"Checks all tasks are recorded and valid, as per each task's validation procedure. Can take some time, especially if the tasks are not already loaded into memory. Returns: (bool): True if all tasks are valid \"\"\" errors = self . validate () if errors : formatted_errors = pprint . pformat ( errors ) logger . critical ( f \"Benchmark has errors! \" f \"Errors: \\n { formatted_errors } \" ) return False else : return True info property readonly Get a formatted string of info about this benchmark and its current state. Returns: Type Description (str) is_classification_complete property readonly Determine if all classification tasks for this benchmark are included Returns: Type Description (bool) is_complete property readonly Determine if all available tasks are included in this benchmark. For matbench v0.1, this means all 13 tasks are in the benchmark. Returns: Type Description (bool) is_composition_complete property readonly Determine if all composition tasks for this benchmark are included Returns: Type Description (bool) is_recorded property readonly All tasks in this benchmark (whether or not it includes all tasks in the benchmark set) are recorded. Returns: Type Description (bool) True if all tasks (even if only a subset of all matbench) for this benchmark are recorded. is_regression_complete property readonly Determine if all regression tasks for this benchmark are included Returns: Type Description (bool) is_structure_complete property readonly Determine if all structure tasks for this benchmark are included Returns: Type Description (bool) is_valid property readonly Checks all tasks are recorded and valid, as per each task's validation procedure. Can take some time, especially if the tasks are not already loaded into memory. Returns: Type Description (bool) True if all tasks are valid scores property readonly Get all score metrics for all tasks as a dictionary. Returns: Type Description (RecursiveDotDict) A nested dictionary-like object of scores for each task. tasks property readonly Return the tasks as a list. Returns: Type Description ([MatbenchTask]) A list of matbench tasks in this benchmark __getattr__ ( self , item ) special Enable MatbenchBenchmark.task_name behavior. Parameters: Name Type Description Default item str The name of the attr. required Returns: Type Description The attr, if not in the metadata defined by the benchmark If the attr is a task name, returns that MatBenchTask object. Source code in matbench/bench.py def __getattr__ ( self , item ): \"\"\" Enable MatbenchBenchmark.task_name behavior. Args: item (str): The name of the attr. Returns: The attr, if not in the metadata defined by the benchmark If the attr is a task name, returns that MatBenchTask object. \"\"\" if item in self . metadata : return self . tasks_map [ item ] else : return self . __getattribute__ ( item ) __init__ ( self , benchmark = 'matbench_v0.1' , autoload = False , subset = None ) special Parameters: Name Type Description Default benchmark str The name of the benchmark. Only supported benchmark currently is \"matbench_v0.1\", though more will be added in the future. 'matbench_v0.1' autoload bool If True, automatically load the dataset into memory For a full benchmark, this can take some time. If False, you'll need to load each task with .load before you can access the raw data. False subset [str] A list of task names to use as a subset of a full benchmark. Only the named tasks will be contained in the class. Must correspond to the metadata file defined by the benchmark name. None Source code in matbench/bench.py def __init__ ( self , benchmark = MBV01_KEY , autoload = False , subset = None ): \"\"\" Args: benchmark (str): The name of the benchmark. Only supported benchmark currently is \"matbench_v0.1\", though more will be added in the future. autoload (bool): If True, automatically load the dataset into memory For a full benchmark, this can take some time. If False, you'll need to load each task with .load before you can access the raw data. subset ([str]): A list of task names to use as a subset of a full benchmark. Only the named tasks will be contained in the class. Must correspond to the metadata file defined by the benchmark name. \"\"\" if benchmark == MBV01_KEY : self . benchmark_name = MBV01_KEY self . metadata = mbv01_metadata else : raise ValueError ( f \"Only ' { MBV01_KEY } ' available. No other benchmarks defined!\" ) if subset : not_datasets = [ k for k in subset if k not in self . metadata ] if not_datasets : raise KeyError ( f \"Some tasks in { subset } are not benchmark=\" f \"' { self . benchmark_name } ' datasets! Remove { not_datasets } .\" ) else : available_tasks = subset else : available_tasks = self . metadata . keys () self . user_metadata = {} self . tasks_map = RecursiveDotDict () for ds in available_tasks : self . tasks_map [ ds ] = MatbenchTask ( ds , autoload = autoload , benchmark = self . benchmark_name ) logger . info ( f \"Initialized benchmark ' { benchmark } ' \" f \"with { len ( available_tasks ) } tasks: \\n \" f \" { pprint . pformat ( list ( available_tasks )) } \" ) add_metadata ( self , metadata ) Add freeform information about this run to the object (and subsequent json), accessible thru the 'user_metadata' attr. All keys must be strings. All values must be either: a. a numpy ndarray b. python native types, such as bools, floats, ints, strs c. a pandas series d. a list/tuple of python native types (bools, floats, ints) OR e. A dictionary where all keys are strs and all values are one of a, b, c, d, or e (recursive). Returns: Type Description None Source code in matbench/bench.py def add_metadata ( self , metadata ): \"\"\"Add freeform information about this run to the object (and subsequent json), accessible thru the 'user_metadata' attr. All keys must be strings. All values must be either: a. a numpy ndarray b. python native types, such as bools, floats, ints, strs c. a pandas series d. a list/tuple of python native types (bools, floats, ints) OR e. A dictionary where all keys are strs and all values are one of a, b, c, d, or e (recursive). Args: metadata (dict) Returns: None \"\"\" # Use logging here so bad metadata addition does not # ruin an entire run... if not isinstance ( metadata , dict ): logger . critical ( f \"User metadata must be reducible to dict format, \" f \"not type( { type ( metadata ) } )\" ) logger . info ( \"User metadata not added.\" ) else : if self . user_metadata : logger . warning ( \"User metadata already exists! Overwriting...\" ) self . user_metadata = immutify_dictionary ( metadata ) logger . info ( \"User metadata added successfully!\" ) as_dict ( self ) Overridden from MSONable.as_dict, get dict repr of this obj Returns: Type Description (dict) Source code in matbench/bench.py def as_dict ( self ): \"\"\"Overridden from MSONable.as_dict, get dict repr of this obj Returns: (dict) \"\"\" tasksd = { mbt . dataset_name : mbt . as_dict () for mbt in self . tasks } tasksd_jsonable = immutify_dictionary ( tasksd ) d = { \"@module\" : self . __class__ . __module__ , \"@class\" : self . __class__ . __name__ , self . _VERSION_KEY : VERSION , self . _TASKS_KEY : tasksd_jsonable , self . _USER_METADATA_KEY : self . user_metadata , self . _BENCHMARK_KEY : self . benchmark_name , self . _DATESTAMP_KEY : datetime . datetime . utcnow () . strftime ( self . _DATESTAMP_FMT ), } # to obtain a hash for this benchmark, immutify the dictionary # and then stringify it d [ self . _HASH_KEY ] = hash_dictionary ( d ) logger . debug ( f \"Successfully converted { self . __class__ . __name__ } to dictionary.\" ) return d from_dict ( d ) classmethod Create a MatbenchBenchmark object from a dictionary. Parameters: Name Type Description Default d dict required Returns: Type Description (MatbenchBenchmark) Source code in matbench/bench.py @classmethod def from_dict ( cls , d ): \"\"\"Create a MatbenchBenchmark object from a dictionary. Args: d (dict): Returns: (MatbenchBenchmark) \"\"\" required_keys = [ \"@module\" , \"@class\" , cls . _VERSION_KEY , cls . _BENCHMARK_KEY , cls . _TASKS_KEY , cls . _USER_METADATA_KEY , cls . _DATESTAMP_KEY , cls . _HASH_KEY , ] missing_keys = [] for k in required_keys : if k not in d : missing_keys . append ( k ) extra_keys = [] for k in d : if k not in required_keys : extra_keys . append ( k ) if missing_keys and not extra_keys : raise ValueError ( f \"Required keys { missing_keys } for { cls . __class__ . __name__ } \" f \"not found!\" ) elif not missing_keys and extra_keys : raise ValueError ( f \"Extra keys { extra_keys } for { cls . __class__ . __name__ } \" f \"present!\" ) elif missing_keys and extra_keys : raise ValueError ( f \"Missing required keys { missing_keys } and extra keys \" f \" { extra_keys } present!\" ) # Check all tasks to make sure their benchmark name is matching in the # benchmark and in the tasks not_matching_bench = [] for t_dict in d [ cls . _TASKS_KEY ] . values (): if t_dict [ MatbenchTask . _BENCHMARK_KEY ] != d [ cls . _BENCHMARK_KEY ]: not_matching_bench . append ( t_dict [ MatbenchTask . _DATASET_KEY ]) if not_matching_bench : raise ValueError ( f \"Tasks { not_matching_bench } do not have a benchmark name \" f \"matching the benchmark ( { d [ cls . _BENCHMARK_KEY ] } )!\" ) # Ensure the hash is matching, i.e., the data was not modified after # matbench got done with it m_from_dict = d . pop ( cls . _HASH_KEY ) m = hash_dictionary ( d ) if m != m_from_dict : raise ValueError ( f \"Hash of dictionary does not match it's reported value! { m } \" f \"!= { m_from_dict } . Was the data modified after saving?)\" ) # Check to see if any tasks have task names not matching their key # names in the benchmark not_matching_tasks = [] for task_name , task_info in d [ cls . _TASKS_KEY ] . items (): key_as_per_task = task_info [ MatbenchTask . _DATASET_KEY ] if task_name != key_as_per_task : not_matching_tasks . append (( task_name , key_as_per_task )) if not_matching_tasks : raise ValueError ( f \"Task names in benchmark and task names in tasks not \" f \"matching: { not_matching_tasks } \" ) # Warn if versions are not matching if d [ cls . _VERSION_KEY ] != VERSION : logger . warning ( f \"Warning! Versions not matching: \" f \"(data file has version { d [ cls . _VERSION_KEY ] } , \" f \"this package is { VERSION } ).\" ) return cls . _from_args ( benchmark_name = d [ cls . _BENCHMARK_KEY ], tasks_dict = d [ cls . _TASKS_KEY ], user_metadata = d [ cls . _USER_METADATA_KEY ], ) from_preset ( benchmark , preset_name , autoload = False ) classmethod The following presets are defined for each benchmark: benchmark: 'matbench_v0.1': - preset: 'structure' - Only structure problems - preset: 'composition' - Only composition problems - preset: 'regression' - Only regression problems - preset: 'classification' - Only classification problems - preset: 'all' - All problems in matbench v0.1 Parameters: Name Type Description Default benchmark str Name of the benchmark set you'd like to use. The only supported benchmark set currently is \"matbench_v0.1\" required preset_name str The name of the preset required autoload bool If true, automatically loads all the datasets upon instantiation. Be warned; this can take a while. False Returns: Type Description (MatbenchBenchmark object) Source code in matbench/bench.py @classmethod def from_preset ( cls , benchmark , preset_name , autoload = False ): \"\"\" The following presets are defined for each benchmark: benchmark: 'matbench_v0.1': - preset: 'structure' - Only structure problems - preset: 'composition' - Only composition problems - preset: 'regression' - Only regression problems - preset: 'classification' - Only classification problems - preset: 'all' - All problems in matbench v0.1 Args: benchmark (str): Name of the benchmark set you'd like to use. The only supported benchmark set currently is \"matbench_v0.1\" preset_name (str): The name of the preset autoload (bool): If true, automatically loads all the datasets upon instantiation. Be warned; this can take a while. Returns: (MatbenchBenchmark object) \"\"\" if benchmark == MBV01_KEY : if preset_name == STRUCTURE_KEY : available_tasks = [ k for k , v in mbv01_metadata . items () if v . input_type == STRUCTURE_KEY ] elif preset_name == COMPOSITION_KEY : available_tasks = [ k for k , v in mbv01_metadata . items () if v . input_type == COMPOSITION_KEY ] elif preset_name == REG_KEY : available_tasks = [ k for k , v in mbv01_metadata . items () if v . task_type == REG_KEY ] elif preset_name == CLF_KEY : available_tasks = [ k for k , v in mbv01_metadata . items () if v . task_type == CLF_KEY ] elif preset_name == cls . ALL_KEY : available_tasks = [ k for k , v in mbv01_metadata . items ()] else : valid_keys = [ STRUCTURE_KEY , COMPOSITION_KEY , CLF_KEY , REG_KEY , cls . ALL_KEY , ] raise ValueError ( f \"Preset name ' { preset_name } ' not recognized for \" f \"benchmark ' { MBV01_KEY } '! Select from \" f \" { valid_keys } \" ) else : raise ValueError ( f \"Only ' { MBV01_KEY } ' available. No other benchmarks defined!\" ) return cls ( benchmark = benchmark , autoload = autoload , subset = available_tasks ) get_info ( self ) Log info about the benchmark to the respective logging handlers. Returns: Type Description None Source code in matbench/bench.py def get_info ( self ): \"\"\"Log info about the benchmark to the respective logging handlers. Returns: None \"\"\" logger . info ( self . info ) load ( self ) Load all tasks in this benchmark. Returns: Type Description None Source code in matbench/bench.py def load ( self ): \"\"\"Load all tasks in this benchmark. Returns: None \"\"\" for t in self . tasks : t . load () validate ( self ) Run validation on each task in this benchmark. Returns: Type Description ({str str}): dict of errors, if they exist Source code in matbench/bench.py def validate ( self ): \"\"\"Run validation on each task in this benchmark. Returns: ({str: str}): dict of errors, if they exist \"\"\" errors = {} for t , t_obj in self . tasks_map . items (): try : t_obj . validate () except BaseException : errors [ t ] = traceback . format_exc () return errors","title":"MatbenchBenchmark"},{"location":"Reference/MatbenchBenchmark/#matbenchbenchmark","text":"The core class for benchmarking with Matbench. MatbenchBenchmark is capable of benchmarking and validating arbitrary materials science benchmarks. It is a container class for sets of MatbenchTasks, objects which provide predetermined sets of training/validation and testing data for any algorithm to benchmark with. MatbenchBenchmark can also give summaries of entire complex benchmarks, including access to individual score statistics for each metric. MatbenchBenchmark can run any benchmark as long as it has a corresponding benchmark name key. Matbench v0.1 (\"matbench_v0.1\") is the only benchmark currently configured for use with MatbenchBenchmark. MatbenchBenchmark is capable of running benchmark subsets; for example, only 3 of the 13 available Matbench v0.1 problems. See the documentation for more details. Attributes: Name Type Description benchmark_name str The benchmark name, defaults to the original Matbench v0.1 \"matbench_v0.1\". Should have an associated validation file in order for the MatbenchTasks to work correctly. metadata dict The corresponding metadata file for this benchmark, which defines the basic configuration for each task. See matbench_v0.1_validation for an example. Each dataset has the same required keys in order to work correctly. user_metadata dict Any metadata about the algorithm or benchmark that the user wants to keep as part of the benchmark file. tasks_map {str MatbenchTask}): A mapping of task name to the corresponding MatbenchTask object. <<task_names>> MatbenchTask Access any task obj via MatbenchTask.< >. For example: mb = MatbenchBenchmark() mb.matbench_dielectric < > Source code in matbench/bench.py class MatbenchBenchmark ( MSONable , MSONable2File ): \"\"\"The core class for benchmarking with Matbench. MatbenchBenchmark is capable of benchmarking and validating arbitrary materials science benchmarks. It is a container class for sets of MatbenchTasks, objects which provide predetermined sets of training/validation and testing data for any algorithm to benchmark with. MatbenchBenchmark can also give summaries of entire complex benchmarks, including access to individual score statistics for each metric. MatbenchBenchmark can run any benchmark as long as it has a corresponding benchmark name key. Matbench v0.1 (\"matbench_v0.1\") is the only benchmark currently configured for use with MatbenchBenchmark. MatbenchBenchmark is capable of running benchmark subsets; for example, only 3 of the 13 available Matbench v0.1 problems. See the documentation for more details. Attributes: benchmark_name (str): The benchmark name, defaults to the original Matbench v0.1 \"matbench_v0.1\". Should have an associated validation file in order for the MatbenchTasks to work correctly. metadata (dict): The corresponding metadata file for this benchmark, which defines the basic configuration for each task. See matbench_v0.1_validation for an example. Each dataset has the same required keys in order to work correctly. user_metadata (dict): Any metadata about the algorithm or benchmark that the user wants to keep as part of the benchmark file. tasks_map ({str: MatbenchTask}): A mapping of task name to the corresponding MatbenchTask object. <<task_names>> (MatbenchTask): Access any task obj via MatbenchTask.<<task_name>>. For example: mb = MatbenchBenchmark() mb.matbench_dielectric <<MatbenchTask object>> \"\"\" # For serialization _VERSION_KEY = \"version\" _BENCHMARK_KEY = \"benchmark_name\" _USER_METADATA_KEY = \"user_metadata\" _TASKS_KEY = \"tasks\" _DATESTAMP_KEY = \"datestamp\" _DATESTAMP_FMT = \"%Y.%m. %d %H:%M.%S\" _HASH_KEY = \"hash\" # For class usage ALL_KEY = \"all\" def __init__ ( self , benchmark = MBV01_KEY , autoload = False , subset = None ): \"\"\" Args: benchmark (str): The name of the benchmark. Only supported benchmark currently is \"matbench_v0.1\", though more will be added in the future. autoload (bool): If True, automatically load the dataset into memory For a full benchmark, this can take some time. If False, you'll need to load each task with .load before you can access the raw data. subset ([str]): A list of task names to use as a subset of a full benchmark. Only the named tasks will be contained in the class. Must correspond to the metadata file defined by the benchmark name. \"\"\" if benchmark == MBV01_KEY : self . benchmark_name = MBV01_KEY self . metadata = mbv01_metadata else : raise ValueError ( f \"Only ' { MBV01_KEY } ' available. No other benchmarks defined!\" ) if subset : not_datasets = [ k for k in subset if k not in self . metadata ] if not_datasets : raise KeyError ( f \"Some tasks in { subset } are not benchmark=\" f \"' { self . benchmark_name } ' datasets! Remove { not_datasets } .\" ) else : available_tasks = subset else : available_tasks = self . metadata . keys () self . user_metadata = {} self . tasks_map = RecursiveDotDict () for ds in available_tasks : self . tasks_map [ ds ] = MatbenchTask ( ds , autoload = autoload , benchmark = self . benchmark_name ) logger . info ( f \"Initialized benchmark ' { benchmark } ' \" f \"with { len ( available_tasks ) } tasks: \\n \" f \" { pprint . pformat ( list ( available_tasks )) } \" ) def __getattr__ ( self , item ): \"\"\" Enable MatbenchBenchmark.task_name behavior. Args: item (str): The name of the attr. Returns: The attr, if not in the metadata defined by the benchmark If the attr is a task name, returns that MatBenchTask object. \"\"\" if item in self . metadata : return self . tasks_map [ item ] else : return self . __getattribute__ ( item ) @classmethod def from_preset ( cls , benchmark , preset_name , autoload = False ): \"\"\" The following presets are defined for each benchmark: benchmark: 'matbench_v0.1': - preset: 'structure' - Only structure problems - preset: 'composition' - Only composition problems - preset: 'regression' - Only regression problems - preset: 'classification' - Only classification problems - preset: 'all' - All problems in matbench v0.1 Args: benchmark (str): Name of the benchmark set you'd like to use. The only supported benchmark set currently is \"matbench_v0.1\" preset_name (str): The name of the preset autoload (bool): If true, automatically loads all the datasets upon instantiation. Be warned; this can take a while. Returns: (MatbenchBenchmark object) \"\"\" if benchmark == MBV01_KEY : if preset_name == STRUCTURE_KEY : available_tasks = [ k for k , v in mbv01_metadata . items () if v . input_type == STRUCTURE_KEY ] elif preset_name == COMPOSITION_KEY : available_tasks = [ k for k , v in mbv01_metadata . items () if v . input_type == COMPOSITION_KEY ] elif preset_name == REG_KEY : available_tasks = [ k for k , v in mbv01_metadata . items () if v . task_type == REG_KEY ] elif preset_name == CLF_KEY : available_tasks = [ k for k , v in mbv01_metadata . items () if v . task_type == CLF_KEY ] elif preset_name == cls . ALL_KEY : available_tasks = [ k for k , v in mbv01_metadata . items ()] else : valid_keys = [ STRUCTURE_KEY , COMPOSITION_KEY , CLF_KEY , REG_KEY , cls . ALL_KEY , ] raise ValueError ( f \"Preset name ' { preset_name } ' not recognized for \" f \"benchmark ' { MBV01_KEY } '! Select from \" f \" { valid_keys } \" ) else : raise ValueError ( f \"Only ' { MBV01_KEY } ' available. No other benchmarks defined!\" ) return cls ( benchmark = benchmark , autoload = autoload , subset = available_tasks ) @classmethod def from_dict ( cls , d ): \"\"\"Create a MatbenchBenchmark object from a dictionary. Args: d (dict): Returns: (MatbenchBenchmark) \"\"\" required_keys = [ \"@module\" , \"@class\" , cls . _VERSION_KEY , cls . _BENCHMARK_KEY , cls . _TASKS_KEY , cls . _USER_METADATA_KEY , cls . _DATESTAMP_KEY , cls . _HASH_KEY , ] missing_keys = [] for k in required_keys : if k not in d : missing_keys . append ( k ) extra_keys = [] for k in d : if k not in required_keys : extra_keys . append ( k ) if missing_keys and not extra_keys : raise ValueError ( f \"Required keys { missing_keys } for { cls . __class__ . __name__ } \" f \"not found!\" ) elif not missing_keys and extra_keys : raise ValueError ( f \"Extra keys { extra_keys } for { cls . __class__ . __name__ } \" f \"present!\" ) elif missing_keys and extra_keys : raise ValueError ( f \"Missing required keys { missing_keys } and extra keys \" f \" { extra_keys } present!\" ) # Check all tasks to make sure their benchmark name is matching in the # benchmark and in the tasks not_matching_bench = [] for t_dict in d [ cls . _TASKS_KEY ] . values (): if t_dict [ MatbenchTask . _BENCHMARK_KEY ] != d [ cls . _BENCHMARK_KEY ]: not_matching_bench . append ( t_dict [ MatbenchTask . _DATASET_KEY ]) if not_matching_bench : raise ValueError ( f \"Tasks { not_matching_bench } do not have a benchmark name \" f \"matching the benchmark ( { d [ cls . _BENCHMARK_KEY ] } )!\" ) # Ensure the hash is matching, i.e., the data was not modified after # matbench got done with it m_from_dict = d . pop ( cls . _HASH_KEY ) m = hash_dictionary ( d ) if m != m_from_dict : raise ValueError ( f \"Hash of dictionary does not match it's reported value! { m } \" f \"!= { m_from_dict } . Was the data modified after saving?)\" ) # Check to see if any tasks have task names not matching their key # names in the benchmark not_matching_tasks = [] for task_name , task_info in d [ cls . _TASKS_KEY ] . items (): key_as_per_task = task_info [ MatbenchTask . _DATASET_KEY ] if task_name != key_as_per_task : not_matching_tasks . append (( task_name , key_as_per_task )) if not_matching_tasks : raise ValueError ( f \"Task names in benchmark and task names in tasks not \" f \"matching: { not_matching_tasks } \" ) # Warn if versions are not matching if d [ cls . _VERSION_KEY ] != VERSION : logger . warning ( f \"Warning! Versions not matching: \" f \"(data file has version { d [ cls . _VERSION_KEY ] } , \" f \"this package is { VERSION } ).\" ) return cls . _from_args ( benchmark_name = d [ cls . _BENCHMARK_KEY ], tasks_dict = d [ cls . _TASKS_KEY ], user_metadata = d [ cls . _USER_METADATA_KEY ], ) @classmethod def _from_args ( cls , benchmark_name , tasks_dict , user_metadata ): \"\"\"Create a MatbenchBenchmark object from arguments Args: benchmark_name (str): name of the benchmark tasks_dict (dict): formatted dict of task data user_metadata (dict): freeform user metadata Returns: (MatbenchBenchmark) \"\"\" subset = list ( tasks_dict . keys ()) obj = cls ( benchmark = benchmark_name , autoload = False , subset = subset ) obj . tasks_map = RecursiveDotDict ( { t_name : MatbenchTask . from_dict ( t_dict ) for t_name , t_dict in tasks_dict . items () } ) logger . warning ( \"To add new data to this benchmark, the \" \"benchmark must be loaded with .load(). Alternatively, \" \"load individual tasks with MatbenchTask.load().\" ) # MatbenchTask automatically validates files during its from_dict obj . user_metadata = user_metadata logger . debug ( f \"Successfully converted dict/args to ' { cls . __name__ } '.\" ) return obj def _determine_completeness ( self , completeness_type ): \"\"\"Determine the completeness of this benchmark. Completeness means the tasks are included (but not necessarily recorded yet) in the benchmark. Supported completeness types are: - \"all\": All tasks are included - \"composition\": All composition tasks are included - \"structure\": All structure tasks are included - \"regression\": All regression problems - \"classification\": All classification problems Args: completeness_type (str): One of the above completeness types. Returns: (bool) True if this benchmark object is complete with respect to the completeness type. \"\"\" if completeness_type == self . ALL_KEY : required_tasks = list ( self . metadata . keys ()) elif completeness_type in ( COMPOSITION_KEY , STRUCTURE_KEY ): required_tasks = [ k for k , v in self . metadata . items () if v . input_type == completeness_type ] elif completeness_type in ( REG_KEY , CLF_KEY ): required_tasks = [ k for k , v in self . metadata . items () if v . task_type == completeness_type ] else : allowed_completeness_types = [ self . ALL_KEY , COMPOSITION_KEY , STRUCTURE_KEY , REG_KEY , CLF_KEY , ] raise ValueError ( \"Only supported completeness types are \" f \" { allowed_completeness_types } \" ) for task in required_tasks : if task not in self . tasks_map : return False else : return True def as_dict ( self ): \"\"\"Overridden from MSONable.as_dict, get dict repr of this obj Returns: (dict) \"\"\" tasksd = { mbt . dataset_name : mbt . as_dict () for mbt in self . tasks } tasksd_jsonable = immutify_dictionary ( tasksd ) d = { \"@module\" : self . __class__ . __module__ , \"@class\" : self . __class__ . __name__ , self . _VERSION_KEY : VERSION , self . _TASKS_KEY : tasksd_jsonable , self . _USER_METADATA_KEY : self . user_metadata , self . _BENCHMARK_KEY : self . benchmark_name , self . _DATESTAMP_KEY : datetime . datetime . utcnow () . strftime ( self . _DATESTAMP_FMT ), } # to obtain a hash for this benchmark, immutify the dictionary # and then stringify it d [ self . _HASH_KEY ] = hash_dictionary ( d ) logger . debug ( f \"Successfully converted { self . __class__ . __name__ } to dictionary.\" ) return d def get_info ( self ): \"\"\"Log info about the benchmark to the respective logging handlers. Returns: None \"\"\" logger . info ( self . info ) def add_metadata ( self , metadata ): \"\"\"Add freeform information about this run to the object (and subsequent json), accessible thru the 'user_metadata' attr. All keys must be strings. All values must be either: a. a numpy ndarray b. python native types, such as bools, floats, ints, strs c. a pandas series d. a list/tuple of python native types (bools, floats, ints) OR e. A dictionary where all keys are strs and all values are one of a, b, c, d, or e (recursive). Args: metadata (dict) Returns: None \"\"\" # Use logging here so bad metadata addition does not # ruin an entire run... if not isinstance ( metadata , dict ): logger . critical ( f \"User metadata must be reducible to dict format, \" f \"not type( { type ( metadata ) } )\" ) logger . info ( \"User metadata not added.\" ) else : if self . user_metadata : logger . warning ( \"User metadata already exists! Overwriting...\" ) self . user_metadata = immutify_dictionary ( metadata ) logger . info ( \"User metadata added successfully!\" ) def load ( self ): \"\"\"Load all tasks in this benchmark. Returns: None \"\"\" for t in self . tasks : t . load () def validate ( self ): \"\"\"Run validation on each task in this benchmark. Returns: ({str: str}): dict of errors, if they exist \"\"\" errors = {} for t , t_obj in self . tasks_map . items (): try : t_obj . validate () except BaseException : errors [ t ] = traceback . format_exc () return errors @property def tasks ( self ): \"\"\"Return the tasks as a list. Returns: ([MatbenchTask]): A list of matbench tasks in this benchmark \"\"\" return self . tasks_map . values () @property def scores ( self ): \"\"\"Get all score metrics for all tasks as a dictionary. Returns: (RecursiveDotDict): A nested dictionary-like object of scores for each task. \"\"\" return RecursiveDotDict ({ t . dataset_name : t . scores for t in self . tasks }) @property def info ( self ): \"\"\"Get a formatted string of info about this benchmark and its current state. Returns: (str) \"\"\" complete = self . is_complete recorded = self . is_recorded valid = self . is_valid s = \"\" s += ( f \" \\n Matbench package { VERSION } running benchmark \" f \"' { self . benchmark_name } '\" ) s += f \" \\n\\t is complete: { complete } \" s += f \" \\n\\t is recorded: { recorded } \" s += f \" \\n\\t is valid: { valid } \" if not recorded : s += ( \" \\n\\n Benchmark is not fully recorded; limited information \" \"shown.\" ) if not valid : s += \" \\n\\n Benchmark is not valid; limited information shown.\" if not valid or not recorded : s += \" \\n\\n Tasks:\" for t in self . tasks_map . values (): s += f \" \\n\\t - ' { t . dataset_name } : recorded= { t . all_folds_recorded } \" if valid and recorded : s += \" \\n\\n Results:\" for t in self . tasks : if t . metadata . task_type == REG_KEY : score_text = ( f \"MAE mean: \" f \" { self . scores [ t . dataset_name ] . mae . mean } \" ) else : score_text = ( f \"ROCAUC mean: \" f \" { self . scores [ t . dataset_name ] . rocauc . mean } \" ) s += f \" \\n\\t - ' { t . dataset_name } ' { score_text } \" return s @property def is_complete ( self ): \"\"\"Determine if all available tasks are included in this benchmark. For matbench v0.1, this means all 13 tasks are in the benchmark. Returns: (bool) \"\"\" return self . _determine_completeness ( completeness_type = self . ALL_KEY ) @property def is_composition_complete ( self ): \"\"\"Determine if all composition tasks for this benchmark are included Returns: (bool) \"\"\" return self . _determine_completeness ( completeness_type = COMPOSITION_KEY ) @property def is_structure_complete ( self ): \"\"\"Determine if all structure tasks for this benchmark are included Returns: (bool) \"\"\" return self . _determine_completeness ( completeness_type = STRUCTURE_KEY ) @property def is_regression_complete ( self ): \"\"\"Determine if all regression tasks for this benchmark are included Returns: (bool) \"\"\" return self . _determine_completeness ( completeness_type = REG_KEY ) @property def is_classification_complete ( self ): \"\"\"Determine if all classification tasks for this benchmark are included Returns: (bool) \"\"\" return self . _determine_completeness ( completeness_type = CLF_KEY ) @property def is_recorded ( self ): \"\"\"All tasks in this benchmark (whether or not it includes all tasks in the benchmark set) are recorded. Returns: (bool): True if all tasks (even if only a subset of all matbench) for this benchmark are recorded. \"\"\" return all ([ t . all_folds_recorded for t in self . tasks_map . values ()]) @property def is_valid ( self ): \"\"\"Checks all tasks are recorded and valid, as per each task's validation procedure. Can take some time, especially if the tasks are not already loaded into memory. Returns: (bool): True if all tasks are valid \"\"\" errors = self . validate () if errors : formatted_errors = pprint . pformat ( errors ) logger . critical ( f \"Benchmark has errors! \" f \"Errors: \\n { formatted_errors } \" ) return False else : return True","title":"MatbenchBenchmark"},{"location":"Reference/MatbenchBenchmark/#matbench.bench.MatbenchBenchmark.info","text":"Get a formatted string of info about this benchmark and its current state. Returns: Type Description (str)","title":"info"},{"location":"Reference/MatbenchBenchmark/#matbench.bench.MatbenchBenchmark.is_classification_complete","text":"Determine if all classification tasks for this benchmark are included Returns: Type Description (bool)","title":"is_classification_complete"},{"location":"Reference/MatbenchBenchmark/#matbench.bench.MatbenchBenchmark.is_complete","text":"Determine if all available tasks are included in this benchmark. For matbench v0.1, this means all 13 tasks are in the benchmark. Returns: Type Description (bool)","title":"is_complete"},{"location":"Reference/MatbenchBenchmark/#matbench.bench.MatbenchBenchmark.is_composition_complete","text":"Determine if all composition tasks for this benchmark are included Returns: Type Description (bool)","title":"is_composition_complete"},{"location":"Reference/MatbenchBenchmark/#matbench.bench.MatbenchBenchmark.is_recorded","text":"All tasks in this benchmark (whether or not it includes all tasks in the benchmark set) are recorded. Returns: Type Description (bool) True if all tasks (even if only a subset of all matbench) for this benchmark are recorded.","title":"is_recorded"},{"location":"Reference/MatbenchBenchmark/#matbench.bench.MatbenchBenchmark.is_regression_complete","text":"Determine if all regression tasks for this benchmark are included Returns: Type Description (bool)","title":"is_regression_complete"},{"location":"Reference/MatbenchBenchmark/#matbench.bench.MatbenchBenchmark.is_structure_complete","text":"Determine if all structure tasks for this benchmark are included Returns: Type Description (bool)","title":"is_structure_complete"},{"location":"Reference/MatbenchBenchmark/#matbench.bench.MatbenchBenchmark.is_valid","text":"Checks all tasks are recorded and valid, as per each task's validation procedure. Can take some time, especially if the tasks are not already loaded into memory. Returns: Type Description (bool) True if all tasks are valid","title":"is_valid"},{"location":"Reference/MatbenchBenchmark/#matbench.bench.MatbenchBenchmark.scores","text":"Get all score metrics for all tasks as a dictionary. Returns: Type Description (RecursiveDotDict) A nested dictionary-like object of scores for each task.","title":"scores"},{"location":"Reference/MatbenchBenchmark/#matbench.bench.MatbenchBenchmark.tasks","text":"Return the tasks as a list. Returns: Type Description ([MatbenchTask]) A list of matbench tasks in this benchmark","title":"tasks"},{"location":"Reference/MatbenchBenchmark/#matbench.bench.MatbenchBenchmark.__getattr__","text":"Enable MatbenchBenchmark.task_name behavior. Parameters: Name Type Description Default item str The name of the attr. required Returns: Type Description The attr, if not in the metadata defined by the benchmark If the attr is a task name, returns that MatBenchTask object. Source code in matbench/bench.py def __getattr__ ( self , item ): \"\"\" Enable MatbenchBenchmark.task_name behavior. Args: item (str): The name of the attr. Returns: The attr, if not in the metadata defined by the benchmark If the attr is a task name, returns that MatBenchTask object. \"\"\" if item in self . metadata : return self . tasks_map [ item ] else : return self . __getattribute__ ( item )","title":"__getattr__()"},{"location":"Reference/MatbenchBenchmark/#matbench.bench.MatbenchBenchmark.__init__","text":"Parameters: Name Type Description Default benchmark str The name of the benchmark. Only supported benchmark currently is \"matbench_v0.1\", though more will be added in the future. 'matbench_v0.1' autoload bool If True, automatically load the dataset into memory For a full benchmark, this can take some time. If False, you'll need to load each task with .load before you can access the raw data. False subset [str] A list of task names to use as a subset of a full benchmark. Only the named tasks will be contained in the class. Must correspond to the metadata file defined by the benchmark name. None Source code in matbench/bench.py def __init__ ( self , benchmark = MBV01_KEY , autoload = False , subset = None ): \"\"\" Args: benchmark (str): The name of the benchmark. Only supported benchmark currently is \"matbench_v0.1\", though more will be added in the future. autoload (bool): If True, automatically load the dataset into memory For a full benchmark, this can take some time. If False, you'll need to load each task with .load before you can access the raw data. subset ([str]): A list of task names to use as a subset of a full benchmark. Only the named tasks will be contained in the class. Must correspond to the metadata file defined by the benchmark name. \"\"\" if benchmark == MBV01_KEY : self . benchmark_name = MBV01_KEY self . metadata = mbv01_metadata else : raise ValueError ( f \"Only ' { MBV01_KEY } ' available. No other benchmarks defined!\" ) if subset : not_datasets = [ k for k in subset if k not in self . metadata ] if not_datasets : raise KeyError ( f \"Some tasks in { subset } are not benchmark=\" f \"' { self . benchmark_name } ' datasets! Remove { not_datasets } .\" ) else : available_tasks = subset else : available_tasks = self . metadata . keys () self . user_metadata = {} self . tasks_map = RecursiveDotDict () for ds in available_tasks : self . tasks_map [ ds ] = MatbenchTask ( ds , autoload = autoload , benchmark = self . benchmark_name ) logger . info ( f \"Initialized benchmark ' { benchmark } ' \" f \"with { len ( available_tasks ) } tasks: \\n \" f \" { pprint . pformat ( list ( available_tasks )) } \" )","title":"__init__()"},{"location":"Reference/MatbenchBenchmark/#matbench.bench.MatbenchBenchmark.add_metadata","text":"Add freeform information about this run to the object (and subsequent json), accessible thru the 'user_metadata' attr. All keys must be strings. All values must be either: a. a numpy ndarray b. python native types, such as bools, floats, ints, strs c. a pandas series d. a list/tuple of python native types (bools, floats, ints) OR e. A dictionary where all keys are strs and all values are one of a, b, c, d, or e (recursive). Returns: Type Description None Source code in matbench/bench.py def add_metadata ( self , metadata ): \"\"\"Add freeform information about this run to the object (and subsequent json), accessible thru the 'user_metadata' attr. All keys must be strings. All values must be either: a. a numpy ndarray b. python native types, such as bools, floats, ints, strs c. a pandas series d. a list/tuple of python native types (bools, floats, ints) OR e. A dictionary where all keys are strs and all values are one of a, b, c, d, or e (recursive). Args: metadata (dict) Returns: None \"\"\" # Use logging here so bad metadata addition does not # ruin an entire run... if not isinstance ( metadata , dict ): logger . critical ( f \"User metadata must be reducible to dict format, \" f \"not type( { type ( metadata ) } )\" ) logger . info ( \"User metadata not added.\" ) else : if self . user_metadata : logger . warning ( \"User metadata already exists! Overwriting...\" ) self . user_metadata = immutify_dictionary ( metadata ) logger . info ( \"User metadata added successfully!\" )","title":"add_metadata()"},{"location":"Reference/MatbenchBenchmark/#matbench.bench.MatbenchBenchmark.as_dict","text":"Overridden from MSONable.as_dict, get dict repr of this obj Returns: Type Description (dict) Source code in matbench/bench.py def as_dict ( self ): \"\"\"Overridden from MSONable.as_dict, get dict repr of this obj Returns: (dict) \"\"\" tasksd = { mbt . dataset_name : mbt . as_dict () for mbt in self . tasks } tasksd_jsonable = immutify_dictionary ( tasksd ) d = { \"@module\" : self . __class__ . __module__ , \"@class\" : self . __class__ . __name__ , self . _VERSION_KEY : VERSION , self . _TASKS_KEY : tasksd_jsonable , self . _USER_METADATA_KEY : self . user_metadata , self . _BENCHMARK_KEY : self . benchmark_name , self . _DATESTAMP_KEY : datetime . datetime . utcnow () . strftime ( self . _DATESTAMP_FMT ), } # to obtain a hash for this benchmark, immutify the dictionary # and then stringify it d [ self . _HASH_KEY ] = hash_dictionary ( d ) logger . debug ( f \"Successfully converted { self . __class__ . __name__ } to dictionary.\" ) return d","title":"as_dict()"},{"location":"Reference/MatbenchBenchmark/#matbench.bench.MatbenchBenchmark.from_dict","text":"Create a MatbenchBenchmark object from a dictionary. Parameters: Name Type Description Default d dict required Returns: Type Description (MatbenchBenchmark) Source code in matbench/bench.py @classmethod def from_dict ( cls , d ): \"\"\"Create a MatbenchBenchmark object from a dictionary. Args: d (dict): Returns: (MatbenchBenchmark) \"\"\" required_keys = [ \"@module\" , \"@class\" , cls . _VERSION_KEY , cls . _BENCHMARK_KEY , cls . _TASKS_KEY , cls . _USER_METADATA_KEY , cls . _DATESTAMP_KEY , cls . _HASH_KEY , ] missing_keys = [] for k in required_keys : if k not in d : missing_keys . append ( k ) extra_keys = [] for k in d : if k not in required_keys : extra_keys . append ( k ) if missing_keys and not extra_keys : raise ValueError ( f \"Required keys { missing_keys } for { cls . __class__ . __name__ } \" f \"not found!\" ) elif not missing_keys and extra_keys : raise ValueError ( f \"Extra keys { extra_keys } for { cls . __class__ . __name__ } \" f \"present!\" ) elif missing_keys and extra_keys : raise ValueError ( f \"Missing required keys { missing_keys } and extra keys \" f \" { extra_keys } present!\" ) # Check all tasks to make sure their benchmark name is matching in the # benchmark and in the tasks not_matching_bench = [] for t_dict in d [ cls . _TASKS_KEY ] . values (): if t_dict [ MatbenchTask . _BENCHMARK_KEY ] != d [ cls . _BENCHMARK_KEY ]: not_matching_bench . append ( t_dict [ MatbenchTask . _DATASET_KEY ]) if not_matching_bench : raise ValueError ( f \"Tasks { not_matching_bench } do not have a benchmark name \" f \"matching the benchmark ( { d [ cls . _BENCHMARK_KEY ] } )!\" ) # Ensure the hash is matching, i.e., the data was not modified after # matbench got done with it m_from_dict = d . pop ( cls . _HASH_KEY ) m = hash_dictionary ( d ) if m != m_from_dict : raise ValueError ( f \"Hash of dictionary does not match it's reported value! { m } \" f \"!= { m_from_dict } . Was the data modified after saving?)\" ) # Check to see if any tasks have task names not matching their key # names in the benchmark not_matching_tasks = [] for task_name , task_info in d [ cls . _TASKS_KEY ] . items (): key_as_per_task = task_info [ MatbenchTask . _DATASET_KEY ] if task_name != key_as_per_task : not_matching_tasks . append (( task_name , key_as_per_task )) if not_matching_tasks : raise ValueError ( f \"Task names in benchmark and task names in tasks not \" f \"matching: { not_matching_tasks } \" ) # Warn if versions are not matching if d [ cls . _VERSION_KEY ] != VERSION : logger . warning ( f \"Warning! Versions not matching: \" f \"(data file has version { d [ cls . _VERSION_KEY ] } , \" f \"this package is { VERSION } ).\" ) return cls . _from_args ( benchmark_name = d [ cls . _BENCHMARK_KEY ], tasks_dict = d [ cls . _TASKS_KEY ], user_metadata = d [ cls . _USER_METADATA_KEY ], )","title":"from_dict()"},{"location":"Reference/MatbenchBenchmark/#matbench.bench.MatbenchBenchmark.from_preset","text":"The following presets are defined for each benchmark: benchmark: 'matbench_v0.1': - preset: 'structure' - Only structure problems - preset: 'composition' - Only composition problems - preset: 'regression' - Only regression problems - preset: 'classification' - Only classification problems - preset: 'all' - All problems in matbench v0.1 Parameters: Name Type Description Default benchmark str Name of the benchmark set you'd like to use. The only supported benchmark set currently is \"matbench_v0.1\" required preset_name str The name of the preset required autoload bool If true, automatically loads all the datasets upon instantiation. Be warned; this can take a while. False Returns: Type Description (MatbenchBenchmark object) Source code in matbench/bench.py @classmethod def from_preset ( cls , benchmark , preset_name , autoload = False ): \"\"\" The following presets are defined for each benchmark: benchmark: 'matbench_v0.1': - preset: 'structure' - Only structure problems - preset: 'composition' - Only composition problems - preset: 'regression' - Only regression problems - preset: 'classification' - Only classification problems - preset: 'all' - All problems in matbench v0.1 Args: benchmark (str): Name of the benchmark set you'd like to use. The only supported benchmark set currently is \"matbench_v0.1\" preset_name (str): The name of the preset autoload (bool): If true, automatically loads all the datasets upon instantiation. Be warned; this can take a while. Returns: (MatbenchBenchmark object) \"\"\" if benchmark == MBV01_KEY : if preset_name == STRUCTURE_KEY : available_tasks = [ k for k , v in mbv01_metadata . items () if v . input_type == STRUCTURE_KEY ] elif preset_name == COMPOSITION_KEY : available_tasks = [ k for k , v in mbv01_metadata . items () if v . input_type == COMPOSITION_KEY ] elif preset_name == REG_KEY : available_tasks = [ k for k , v in mbv01_metadata . items () if v . task_type == REG_KEY ] elif preset_name == CLF_KEY : available_tasks = [ k for k , v in mbv01_metadata . items () if v . task_type == CLF_KEY ] elif preset_name == cls . ALL_KEY : available_tasks = [ k for k , v in mbv01_metadata . items ()] else : valid_keys = [ STRUCTURE_KEY , COMPOSITION_KEY , CLF_KEY , REG_KEY , cls . ALL_KEY , ] raise ValueError ( f \"Preset name ' { preset_name } ' not recognized for \" f \"benchmark ' { MBV01_KEY } '! Select from \" f \" { valid_keys } \" ) else : raise ValueError ( f \"Only ' { MBV01_KEY } ' available. No other benchmarks defined!\" ) return cls ( benchmark = benchmark , autoload = autoload , subset = available_tasks )","title":"from_preset()"},{"location":"Reference/MatbenchBenchmark/#matbench.bench.MatbenchBenchmark.get_info","text":"Log info about the benchmark to the respective logging handlers. Returns: Type Description None Source code in matbench/bench.py def get_info ( self ): \"\"\"Log info about the benchmark to the respective logging handlers. Returns: None \"\"\" logger . info ( self . info )","title":"get_info()"},{"location":"Reference/MatbenchBenchmark/#matbench.bench.MatbenchBenchmark.load","text":"Load all tasks in this benchmark. Returns: Type Description None Source code in matbench/bench.py def load ( self ): \"\"\"Load all tasks in this benchmark. Returns: None \"\"\" for t in self . tasks : t . load ()","title":"load()"},{"location":"Reference/MatbenchBenchmark/#matbench.bench.MatbenchBenchmark.validate","text":"Run validation on each task in this benchmark. Returns: Type Description ({str str}): dict of errors, if they exist Source code in matbench/bench.py def validate ( self ): \"\"\"Run validation on each task in this benchmark. Returns: ({str: str}): dict of errors, if they exist \"\"\" errors = {} for t , t_obj in self . tasks_map . items (): try : t_obj . validate () except BaseException : errors [ t ] = traceback . format_exc () return errors","title":"validate()"},{"location":"Reference/MatbenchTask/","text":"MatbenchTask The core interface for running a Matbench task and recording its results. MatbenchTask handles creating training/validation and testing sets, as well as recording and managing all data in a consistent fashion. MatbenchTask also validates data according to te specifications in the validation file. MatbenchTasks have a few core methods: MatbenchTask.get_train_and_val_data: Get nested cross validation data to be used for all training and validation. MatbenchTask.get_test_data: Get test data for nested cross validation. MatbenchTask.record: Record your predicted results for the test data. MatbenchTask.validate: Check to make sure the data you recorded for this task is valid. You can iterate through the folds of a matbench task using .folds and the .get_*_data methods. You can load the results of a task without having to load large datasets themselves. However, to get training and testing data, you must load the datasets. Tasks loaded from files do not automatically load the dataset into memory; to load a dataset into memory, use MatbenchTask.load(). See the full documentation online for more info and tutorials on using MatbenchTask. Attributes: Name Type Description benchmark_name str The name of the benchmark this task belongs to. df pd.DataFrame the dataframe of the dataset for this task info str Info about this dataset metadata RecursiveDotDict all metadata about this dataset validation RecursiveDotDict The validation specification for this task, including the training and testing splits for each fold. folds_keys [str] Keys of folds, fold_i for the ith fold. folds_nums [int] Values of folds, i for the ith fold. folds_map {int str}): Mapping of folds_nums to folds_keys folds [int] Alias for folds_nums results RecursiveDotDict all raw results in dict-like form. Source code in matbench/task.py class MatbenchTask ( MSONable , MSONable2File ): \"\"\"The core interface for running a Matbench task and recording its results. MatbenchTask handles creating training/validation and testing sets, as well as recording and managing all data in a consistent fashion. MatbenchTask also validates data according to te specifications in the validation file. MatbenchTasks have a few core methods: - MatbenchTask.get_train_and_val_data: Get nested cross validation data to be used for all training and validation. - MatbenchTask.get_test_data: Get test data for nested cross validation. - MatbenchTask.record: Record your predicted results for the test data. - MatbenchTask.validate: Check to make sure the data you recorded for this task is valid. You can iterate through the folds of a matbench task using .folds and the .get_*_data methods. You can load the results of a task without having to load large datasets themselves. However, to get training and testing data, you must load the datasets. Tasks loaded from files do not automatically load the dataset into memory; to load a dataset into memory, use MatbenchTask.load(). See the full documentation online for more info and tutorials on using MatbenchTask. Attributes: benchmark_name (str): The name of the benchmark this task belongs to. df (pd.DataFrame): the dataframe of the dataset for this task info (str): Info about this dataset metadata (RecursiveDotDict): all metadata about this dataset validation (RecursiveDotDict): The validation specification for this task, including the training and testing splits for each fold. folds_keys ([str]): Keys of folds, fold_i for the ith fold. folds_nums ([int]): Values of folds, i for the ith fold. folds_map ({int: str}): Mapping of folds_nums to folds_keys folds ([int]): Alias for folds_nums results (RecursiveDotDict): all raw results in dict-like form. \"\"\" _RESULTS_KEY = \"results\" _BENCHMARK_KEY = \"benchmark_name\" _DATASET_KEY = \"dataset_name\" _DATA_KEY = \"data\" _PARAMS_KEY = \"parameters\" _SCORES_KEY = \"scores\" def __init__ ( self , dataset_name , autoload = True , benchmark = MBV01_KEY ): \"\"\" Args: dataset_name (str): Name of the task. Must belong to the benchmark given in the 'benchmark' argument. autoload (bool): If True, will load the benchmark's raw data. This includes deserializing many large structures for some datasets, so loading make take some time. If False, you will need to run .load() before running .get_*_data() methods. benchmark (str): Name of the benchmark this task belongs to. \"\"\" self . dataset_name = dataset_name self . df = load ( self . dataset_name ) if autoload else None self . info = get_all_dataset_info ( dataset_name ) # define all static data needed for this task # including citations, data size, as well as specific validation splits if benchmark == MBV01_KEY : self . benchmark_name = MBV01_KEY self . metadata = mbv01_metadata [ dataset_name ] self . validation = mbv01_validation . splits [ dataset_name ] else : raise ValueError ( f \"Only { MBV01_KEY } available. No other benchmarks defined!\" ) # keeping track of folds self . folds_keys = list ( self . validation . keys ()) self . folds_nums = list ( range ( len ( self . folds_keys ))) self . folds_map = dict ( zip ( self . folds_nums , self . folds_keys )) # Alias for ease of use self . folds = self . folds_nums self . results = RecursiveDotDict ({}) def _get_data_from_df ( self , ids , as_type ): \"\"\"Private function to get fold data from the task dataframe. Args: ids (list-like): List of string indices to grab from the df. as_type (str): either \"df\" or \"tuple\". If \"df\", returns the data as a subset of the task df. If \"tuple\", returns list-likes of the inputs and outputs as a 2-tuple. Returns: (pd.DataFrame or (list-like, list-like)) \"\"\" relevant_df = self . df . loc [ ids ] if as_type == \"df\" : return relevant_df elif as_type == \"tuple\" : # inputs, outputs return ( relevant_df [ self . metadata . input_type ], relevant_df [ self . metadata . target ], ) def _check_is_loaded ( self ): \"\"\"Private method to check if the dataset is loaded. Throws error if the dataset is not loaded. Returns: None \"\"\" if self . df is None : raise ValueError ( \"Task dataset is not loaded! Run MatbenchTask.load() to \" \"load the dataset into memory.\" ) def _check_all_folds_recorded ( self , msg ): \"\"\"Private method to check if all folds have been recorded. Throws error if all folds have not been recorded. Args: msg (str): Error message to be displayed. Returns: None \"\"\" if not self . all_folds_recorded : raise ValueError ( f \" { msg } ; folds \" f \" { [ f for f in self . is_recorded if not self . is_recorded [ f ]] } \" f \"not recorded!\" ) @classmethod def from_dict ( cls , d ): \"\"\"Create a MatbenchTask from a dictionary input. Required method from MSONable. Args: d (dict): Returns: (MatbenchTask) \"\"\" req_base_keys = [ \"@module\" , \"@class\" , cls . _DATASET_KEY , cls . _RESULTS_KEY , cls . _BENCHMARK_KEY , ] for k in req_base_keys : if k not in d : raise KeyError ( f \"Required key ' { k } ' not found.\" ) extra_base_keys = [ k for k in d . keys () if k not in req_base_keys ] if extra_base_keys : raise KeyError ( f \"Extra keys { extra_base_keys } not allowed.\" ) return cls . _from_args ( dataset_name = d [ cls . _DATASET_KEY ], benchmark_name = d [ cls . _BENCHMARK_KEY ], results_dict = d [ cls . _RESULTS_KEY ], ) @classmethod def _from_args ( cls , dataset_name , benchmark_name , results_dict ): \"\"\"Instantiate a MatbenchTask from a arguments Args: dataset_name (str): The name of the dataset/task benchmark_name (str): The name of the corresponding benchmark results_dict (dict): A formatted dictionary of raw results. Returns: (MatbenchTask) \"\"\" obj = cls ( dataset_name , autoload = False , benchmark = benchmark_name ) obj . results = RecursiveDotDict ( results_dict ) obj . validate () return obj def load ( self ): \"\"\"Load the dataset for this task into memory. Returns: None \"\"\" if self . df is None : logger . info ( f \"Loading dataset ' { self . dataset_name } '...\" ) self . df = load ( self . dataset_name ) logger . info ( f \"Dataset ' { self . dataset_name } loaded.\" ) else : logger . info ( f \"Dataset { self . dataset_name } already loaded; \" f \"not reloading dataset.\" ) def get_info ( self ): logger . info ( self . info ) def get_train_and_val_data ( self , fold_number , as_type = \"tuple\" ): \"\"\" The training + validation data. All model tuning and hyperparameter selection must be done on this data, NOT test data. Args: fold_number: Returns: \"\"\" self . _check_is_loaded () fold_key = self . folds_map [ fold_number ] ids = self . validation [ fold_key ] . train return self . _get_data_from_df ( ids , as_type ) def get_test_data ( self , fold_number , as_type = \"tuple\" , include_target = False ): \"\"\" The test data used for recording benchmarks. Args: fold_number: Returns: \"\"\" self . _check_is_loaded () fold_key = self . folds_map [ fold_number ] ids = self . validation [ fold_key ] . test if include_target : return self . _get_data_from_df ( ids , as_type ) else : if as_type == \"tuple\" : return self . _get_data_from_df ( ids , as_type )[ 0 ] elif as_type == \"df\" : return self . _get_data_from_df ( ids , as_type )[ [ self . metadata . input_type ] ] def record ( self , fold_number , predictions , params = None ): \"\"\"Record the test data as well as parameters about the model trained on this fold. Args: fold_number (int): The fold number. predictions ([float] or [bool] or np.ndarray): A list of predictions for fold number {fold_number} params (dict): Any free-form parameters for information about the algorithm on this fold. For example, hyperparameters determined during validation. Parameters must be a dictionary; dictionary types must adhere to the same requirements as in the MatbenchBenchmark.add_metadata docstring. Returns: None \"\"\" if self . is_recorded [ fold_number ]: logger . error ( f \"Fold number { fold_number } already recorded! Aborting record...\" ) else : # avoid problems with json serialization if isinstance ( predictions , np . ndarray ): predictions = predictions . tolist () fold_key = self . folds_map [ fold_number ] # create map of original df index to prediction, e.g., # {ix_of_original_df1: prediction1, ... etc.} split_ids = self . validation [ fold_key ] . test if len ( predictions ) != len ( split_ids ): raise ValueError ( f \"Prediction outputs must be the same length as the \" f \"inputs! { len ( predictions ) } != { len ( split_ids ) } \" ) ids_to_predictions = { split_ids [ i ]: p for i , p in enumerate ( predictions )} self . results [ fold_key ][ self . _DATA_KEY ] = ids_to_predictions if not isinstance ( params , ( dict , type ( None ))): raise TypeError ( f \"Parameters must be stored as a dictionary, not { type ( params ) } !\" ) params = immutify_dictionary ( params ) if params else params self . results [ fold_key ][ self . _PARAMS_KEY ] = params if params else {} self . is_recorded [ fold_number ] = True logger . info ( f \"Recorded fold \" f \" { self . dataset_name } - { fold_number } successfully.\" ) truth = self . _get_data_from_df ( split_ids , as_type = \"tuple\" )[ 1 ] self . results [ fold_key ][ self . _SCORES_KEY ] = score_array ( truth , predictions , self . metadata . task_type ) logger . debug ( f \"Scored fold '\" f \" { self . dataset_name } - { fold_key } successfully.\" ) def as_dict ( self ): \"\"\"Return a MatbenchTask object as a dictionary. Required method from MSONAble. Returns: (dict) \"\"\" return { \"@module\" : self . __class__ . __module__ , \"@class\" : self . __class__ . __name__ , self . _BENCHMARK_KEY : self . benchmark_name , self . _DATASET_KEY : self . dataset_name , self . _RESULTS_KEY : dict ( self . results ), } def validate ( self ): \"\"\"Validate a task after all folds have been recorded. There are a few requirements for a task to be validated: - Data types of each predicted sample must match those specified by the validation procedure - All folds must be recorded - There must be no extra or missing required keys from the data, including indices. Every index specified in the validation procedure must be present in its correct fold, and no extras may be present. Returns: \"\"\" self . _check_all_folds_recorded ( f \"Cannot validate task { self . dataset_name } \" f \"unless all folds recorded!\" ) task_type = self . metadata . task_type # Check for extra fold keys extra_fold_keys = [ k for k in self . results if k not in self . folds_keys ] if extra_fold_keys : raise KeyError ( f \"Extra fold keys { extra_fold_keys } for task \" f \" { self . dataset_name } not allowed.\" ) for fold_key in self . folds_keys : if fold_key not in self . results : raise KeyError ( f \"Required fold data for fold ' { fold_key } ' \" f \"for task { self . dataset_name } not found.\" ) # Check for extra or missing keys inside each fold: # need params, scores, and data. req_subfold_keys = [ self . _SCORES_KEY , self . _DATA_KEY , self . _PARAMS_KEY ] extra_subfold_keys = [ k for k in self . results [ fold_key ] if k not in req_subfold_keys ] if extra_subfold_keys : raise KeyError ( f \"Extra keys { extra_subfold_keys } for fold results of \" f \"' { fold_key } ' for task { self . dataset_name } not allowed.\" ) for subkey in req_subfold_keys : fold_results = self . results [ fold_key ] if subkey not in fold_results : raise KeyError ( f \"Required key ' { subkey } ' for task { self . dataset_name } \" f \"not found for fold ' { fold_key } '.\" ) if subkey == self . _SCORES_KEY : scores = self . results [ fold_key ][ subkey ] metrics = REG_METRICS if task_type == REG_KEY else CLF_METRICS for m in metrics : if m not in scores : raise KeyError ( f \"Required score ' { m } ' for task \" f \" { self . dataset_name } \" f \"not found for ' { fold_key } '.\" ) elif not isinstance ( scores [ m ], float ): raise TypeError ( f \"Required score ' { m } ' for task \" f \" { self . dataset_name } \" f \"is not float-type for ' { fold_key } '!\" ) extra_metrics = [ k for k in scores if k not in metrics ] if extra_metrics : raise KeyError ( f \"Extra keys { extra_metrics } for fold scores of \" f \"' { fold_key } ' for task { self . dataset_name } \" f \"not allowed.\" ) # results data indices are cast by json to be strings, # so must be converted to int elif subkey == self . _DATA_KEY : fold_data = self . results [ fold_key ] . data # Ensure all the indices are present with no # extras for each fold req_indices = set ( self . validation [ fold_key ] . test ) remaining_indices = copy . deepcopy ( req_indices ) extra_indices = {} if self . metadata . task_type == REG_KEY : allowed_types = ( float ,) else : allowed_types = ( bool , float ) for ix , datum in fold_data . items (): if ix not in req_indices : extra_indices [ ix ] = datum else : if not isinstance ( datum , allowed_types ): raise TypeError ( f \"Data point ' { ix } : { datum } ' has data type \" f \" { type ( datum ) } while required type is \" f \" { allowed_types } for task \" f \" { self . dataset_name } !\" ) if self . metadata . task_type == CLF_KEY : if isinstance ( datum , float ): if datum < 0 or datum > 1 : raise ValueError ( f \"Probability estimate ' { ix } ': { datum } \" f \"for task { self . dataset_name } outside \" f \"of range [0, 1].\" ) remaining_indices . remove ( ix ) if extra_indices and not remaining_indices : raise ValueError ( f \" { len ( extra_indices ) } extra indices for problem \" f \" { self . dataset_name } are not allowed (found in \" f \" { fold_key } : { remaining_indices } \" ) elif not extra_indices and remaining_indices : raise ValueError ( f \" { len ( remaining_indices ) } required indices \" f \"for problem { self . dataset_name } not \" f \"found for { fold_key } : { remaining_indices } \" ) elif extra_indices and remaining_indices : raise ValueError ( f \" { len ( remaining_indices ) } required indices \" f \"for problem { self . dataset_name } not \" f \"found and { len ( extra_indices ) } not \" f \"allowed indices found for { fold_key } !\" ) else : pass # Params key has no required form; # it is up to the model to determine it. logger . debug ( f \"Data for { self . dataset_name } successfully validated.\" ) @property def scores ( self ): \"\"\"Comprehensive score metrics for this task. Gets means, maxes, mins, and more distribution stats (across folds) for all scoring metrics defined for this task. There will be different scores for classification problems and regression problems. Returns: (dict): A dictionary of all the scores for this \"\"\" metric_keys = ( REG_METRICS if self . metadata . task_type == REG_KEY else CLF_METRICS ) scores = {} self . _check_all_folds_recorded ( \"Cannot score unless all folds are recorded!\" ) for mk in metric_keys : metric = {} # scores for a metric among all folds raw_metrics_on_folds = [ self . results [ fk ][ self . _SCORES_KEY ][ mk ] for fk in self . folds_map . values () ] for op in FOLD_DIST_METRICS : metric [ op ] = getattr ( np , op )( raw_metrics_on_folds ) scores [ mk ] = metric return RecursiveDotDict ( scores ) @property def is_recorded ( self ): \"\"\"Determine what folds in the task are recorded. Returns: ({int: bool}): Keys are fold numbers, values are whether the fold is recorded or not. \"\"\" is_recorded = {} for fnum , fkey in self . folds_map . items (): if self . results [ fkey ][ self . _DATA_KEY ]: is_recorded [ fnum ] = True else : is_recorded [ fnum ] = False return is_recorded @property def all_folds_recorded ( self ): \"\"\"Determine if all folds are recorded. Returns: (bool): True if all folds are recorded, False otherwise. \"\"\" return all ([ v for v in self . is_recorded . values ()]) @property def has_polymorphs ( self ): \"\"\"Determine if a task's raw data contains polymorphs. Returns: (bool) If true, contains polymorphs. \"\"\" checker_key = \"pmg_composition\" self . _check_is_loaded () if self . metadata . input_type == \"composition\" : stc = StrToComposition ( target_col_id = checker_key , reduce = True ) comps = stc . featurize_dataframe ( self . df , \"composition\" )[ checker_key ] . values elif self . metadata . input_type == \"structure\" : stc = StructureToComposition ( target_col_id = checker_key , reduce = True ) comps = stc . featurize_dataframe ( self . df , \"structure\" )[ checker_key ] . values else : raise ValueError ( \"Cannot check for polymorphs without input type in \" \"(structure, composition)!\" ) unique_comps = set ( comps ) if len ( unique_comps ) != len ( comps ): return True else : return False all_folds_recorded property readonly Determine if all folds are recorded. Returns: Type Description (bool) True if all folds are recorded, False otherwise. has_polymorphs property readonly Determine if a task's raw data contains polymorphs. Returns: Type Description (bool) If true, contains polymorphs. is_recorded property readonly Determine what folds in the task are recorded. Returns: Type Description ({int bool}): Keys are fold numbers, values are whether the fold is recorded or not. scores property readonly Comprehensive score metrics for this task. Gets means, maxes, mins, and more distribution stats (across folds) for all scoring metrics defined for this task. There will be different scores for classification problems and regression problems. Returns: Type Description (dict) A dictionary of all the scores for this __init__ ( self , dataset_name , autoload = True , benchmark = 'matbench_v0.1' ) special Parameters: Name Type Description Default dataset_name str Name of the task. Must belong to the benchmark given in the 'benchmark' argument. required autoload bool If True, will load the benchmark's raw data. This includes deserializing many large structures for some datasets, so loading make take some time. If False, you will need to run .load() before running .get_*_data() methods. True benchmark str Name of the benchmark this task belongs to. 'matbench_v0.1' Source code in matbench/task.py def __init__ ( self , dataset_name , autoload = True , benchmark = MBV01_KEY ): \"\"\" Args: dataset_name (str): Name of the task. Must belong to the benchmark given in the 'benchmark' argument. autoload (bool): If True, will load the benchmark's raw data. This includes deserializing many large structures for some datasets, so loading make take some time. If False, you will need to run .load() before running .get_*_data() methods. benchmark (str): Name of the benchmark this task belongs to. \"\"\" self . dataset_name = dataset_name self . df = load ( self . dataset_name ) if autoload else None self . info = get_all_dataset_info ( dataset_name ) # define all static data needed for this task # including citations, data size, as well as specific validation splits if benchmark == MBV01_KEY : self . benchmark_name = MBV01_KEY self . metadata = mbv01_metadata [ dataset_name ] self . validation = mbv01_validation . splits [ dataset_name ] else : raise ValueError ( f \"Only { MBV01_KEY } available. No other benchmarks defined!\" ) # keeping track of folds self . folds_keys = list ( self . validation . keys ()) self . folds_nums = list ( range ( len ( self . folds_keys ))) self . folds_map = dict ( zip ( self . folds_nums , self . folds_keys )) # Alias for ease of use self . folds = self . folds_nums self . results = RecursiveDotDict ({}) as_dict ( self ) Return a MatbenchTask object as a dictionary. Required method from MSONAble. Returns: Type Description (dict) Source code in matbench/task.py def as_dict ( self ): \"\"\"Return a MatbenchTask object as a dictionary. Required method from MSONAble. Returns: (dict) \"\"\" return { \"@module\" : self . __class__ . __module__ , \"@class\" : self . __class__ . __name__ , self . _BENCHMARK_KEY : self . benchmark_name , self . _DATASET_KEY : self . dataset_name , self . _RESULTS_KEY : dict ( self . results ), } from_dict ( d ) classmethod Create a MatbenchTask from a dictionary input. Required method from MSONable. Parameters: Name Type Description Default d dict required Returns: Type Description (MatbenchTask) Source code in matbench/task.py @classmethod def from_dict ( cls , d ): \"\"\"Create a MatbenchTask from a dictionary input. Required method from MSONable. Args: d (dict): Returns: (MatbenchTask) \"\"\" req_base_keys = [ \"@module\" , \"@class\" , cls . _DATASET_KEY , cls . _RESULTS_KEY , cls . _BENCHMARK_KEY , ] for k in req_base_keys : if k not in d : raise KeyError ( f \"Required key ' { k } ' not found.\" ) extra_base_keys = [ k for k in d . keys () if k not in req_base_keys ] if extra_base_keys : raise KeyError ( f \"Extra keys { extra_base_keys } not allowed.\" ) return cls . _from_args ( dataset_name = d [ cls . _DATASET_KEY ], benchmark_name = d [ cls . _BENCHMARK_KEY ], results_dict = d [ cls . _RESULTS_KEY ], ) get_test_data ( self , fold_number , as_type = 'tuple' , include_target = False ) The test data used for recording benchmarks. Parameters: Name Type Description Default fold_number required Source code in matbench/task.py def get_test_data ( self , fold_number , as_type = \"tuple\" , include_target = False ): \"\"\" The test data used for recording benchmarks. Args: fold_number: Returns: \"\"\" self . _check_is_loaded () fold_key = self . folds_map [ fold_number ] ids = self . validation [ fold_key ] . test if include_target : return self . _get_data_from_df ( ids , as_type ) else : if as_type == \"tuple\" : return self . _get_data_from_df ( ids , as_type )[ 0 ] elif as_type == \"df\" : return self . _get_data_from_df ( ids , as_type )[ [ self . metadata . input_type ] ] get_train_and_val_data ( self , fold_number , as_type = 'tuple' ) The training + validation data. All model tuning and hyperparameter selection must be done on this data, NOT test data. Parameters: Name Type Description Default fold_number required Source code in matbench/task.py def get_train_and_val_data ( self , fold_number , as_type = \"tuple\" ): \"\"\" The training + validation data. All model tuning and hyperparameter selection must be done on this data, NOT test data. Args: fold_number: Returns: \"\"\" self . _check_is_loaded () fold_key = self . folds_map [ fold_number ] ids = self . validation [ fold_key ] . train return self . _get_data_from_df ( ids , as_type ) load ( self ) Load the dataset for this task into memory. Returns: Type Description None Source code in matbench/task.py def load ( self ): \"\"\"Load the dataset for this task into memory. Returns: None \"\"\" if self . df is None : logger . info ( f \"Loading dataset ' { self . dataset_name } '...\" ) self . df = load ( self . dataset_name ) logger . info ( f \"Dataset ' { self . dataset_name } loaded.\" ) else : logger . info ( f \"Dataset { self . dataset_name } already loaded; \" f \"not reloading dataset.\" ) record ( self , fold_number , predictions , params = None ) Record the test data as well as parameters about the model trained on this fold. Parameters: Name Type Description Default fold_number int The fold number. required predictions [float] or [bool] or np.ndarray A list of predictions for fold number {fold_number} required params dict Any free-form parameters for information about the algorithm on this fold. For example, hyperparameters determined during validation. Parameters must be a dictionary; dictionary types must adhere to the same requirements as in the MatbenchBenchmark.add_metadata docstring. None Returns: Type Description None Source code in matbench/task.py def record ( self , fold_number , predictions , params = None ): \"\"\"Record the test data as well as parameters about the model trained on this fold. Args: fold_number (int): The fold number. predictions ([float] or [bool] or np.ndarray): A list of predictions for fold number {fold_number} params (dict): Any free-form parameters for information about the algorithm on this fold. For example, hyperparameters determined during validation. Parameters must be a dictionary; dictionary types must adhere to the same requirements as in the MatbenchBenchmark.add_metadata docstring. Returns: None \"\"\" if self . is_recorded [ fold_number ]: logger . error ( f \"Fold number { fold_number } already recorded! Aborting record...\" ) else : # avoid problems with json serialization if isinstance ( predictions , np . ndarray ): predictions = predictions . tolist () fold_key = self . folds_map [ fold_number ] # create map of original df index to prediction, e.g., # {ix_of_original_df1: prediction1, ... etc.} split_ids = self . validation [ fold_key ] . test if len ( predictions ) != len ( split_ids ): raise ValueError ( f \"Prediction outputs must be the same length as the \" f \"inputs! { len ( predictions ) } != { len ( split_ids ) } \" ) ids_to_predictions = { split_ids [ i ]: p for i , p in enumerate ( predictions )} self . results [ fold_key ][ self . _DATA_KEY ] = ids_to_predictions if not isinstance ( params , ( dict , type ( None ))): raise TypeError ( f \"Parameters must be stored as a dictionary, not { type ( params ) } !\" ) params = immutify_dictionary ( params ) if params else params self . results [ fold_key ][ self . _PARAMS_KEY ] = params if params else {} self . is_recorded [ fold_number ] = True logger . info ( f \"Recorded fold \" f \" { self . dataset_name } - { fold_number } successfully.\" ) truth = self . _get_data_from_df ( split_ids , as_type = \"tuple\" )[ 1 ] self . results [ fold_key ][ self . _SCORES_KEY ] = score_array ( truth , predictions , self . metadata . task_type ) logger . debug ( f \"Scored fold '\" f \" { self . dataset_name } - { fold_key } successfully.\" ) validate ( self ) Validate a task after all folds have been recorded. There are a few requirements for a task to be validated: - Data types of each predicted sample must match those specified by the validation procedure - All folds must be recorded - There must be no extra or missing required keys from the data, including indices. Every index specified in the validation procedure must be present in its correct fold, and no extras may be present. Source code in matbench/task.py def validate ( self ): \"\"\"Validate a task after all folds have been recorded. There are a few requirements for a task to be validated: - Data types of each predicted sample must match those specified by the validation procedure - All folds must be recorded - There must be no extra or missing required keys from the data, including indices. Every index specified in the validation procedure must be present in its correct fold, and no extras may be present. Returns: \"\"\" self . _check_all_folds_recorded ( f \"Cannot validate task { self . dataset_name } \" f \"unless all folds recorded!\" ) task_type = self . metadata . task_type # Check for extra fold keys extra_fold_keys = [ k for k in self . results if k not in self . folds_keys ] if extra_fold_keys : raise KeyError ( f \"Extra fold keys { extra_fold_keys } for task \" f \" { self . dataset_name } not allowed.\" ) for fold_key in self . folds_keys : if fold_key not in self . results : raise KeyError ( f \"Required fold data for fold ' { fold_key } ' \" f \"for task { self . dataset_name } not found.\" ) # Check for extra or missing keys inside each fold: # need params, scores, and data. req_subfold_keys = [ self . _SCORES_KEY , self . _DATA_KEY , self . _PARAMS_KEY ] extra_subfold_keys = [ k for k in self . results [ fold_key ] if k not in req_subfold_keys ] if extra_subfold_keys : raise KeyError ( f \"Extra keys { extra_subfold_keys } for fold results of \" f \"' { fold_key } ' for task { self . dataset_name } not allowed.\" ) for subkey in req_subfold_keys : fold_results = self . results [ fold_key ] if subkey not in fold_results : raise KeyError ( f \"Required key ' { subkey } ' for task { self . dataset_name } \" f \"not found for fold ' { fold_key } '.\" ) if subkey == self . _SCORES_KEY : scores = self . results [ fold_key ][ subkey ] metrics = REG_METRICS if task_type == REG_KEY else CLF_METRICS for m in metrics : if m not in scores : raise KeyError ( f \"Required score ' { m } ' for task \" f \" { self . dataset_name } \" f \"not found for ' { fold_key } '.\" ) elif not isinstance ( scores [ m ], float ): raise TypeError ( f \"Required score ' { m } ' for task \" f \" { self . dataset_name } \" f \"is not float-type for ' { fold_key } '!\" ) extra_metrics = [ k for k in scores if k not in metrics ] if extra_metrics : raise KeyError ( f \"Extra keys { extra_metrics } for fold scores of \" f \"' { fold_key } ' for task { self . dataset_name } \" f \"not allowed.\" ) # results data indices are cast by json to be strings, # so must be converted to int elif subkey == self . _DATA_KEY : fold_data = self . results [ fold_key ] . data # Ensure all the indices are present with no # extras for each fold req_indices = set ( self . validation [ fold_key ] . test ) remaining_indices = copy . deepcopy ( req_indices ) extra_indices = {} if self . metadata . task_type == REG_KEY : allowed_types = ( float ,) else : allowed_types = ( bool , float ) for ix , datum in fold_data . items (): if ix not in req_indices : extra_indices [ ix ] = datum else : if not isinstance ( datum , allowed_types ): raise TypeError ( f \"Data point ' { ix } : { datum } ' has data type \" f \" { type ( datum ) } while required type is \" f \" { allowed_types } for task \" f \" { self . dataset_name } !\" ) if self . metadata . task_type == CLF_KEY : if isinstance ( datum , float ): if datum < 0 or datum > 1 : raise ValueError ( f \"Probability estimate ' { ix } ': { datum } \" f \"for task { self . dataset_name } outside \" f \"of range [0, 1].\" ) remaining_indices . remove ( ix ) if extra_indices and not remaining_indices : raise ValueError ( f \" { len ( extra_indices ) } extra indices for problem \" f \" { self . dataset_name } are not allowed (found in \" f \" { fold_key } : { remaining_indices } \" ) elif not extra_indices and remaining_indices : raise ValueError ( f \" { len ( remaining_indices ) } required indices \" f \"for problem { self . dataset_name } not \" f \"found for { fold_key } : { remaining_indices } \" ) elif extra_indices and remaining_indices : raise ValueError ( f \" { len ( remaining_indices ) } required indices \" f \"for problem { self . dataset_name } not \" f \"found and { len ( extra_indices ) } not \" f \"allowed indices found for { fold_key } !\" ) else : pass # Params key has no required form; # it is up to the model to determine it. logger . debug ( f \"Data for { self . dataset_name } successfully validated.\" )","title":"MatbenchTask"},{"location":"Reference/MatbenchTask/#matbenchtask","text":"The core interface for running a Matbench task and recording its results. MatbenchTask handles creating training/validation and testing sets, as well as recording and managing all data in a consistent fashion. MatbenchTask also validates data according to te specifications in the validation file. MatbenchTasks have a few core methods: MatbenchTask.get_train_and_val_data: Get nested cross validation data to be used for all training and validation. MatbenchTask.get_test_data: Get test data for nested cross validation. MatbenchTask.record: Record your predicted results for the test data. MatbenchTask.validate: Check to make sure the data you recorded for this task is valid. You can iterate through the folds of a matbench task using .folds and the .get_*_data methods. You can load the results of a task without having to load large datasets themselves. However, to get training and testing data, you must load the datasets. Tasks loaded from files do not automatically load the dataset into memory; to load a dataset into memory, use MatbenchTask.load(). See the full documentation online for more info and tutorials on using MatbenchTask. Attributes: Name Type Description benchmark_name str The name of the benchmark this task belongs to. df pd.DataFrame the dataframe of the dataset for this task info str Info about this dataset metadata RecursiveDotDict all metadata about this dataset validation RecursiveDotDict The validation specification for this task, including the training and testing splits for each fold. folds_keys [str] Keys of folds, fold_i for the ith fold. folds_nums [int] Values of folds, i for the ith fold. folds_map {int str}): Mapping of folds_nums to folds_keys folds [int] Alias for folds_nums results RecursiveDotDict all raw results in dict-like form. Source code in matbench/task.py class MatbenchTask ( MSONable , MSONable2File ): \"\"\"The core interface for running a Matbench task and recording its results. MatbenchTask handles creating training/validation and testing sets, as well as recording and managing all data in a consistent fashion. MatbenchTask also validates data according to te specifications in the validation file. MatbenchTasks have a few core methods: - MatbenchTask.get_train_and_val_data: Get nested cross validation data to be used for all training and validation. - MatbenchTask.get_test_data: Get test data for nested cross validation. - MatbenchTask.record: Record your predicted results for the test data. - MatbenchTask.validate: Check to make sure the data you recorded for this task is valid. You can iterate through the folds of a matbench task using .folds and the .get_*_data methods. You can load the results of a task without having to load large datasets themselves. However, to get training and testing data, you must load the datasets. Tasks loaded from files do not automatically load the dataset into memory; to load a dataset into memory, use MatbenchTask.load(). See the full documentation online for more info and tutorials on using MatbenchTask. Attributes: benchmark_name (str): The name of the benchmark this task belongs to. df (pd.DataFrame): the dataframe of the dataset for this task info (str): Info about this dataset metadata (RecursiveDotDict): all metadata about this dataset validation (RecursiveDotDict): The validation specification for this task, including the training and testing splits for each fold. folds_keys ([str]): Keys of folds, fold_i for the ith fold. folds_nums ([int]): Values of folds, i for the ith fold. folds_map ({int: str}): Mapping of folds_nums to folds_keys folds ([int]): Alias for folds_nums results (RecursiveDotDict): all raw results in dict-like form. \"\"\" _RESULTS_KEY = \"results\" _BENCHMARK_KEY = \"benchmark_name\" _DATASET_KEY = \"dataset_name\" _DATA_KEY = \"data\" _PARAMS_KEY = \"parameters\" _SCORES_KEY = \"scores\" def __init__ ( self , dataset_name , autoload = True , benchmark = MBV01_KEY ): \"\"\" Args: dataset_name (str): Name of the task. Must belong to the benchmark given in the 'benchmark' argument. autoload (bool): If True, will load the benchmark's raw data. This includes deserializing many large structures for some datasets, so loading make take some time. If False, you will need to run .load() before running .get_*_data() methods. benchmark (str): Name of the benchmark this task belongs to. \"\"\" self . dataset_name = dataset_name self . df = load ( self . dataset_name ) if autoload else None self . info = get_all_dataset_info ( dataset_name ) # define all static data needed for this task # including citations, data size, as well as specific validation splits if benchmark == MBV01_KEY : self . benchmark_name = MBV01_KEY self . metadata = mbv01_metadata [ dataset_name ] self . validation = mbv01_validation . splits [ dataset_name ] else : raise ValueError ( f \"Only { MBV01_KEY } available. No other benchmarks defined!\" ) # keeping track of folds self . folds_keys = list ( self . validation . keys ()) self . folds_nums = list ( range ( len ( self . folds_keys ))) self . folds_map = dict ( zip ( self . folds_nums , self . folds_keys )) # Alias for ease of use self . folds = self . folds_nums self . results = RecursiveDotDict ({}) def _get_data_from_df ( self , ids , as_type ): \"\"\"Private function to get fold data from the task dataframe. Args: ids (list-like): List of string indices to grab from the df. as_type (str): either \"df\" or \"tuple\". If \"df\", returns the data as a subset of the task df. If \"tuple\", returns list-likes of the inputs and outputs as a 2-tuple. Returns: (pd.DataFrame or (list-like, list-like)) \"\"\" relevant_df = self . df . loc [ ids ] if as_type == \"df\" : return relevant_df elif as_type == \"tuple\" : # inputs, outputs return ( relevant_df [ self . metadata . input_type ], relevant_df [ self . metadata . target ], ) def _check_is_loaded ( self ): \"\"\"Private method to check if the dataset is loaded. Throws error if the dataset is not loaded. Returns: None \"\"\" if self . df is None : raise ValueError ( \"Task dataset is not loaded! Run MatbenchTask.load() to \" \"load the dataset into memory.\" ) def _check_all_folds_recorded ( self , msg ): \"\"\"Private method to check if all folds have been recorded. Throws error if all folds have not been recorded. Args: msg (str): Error message to be displayed. Returns: None \"\"\" if not self . all_folds_recorded : raise ValueError ( f \" { msg } ; folds \" f \" { [ f for f in self . is_recorded if not self . is_recorded [ f ]] } \" f \"not recorded!\" ) @classmethod def from_dict ( cls , d ): \"\"\"Create a MatbenchTask from a dictionary input. Required method from MSONable. Args: d (dict): Returns: (MatbenchTask) \"\"\" req_base_keys = [ \"@module\" , \"@class\" , cls . _DATASET_KEY , cls . _RESULTS_KEY , cls . _BENCHMARK_KEY , ] for k in req_base_keys : if k not in d : raise KeyError ( f \"Required key ' { k } ' not found.\" ) extra_base_keys = [ k for k in d . keys () if k not in req_base_keys ] if extra_base_keys : raise KeyError ( f \"Extra keys { extra_base_keys } not allowed.\" ) return cls . _from_args ( dataset_name = d [ cls . _DATASET_KEY ], benchmark_name = d [ cls . _BENCHMARK_KEY ], results_dict = d [ cls . _RESULTS_KEY ], ) @classmethod def _from_args ( cls , dataset_name , benchmark_name , results_dict ): \"\"\"Instantiate a MatbenchTask from a arguments Args: dataset_name (str): The name of the dataset/task benchmark_name (str): The name of the corresponding benchmark results_dict (dict): A formatted dictionary of raw results. Returns: (MatbenchTask) \"\"\" obj = cls ( dataset_name , autoload = False , benchmark = benchmark_name ) obj . results = RecursiveDotDict ( results_dict ) obj . validate () return obj def load ( self ): \"\"\"Load the dataset for this task into memory. Returns: None \"\"\" if self . df is None : logger . info ( f \"Loading dataset ' { self . dataset_name } '...\" ) self . df = load ( self . dataset_name ) logger . info ( f \"Dataset ' { self . dataset_name } loaded.\" ) else : logger . info ( f \"Dataset { self . dataset_name } already loaded; \" f \"not reloading dataset.\" ) def get_info ( self ): logger . info ( self . info ) def get_train_and_val_data ( self , fold_number , as_type = \"tuple\" ): \"\"\" The training + validation data. All model tuning and hyperparameter selection must be done on this data, NOT test data. Args: fold_number: Returns: \"\"\" self . _check_is_loaded () fold_key = self . folds_map [ fold_number ] ids = self . validation [ fold_key ] . train return self . _get_data_from_df ( ids , as_type ) def get_test_data ( self , fold_number , as_type = \"tuple\" , include_target = False ): \"\"\" The test data used for recording benchmarks. Args: fold_number: Returns: \"\"\" self . _check_is_loaded () fold_key = self . folds_map [ fold_number ] ids = self . validation [ fold_key ] . test if include_target : return self . _get_data_from_df ( ids , as_type ) else : if as_type == \"tuple\" : return self . _get_data_from_df ( ids , as_type )[ 0 ] elif as_type == \"df\" : return self . _get_data_from_df ( ids , as_type )[ [ self . metadata . input_type ] ] def record ( self , fold_number , predictions , params = None ): \"\"\"Record the test data as well as parameters about the model trained on this fold. Args: fold_number (int): The fold number. predictions ([float] or [bool] or np.ndarray): A list of predictions for fold number {fold_number} params (dict): Any free-form parameters for information about the algorithm on this fold. For example, hyperparameters determined during validation. Parameters must be a dictionary; dictionary types must adhere to the same requirements as in the MatbenchBenchmark.add_metadata docstring. Returns: None \"\"\" if self . is_recorded [ fold_number ]: logger . error ( f \"Fold number { fold_number } already recorded! Aborting record...\" ) else : # avoid problems with json serialization if isinstance ( predictions , np . ndarray ): predictions = predictions . tolist () fold_key = self . folds_map [ fold_number ] # create map of original df index to prediction, e.g., # {ix_of_original_df1: prediction1, ... etc.} split_ids = self . validation [ fold_key ] . test if len ( predictions ) != len ( split_ids ): raise ValueError ( f \"Prediction outputs must be the same length as the \" f \"inputs! { len ( predictions ) } != { len ( split_ids ) } \" ) ids_to_predictions = { split_ids [ i ]: p for i , p in enumerate ( predictions )} self . results [ fold_key ][ self . _DATA_KEY ] = ids_to_predictions if not isinstance ( params , ( dict , type ( None ))): raise TypeError ( f \"Parameters must be stored as a dictionary, not { type ( params ) } !\" ) params = immutify_dictionary ( params ) if params else params self . results [ fold_key ][ self . _PARAMS_KEY ] = params if params else {} self . is_recorded [ fold_number ] = True logger . info ( f \"Recorded fold \" f \" { self . dataset_name } - { fold_number } successfully.\" ) truth = self . _get_data_from_df ( split_ids , as_type = \"tuple\" )[ 1 ] self . results [ fold_key ][ self . _SCORES_KEY ] = score_array ( truth , predictions , self . metadata . task_type ) logger . debug ( f \"Scored fold '\" f \" { self . dataset_name } - { fold_key } successfully.\" ) def as_dict ( self ): \"\"\"Return a MatbenchTask object as a dictionary. Required method from MSONAble. Returns: (dict) \"\"\" return { \"@module\" : self . __class__ . __module__ , \"@class\" : self . __class__ . __name__ , self . _BENCHMARK_KEY : self . benchmark_name , self . _DATASET_KEY : self . dataset_name , self . _RESULTS_KEY : dict ( self . results ), } def validate ( self ): \"\"\"Validate a task after all folds have been recorded. There are a few requirements for a task to be validated: - Data types of each predicted sample must match those specified by the validation procedure - All folds must be recorded - There must be no extra or missing required keys from the data, including indices. Every index specified in the validation procedure must be present in its correct fold, and no extras may be present. Returns: \"\"\" self . _check_all_folds_recorded ( f \"Cannot validate task { self . dataset_name } \" f \"unless all folds recorded!\" ) task_type = self . metadata . task_type # Check for extra fold keys extra_fold_keys = [ k for k in self . results if k not in self . folds_keys ] if extra_fold_keys : raise KeyError ( f \"Extra fold keys { extra_fold_keys } for task \" f \" { self . dataset_name } not allowed.\" ) for fold_key in self . folds_keys : if fold_key not in self . results : raise KeyError ( f \"Required fold data for fold ' { fold_key } ' \" f \"for task { self . dataset_name } not found.\" ) # Check for extra or missing keys inside each fold: # need params, scores, and data. req_subfold_keys = [ self . _SCORES_KEY , self . _DATA_KEY , self . _PARAMS_KEY ] extra_subfold_keys = [ k for k in self . results [ fold_key ] if k not in req_subfold_keys ] if extra_subfold_keys : raise KeyError ( f \"Extra keys { extra_subfold_keys } for fold results of \" f \"' { fold_key } ' for task { self . dataset_name } not allowed.\" ) for subkey in req_subfold_keys : fold_results = self . results [ fold_key ] if subkey not in fold_results : raise KeyError ( f \"Required key ' { subkey } ' for task { self . dataset_name } \" f \"not found for fold ' { fold_key } '.\" ) if subkey == self . _SCORES_KEY : scores = self . results [ fold_key ][ subkey ] metrics = REG_METRICS if task_type == REG_KEY else CLF_METRICS for m in metrics : if m not in scores : raise KeyError ( f \"Required score ' { m } ' for task \" f \" { self . dataset_name } \" f \"not found for ' { fold_key } '.\" ) elif not isinstance ( scores [ m ], float ): raise TypeError ( f \"Required score ' { m } ' for task \" f \" { self . dataset_name } \" f \"is not float-type for ' { fold_key } '!\" ) extra_metrics = [ k for k in scores if k not in metrics ] if extra_metrics : raise KeyError ( f \"Extra keys { extra_metrics } for fold scores of \" f \"' { fold_key } ' for task { self . dataset_name } \" f \"not allowed.\" ) # results data indices are cast by json to be strings, # so must be converted to int elif subkey == self . _DATA_KEY : fold_data = self . results [ fold_key ] . data # Ensure all the indices are present with no # extras for each fold req_indices = set ( self . validation [ fold_key ] . test ) remaining_indices = copy . deepcopy ( req_indices ) extra_indices = {} if self . metadata . task_type == REG_KEY : allowed_types = ( float ,) else : allowed_types = ( bool , float ) for ix , datum in fold_data . items (): if ix not in req_indices : extra_indices [ ix ] = datum else : if not isinstance ( datum , allowed_types ): raise TypeError ( f \"Data point ' { ix } : { datum } ' has data type \" f \" { type ( datum ) } while required type is \" f \" { allowed_types } for task \" f \" { self . dataset_name } !\" ) if self . metadata . task_type == CLF_KEY : if isinstance ( datum , float ): if datum < 0 or datum > 1 : raise ValueError ( f \"Probability estimate ' { ix } ': { datum } \" f \"for task { self . dataset_name } outside \" f \"of range [0, 1].\" ) remaining_indices . remove ( ix ) if extra_indices and not remaining_indices : raise ValueError ( f \" { len ( extra_indices ) } extra indices for problem \" f \" { self . dataset_name } are not allowed (found in \" f \" { fold_key } : { remaining_indices } \" ) elif not extra_indices and remaining_indices : raise ValueError ( f \" { len ( remaining_indices ) } required indices \" f \"for problem { self . dataset_name } not \" f \"found for { fold_key } : { remaining_indices } \" ) elif extra_indices and remaining_indices : raise ValueError ( f \" { len ( remaining_indices ) } required indices \" f \"for problem { self . dataset_name } not \" f \"found and { len ( extra_indices ) } not \" f \"allowed indices found for { fold_key } !\" ) else : pass # Params key has no required form; # it is up to the model to determine it. logger . debug ( f \"Data for { self . dataset_name } successfully validated.\" ) @property def scores ( self ): \"\"\"Comprehensive score metrics for this task. Gets means, maxes, mins, and more distribution stats (across folds) for all scoring metrics defined for this task. There will be different scores for classification problems and regression problems. Returns: (dict): A dictionary of all the scores for this \"\"\" metric_keys = ( REG_METRICS if self . metadata . task_type == REG_KEY else CLF_METRICS ) scores = {} self . _check_all_folds_recorded ( \"Cannot score unless all folds are recorded!\" ) for mk in metric_keys : metric = {} # scores for a metric among all folds raw_metrics_on_folds = [ self . results [ fk ][ self . _SCORES_KEY ][ mk ] for fk in self . folds_map . values () ] for op in FOLD_DIST_METRICS : metric [ op ] = getattr ( np , op )( raw_metrics_on_folds ) scores [ mk ] = metric return RecursiveDotDict ( scores ) @property def is_recorded ( self ): \"\"\"Determine what folds in the task are recorded. Returns: ({int: bool}): Keys are fold numbers, values are whether the fold is recorded or not. \"\"\" is_recorded = {} for fnum , fkey in self . folds_map . items (): if self . results [ fkey ][ self . _DATA_KEY ]: is_recorded [ fnum ] = True else : is_recorded [ fnum ] = False return is_recorded @property def all_folds_recorded ( self ): \"\"\"Determine if all folds are recorded. Returns: (bool): True if all folds are recorded, False otherwise. \"\"\" return all ([ v for v in self . is_recorded . values ()]) @property def has_polymorphs ( self ): \"\"\"Determine if a task's raw data contains polymorphs. Returns: (bool) If true, contains polymorphs. \"\"\" checker_key = \"pmg_composition\" self . _check_is_loaded () if self . metadata . input_type == \"composition\" : stc = StrToComposition ( target_col_id = checker_key , reduce = True ) comps = stc . featurize_dataframe ( self . df , \"composition\" )[ checker_key ] . values elif self . metadata . input_type == \"structure\" : stc = StructureToComposition ( target_col_id = checker_key , reduce = True ) comps = stc . featurize_dataframe ( self . df , \"structure\" )[ checker_key ] . values else : raise ValueError ( \"Cannot check for polymorphs without input type in \" \"(structure, composition)!\" ) unique_comps = set ( comps ) if len ( unique_comps ) != len ( comps ): return True else : return False","title":"MatbenchTask"},{"location":"Reference/MatbenchTask/#matbench.task.MatbenchTask.all_folds_recorded","text":"Determine if all folds are recorded. Returns: Type Description (bool) True if all folds are recorded, False otherwise.","title":"all_folds_recorded"},{"location":"Reference/MatbenchTask/#matbench.task.MatbenchTask.has_polymorphs","text":"Determine if a task's raw data contains polymorphs. Returns: Type Description (bool) If true, contains polymorphs.","title":"has_polymorphs"},{"location":"Reference/MatbenchTask/#matbench.task.MatbenchTask.is_recorded","text":"Determine what folds in the task are recorded. Returns: Type Description ({int bool}): Keys are fold numbers, values are whether the fold is recorded or not.","title":"is_recorded"},{"location":"Reference/MatbenchTask/#matbench.task.MatbenchTask.scores","text":"Comprehensive score metrics for this task. Gets means, maxes, mins, and more distribution stats (across folds) for all scoring metrics defined for this task. There will be different scores for classification problems and regression problems. Returns: Type Description (dict) A dictionary of all the scores for this","title":"scores"},{"location":"Reference/MatbenchTask/#matbench.task.MatbenchTask.__init__","text":"Parameters: Name Type Description Default dataset_name str Name of the task. Must belong to the benchmark given in the 'benchmark' argument. required autoload bool If True, will load the benchmark's raw data. This includes deserializing many large structures for some datasets, so loading make take some time. If False, you will need to run .load() before running .get_*_data() methods. True benchmark str Name of the benchmark this task belongs to. 'matbench_v0.1' Source code in matbench/task.py def __init__ ( self , dataset_name , autoload = True , benchmark = MBV01_KEY ): \"\"\" Args: dataset_name (str): Name of the task. Must belong to the benchmark given in the 'benchmark' argument. autoload (bool): If True, will load the benchmark's raw data. This includes deserializing many large structures for some datasets, so loading make take some time. If False, you will need to run .load() before running .get_*_data() methods. benchmark (str): Name of the benchmark this task belongs to. \"\"\" self . dataset_name = dataset_name self . df = load ( self . dataset_name ) if autoload else None self . info = get_all_dataset_info ( dataset_name ) # define all static data needed for this task # including citations, data size, as well as specific validation splits if benchmark == MBV01_KEY : self . benchmark_name = MBV01_KEY self . metadata = mbv01_metadata [ dataset_name ] self . validation = mbv01_validation . splits [ dataset_name ] else : raise ValueError ( f \"Only { MBV01_KEY } available. No other benchmarks defined!\" ) # keeping track of folds self . folds_keys = list ( self . validation . keys ()) self . folds_nums = list ( range ( len ( self . folds_keys ))) self . folds_map = dict ( zip ( self . folds_nums , self . folds_keys )) # Alias for ease of use self . folds = self . folds_nums self . results = RecursiveDotDict ({})","title":"__init__()"},{"location":"Reference/MatbenchTask/#matbench.task.MatbenchTask.as_dict","text":"Return a MatbenchTask object as a dictionary. Required method from MSONAble. Returns: Type Description (dict) Source code in matbench/task.py def as_dict ( self ): \"\"\"Return a MatbenchTask object as a dictionary. Required method from MSONAble. Returns: (dict) \"\"\" return { \"@module\" : self . __class__ . __module__ , \"@class\" : self . __class__ . __name__ , self . _BENCHMARK_KEY : self . benchmark_name , self . _DATASET_KEY : self . dataset_name , self . _RESULTS_KEY : dict ( self . results ), }","title":"as_dict()"},{"location":"Reference/MatbenchTask/#matbench.task.MatbenchTask.from_dict","text":"Create a MatbenchTask from a dictionary input. Required method from MSONable. Parameters: Name Type Description Default d dict required Returns: Type Description (MatbenchTask) Source code in matbench/task.py @classmethod def from_dict ( cls , d ): \"\"\"Create a MatbenchTask from a dictionary input. Required method from MSONable. Args: d (dict): Returns: (MatbenchTask) \"\"\" req_base_keys = [ \"@module\" , \"@class\" , cls . _DATASET_KEY , cls . _RESULTS_KEY , cls . _BENCHMARK_KEY , ] for k in req_base_keys : if k not in d : raise KeyError ( f \"Required key ' { k } ' not found.\" ) extra_base_keys = [ k for k in d . keys () if k not in req_base_keys ] if extra_base_keys : raise KeyError ( f \"Extra keys { extra_base_keys } not allowed.\" ) return cls . _from_args ( dataset_name = d [ cls . _DATASET_KEY ], benchmark_name = d [ cls . _BENCHMARK_KEY ], results_dict = d [ cls . _RESULTS_KEY ], )","title":"from_dict()"},{"location":"Reference/MatbenchTask/#matbench.task.MatbenchTask.get_test_data","text":"The test data used for recording benchmarks. Parameters: Name Type Description Default fold_number required Source code in matbench/task.py def get_test_data ( self , fold_number , as_type = \"tuple\" , include_target = False ): \"\"\" The test data used for recording benchmarks. Args: fold_number: Returns: \"\"\" self . _check_is_loaded () fold_key = self . folds_map [ fold_number ] ids = self . validation [ fold_key ] . test if include_target : return self . _get_data_from_df ( ids , as_type ) else : if as_type == \"tuple\" : return self . _get_data_from_df ( ids , as_type )[ 0 ] elif as_type == \"df\" : return self . _get_data_from_df ( ids , as_type )[ [ self . metadata . input_type ] ]","title":"get_test_data()"},{"location":"Reference/MatbenchTask/#matbench.task.MatbenchTask.get_train_and_val_data","text":"The training + validation data. All model tuning and hyperparameter selection must be done on this data, NOT test data. Parameters: Name Type Description Default fold_number required Source code in matbench/task.py def get_train_and_val_data ( self , fold_number , as_type = \"tuple\" ): \"\"\" The training + validation data. All model tuning and hyperparameter selection must be done on this data, NOT test data. Args: fold_number: Returns: \"\"\" self . _check_is_loaded () fold_key = self . folds_map [ fold_number ] ids = self . validation [ fold_key ] . train return self . _get_data_from_df ( ids , as_type )","title":"get_train_and_val_data()"},{"location":"Reference/MatbenchTask/#matbench.task.MatbenchTask.load","text":"Load the dataset for this task into memory. Returns: Type Description None Source code in matbench/task.py def load ( self ): \"\"\"Load the dataset for this task into memory. Returns: None \"\"\" if self . df is None : logger . info ( f \"Loading dataset ' { self . dataset_name } '...\" ) self . df = load ( self . dataset_name ) logger . info ( f \"Dataset ' { self . dataset_name } loaded.\" ) else : logger . info ( f \"Dataset { self . dataset_name } already loaded; \" f \"not reloading dataset.\" )","title":"load()"},{"location":"Reference/MatbenchTask/#matbench.task.MatbenchTask.record","text":"Record the test data as well as parameters about the model trained on this fold. Parameters: Name Type Description Default fold_number int The fold number. required predictions [float] or [bool] or np.ndarray A list of predictions for fold number {fold_number} required params dict Any free-form parameters for information about the algorithm on this fold. For example, hyperparameters determined during validation. Parameters must be a dictionary; dictionary types must adhere to the same requirements as in the MatbenchBenchmark.add_metadata docstring. None Returns: Type Description None Source code in matbench/task.py def record ( self , fold_number , predictions , params = None ): \"\"\"Record the test data as well as parameters about the model trained on this fold. Args: fold_number (int): The fold number. predictions ([float] or [bool] or np.ndarray): A list of predictions for fold number {fold_number} params (dict): Any free-form parameters for information about the algorithm on this fold. For example, hyperparameters determined during validation. Parameters must be a dictionary; dictionary types must adhere to the same requirements as in the MatbenchBenchmark.add_metadata docstring. Returns: None \"\"\" if self . is_recorded [ fold_number ]: logger . error ( f \"Fold number { fold_number } already recorded! Aborting record...\" ) else : # avoid problems with json serialization if isinstance ( predictions , np . ndarray ): predictions = predictions . tolist () fold_key = self . folds_map [ fold_number ] # create map of original df index to prediction, e.g., # {ix_of_original_df1: prediction1, ... etc.} split_ids = self . validation [ fold_key ] . test if len ( predictions ) != len ( split_ids ): raise ValueError ( f \"Prediction outputs must be the same length as the \" f \"inputs! { len ( predictions ) } != { len ( split_ids ) } \" ) ids_to_predictions = { split_ids [ i ]: p for i , p in enumerate ( predictions )} self . results [ fold_key ][ self . _DATA_KEY ] = ids_to_predictions if not isinstance ( params , ( dict , type ( None ))): raise TypeError ( f \"Parameters must be stored as a dictionary, not { type ( params ) } !\" ) params = immutify_dictionary ( params ) if params else params self . results [ fold_key ][ self . _PARAMS_KEY ] = params if params else {} self . is_recorded [ fold_number ] = True logger . info ( f \"Recorded fold \" f \" { self . dataset_name } - { fold_number } successfully.\" ) truth = self . _get_data_from_df ( split_ids , as_type = \"tuple\" )[ 1 ] self . results [ fold_key ][ self . _SCORES_KEY ] = score_array ( truth , predictions , self . metadata . task_type ) logger . debug ( f \"Scored fold '\" f \" { self . dataset_name } - { fold_key } successfully.\" )","title":"record()"},{"location":"Reference/MatbenchTask/#matbench.task.MatbenchTask.validate","text":"Validate a task after all folds have been recorded. There are a few requirements for a task to be validated: - Data types of each predicted sample must match those specified by the validation procedure - All folds must be recorded - There must be no extra or missing required keys from the data, including indices. Every index specified in the validation procedure must be present in its correct fold, and no extras may be present. Source code in matbench/task.py def validate ( self ): \"\"\"Validate a task after all folds have been recorded. There are a few requirements for a task to be validated: - Data types of each predicted sample must match those specified by the validation procedure - All folds must be recorded - There must be no extra or missing required keys from the data, including indices. Every index specified in the validation procedure must be present in its correct fold, and no extras may be present. Returns: \"\"\" self . _check_all_folds_recorded ( f \"Cannot validate task { self . dataset_name } \" f \"unless all folds recorded!\" ) task_type = self . metadata . task_type # Check for extra fold keys extra_fold_keys = [ k for k in self . results if k not in self . folds_keys ] if extra_fold_keys : raise KeyError ( f \"Extra fold keys { extra_fold_keys } for task \" f \" { self . dataset_name } not allowed.\" ) for fold_key in self . folds_keys : if fold_key not in self . results : raise KeyError ( f \"Required fold data for fold ' { fold_key } ' \" f \"for task { self . dataset_name } not found.\" ) # Check for extra or missing keys inside each fold: # need params, scores, and data. req_subfold_keys = [ self . _SCORES_KEY , self . _DATA_KEY , self . _PARAMS_KEY ] extra_subfold_keys = [ k for k in self . results [ fold_key ] if k not in req_subfold_keys ] if extra_subfold_keys : raise KeyError ( f \"Extra keys { extra_subfold_keys } for fold results of \" f \"' { fold_key } ' for task { self . dataset_name } not allowed.\" ) for subkey in req_subfold_keys : fold_results = self . results [ fold_key ] if subkey not in fold_results : raise KeyError ( f \"Required key ' { subkey } ' for task { self . dataset_name } \" f \"not found for fold ' { fold_key } '.\" ) if subkey == self . _SCORES_KEY : scores = self . results [ fold_key ][ subkey ] metrics = REG_METRICS if task_type == REG_KEY else CLF_METRICS for m in metrics : if m not in scores : raise KeyError ( f \"Required score ' { m } ' for task \" f \" { self . dataset_name } \" f \"not found for ' { fold_key } '.\" ) elif not isinstance ( scores [ m ], float ): raise TypeError ( f \"Required score ' { m } ' for task \" f \" { self . dataset_name } \" f \"is not float-type for ' { fold_key } '!\" ) extra_metrics = [ k for k in scores if k not in metrics ] if extra_metrics : raise KeyError ( f \"Extra keys { extra_metrics } for fold scores of \" f \"' { fold_key } ' for task { self . dataset_name } \" f \"not allowed.\" ) # results data indices are cast by json to be strings, # so must be converted to int elif subkey == self . _DATA_KEY : fold_data = self . results [ fold_key ] . data # Ensure all the indices are present with no # extras for each fold req_indices = set ( self . validation [ fold_key ] . test ) remaining_indices = copy . deepcopy ( req_indices ) extra_indices = {} if self . metadata . task_type == REG_KEY : allowed_types = ( float ,) else : allowed_types = ( bool , float ) for ix , datum in fold_data . items (): if ix not in req_indices : extra_indices [ ix ] = datum else : if not isinstance ( datum , allowed_types ): raise TypeError ( f \"Data point ' { ix } : { datum } ' has data type \" f \" { type ( datum ) } while required type is \" f \" { allowed_types } for task \" f \" { self . dataset_name } !\" ) if self . metadata . task_type == CLF_KEY : if isinstance ( datum , float ): if datum < 0 or datum > 1 : raise ValueError ( f \"Probability estimate ' { ix } ': { datum } \" f \"for task { self . dataset_name } outside \" f \"of range [0, 1].\" ) remaining_indices . remove ( ix ) if extra_indices and not remaining_indices : raise ValueError ( f \" { len ( extra_indices ) } extra indices for problem \" f \" { self . dataset_name } are not allowed (found in \" f \" { fold_key } : { remaining_indices } \" ) elif not extra_indices and remaining_indices : raise ValueError ( f \" { len ( remaining_indices ) } required indices \" f \"for problem { self . dataset_name } not \" f \"found for { fold_key } : { remaining_indices } \" ) elif extra_indices and remaining_indices : raise ValueError ( f \" { len ( remaining_indices ) } required indices \" f \"for problem { self . dataset_name } not \" f \"found and { len ( extra_indices ) } not \" f \"allowed indices found for { fold_key } !\" ) else : pass # Params key has no required form; # it is up to the model to determine it. logger . debug ( f \"Data for { self . dataset_name } successfully validated.\" )","title":"validate()"}]}